{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU, Linear, ReLU, Sequential\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, Set2Set\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "import torch_geometric\n",
    "\n",
    "from detanet_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 2  # Target property = homo, see https://pytorch-geometric.readthedocs.io/en/2.6.1/generated/torch_geometric.datasets.QM9.html for correct number \n",
    "\n",
    "def preprocess_data(data):\n",
    "    #Create a copy to avoid in place modification\n",
    "\n",
    "    new_dataset = []\n",
    "\n",
    "    for molecule in data:\n",
    "        pos = molecule.pos\n",
    "        z = molecule.z\n",
    "        homo = molecule.y.squeeze()[2]\n",
    "        lumo = molecule.y.squeeze()[3]\n",
    "        gap = molecule.y.squeeze()[4]\n",
    "\n",
    "        \n",
    "\n",
    "        # Create the dataset entry\n",
    "        data_entry = torch_geometric.data.Data(\n",
    "            pos=pos.to(torch.float32),    # Atomic positions\n",
    "            z=torch.LongTensor(z),        # Atomic numbers\n",
    "            homo=homo.to(torch.float32), # Polarizability tensor (target\n",
    "            lumo = lumo.to(torch.float32),\n",
    "            gap = gap.to(torch.float32)    \n",
    "        )\n",
    "\n",
    "        new_dataset.append(data_entry)\n",
    "    return new_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[17, 11], edge_index=[2, 34], edge_attr=[34, 4], y=[1, 19], pos=[17, 3], z=[17], smiles='[H][N-][C+]([H])O[C@@]1([H])C([H])([H])[C@@]1([H])O[C+]([H])[N-][H]', name='gdb_51600', idx=[1])\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('..', 'data', 'QM9')\n",
    "dataset = QM9(path).shuffle()\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(pos=[17, 3], z=[17], homo=-6.925297737121582, lumo=0.5768813490867615, gap=7.502179145812988)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(124289, 6542)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We divided the dataset evenly and used 5% of the data for testing and other for training:'''\n",
    "dataset = preprocess_data(dataset)\n",
    "print(dataset[0])\n",
    "\n",
    "\n",
    "train_datasets=[]\n",
    "val_datasets=[]\n",
    "for i in range(len(dataset)):\n",
    "    if i%20==0:\n",
    "        val_datasets.append(dataset[i])\n",
    "    else:\n",
    "        train_datasets.append(dataset[i])\n",
    "        \n",
    "len(train_datasets),len(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lumo:  tensor(0.2912)\n",
      "homo:  tensor(-5.4368)\n",
      "gap:  tensor(5.7280)\n",
      "z tensor([8, 6, 6, 7, 6, 6, 6, 6, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for data in train_datasets:\n",
    "    print(\"lumo: \", data.lumo)\n",
    "    print(\"homo: \", data.homo)\n",
    "    print(\"gap: \", data.gap )\n",
    "    print(\"z\", data.z)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using torch_Geometric.dataloader.DataLoader Converts a dataset into a batch of 64 molecules of training data.'''\n",
    "batches=64\n",
    "trainloader=DataLoader(train_datasets,batch_size=batches,shuffle=True)\n",
    "valloader=DataLoader(val_datasets,batch_size=batches,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetaNet(\n",
       "  (Embedding): Embedding(\n",
       "    (act): Swish()\n",
       "    (elec_emb): Linear(in_features=16, out_features=128, bias=False)\n",
       "    (nuclare_emb): Embedding(10, 128)\n",
       "    (ls): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (Radial): Radial_Basis(\n",
       "    (radial): Bessel_Function()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sout): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''After loading the dataset, we train a model using NPA charge as an example.\n",
    " \tFirstly, construct an untrained model:'''\n",
    "model=DetaNet(num_features=128,\n",
    "                 act='swish',\n",
    "                 maxl=3,\n",
    "                 num_block=3,\n",
    "                 radial_type='trainable_bessel',\n",
    "                 num_radial=32,\n",
    "                 attention_head=8,\n",
    "                 rc=5.0,\n",
    "                 dropout=0.0,\n",
    "                 use_cutoff=False,\n",
    "                 max_atomic_number=9,\n",
    "                 atom_ref=None,\n",
    "                 scale=None,\n",
    "                 scalar_outsize=1,\n",
    "                 irreps_out=None,\n",
    "                 summation=False,\n",
    "                 norm=False,\n",
    "                 out_type='scalar',\n",
    "                 grad_type=None ,\n",
    "                 device=torch.device(device))\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "'''Next, define the trainer and the parameters used for training.'''\n",
    "class Trainer:\n",
    "    def __init__(self,model,train_loader,val_loader=None,loss_function=l2loss,device=torch.device(device),\n",
    "                 optimizer='Adam_amsgrad',lr=5e-4,weight_decay=0):\n",
    "        self.opt_type=optimizer\n",
    "        self.device=device\n",
    "        self.model=model\n",
    "        self.train_data=train_loader\n",
    "        self.val_data=val_loader\n",
    "        self.device=device\n",
    "        self.opts={'AdamW':torch.optim.AdamW(self.model.parameters(),lr=lr,amsgrad=False,weight_decay=weight_decay),\n",
    "              'AdamW_amsgrad':torch.optim.AdamW(self.model.parameters(),lr=lr,amsgrad=True,weight_decay=weight_decay),\n",
    "              'Adam':torch.optim.Adam(self.model.parameters(),lr=lr,amsgrad=False,weight_decay=weight_decay),\n",
    "              'Adam_amsgrad':torch.optim.Adam(self.model.parameters(),lr=lr,amsgrad=True,weight_decay=weight_decay),\n",
    "              'Adadelta':torch.optim.Adadelta(self.model.parameters(),lr=lr,weight_decay=weight_decay),\n",
    "              'RMSprop':torch.optim.RMSprop(self.model.parameters(),lr=lr,weight_decay=weight_decay),\n",
    "              'SGD':torch.optim.SGD(self.model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "        }\n",
    "        self.optimizer=self.opts[self.opt_type]\n",
    "        self.loss_function=loss_function\n",
    "        self.step=-1\n",
    "    def train(self,num_train,targ,stop_loss=1e-8, val_per_train=50, print_per_epoch=10):\n",
    "        self.model.train()\n",
    "        len_train=len(self.train_data)\n",
    "        for i in range(num_train):\n",
    "            val_datas=iter(self.val_data)\n",
    "            for j,batch in enumerate(self.train_data):\n",
    "                self.step=self.step+1\n",
    "                torch.cuda.empty_cache()\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(pos=batch.pos.to(self.device), z=batch.z.to(self.device),\n",
    "                                     batch=batch.batch.to(self.device))\n",
    "                graph_out = global_mean_pool(out, batch.batch)  # Shape: [batch_size, d]\n",
    "                target = batch[targ].to(self.device)\n",
    "                loss = self.loss_function(graph_out.reshape(target.shape),target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if (self.step%val_per_train==0) and (self.val_data is not None):\n",
    "                    val_batch = next(val_datas)\n",
    "                    val_target=val_batch[targ].to(self.device).reshape(-1)\n",
    "\n",
    "                    val_out = self.model(pos=val_batch.pos.to(self.device), z=val_batch.z.to(self.device),\n",
    "                    batch=val_batch.batch.to(self.device))\n",
    "\n",
    "                    # Aggregate node-level outputs to graph-level outputs\n",
    "                    val_graph_out = global_mean_pool(val_out, val_batch.batch)  # Shape: [val_batch_size, d]\n",
    "\n",
    "                    # Ensure the shapes match\n",
    "                    val_loss = self.loss_function(val_graph_out.reshape(val_target.shape), val_target).item()\n",
    "                    val_mae = l1loss(val_graph_out, val_target).item()\n",
    "                    val_R2 = R2(val_graph_out, val_target).item()\n",
    "\n",
    "                    if self.step % print_per_epoch==0:\n",
    "                        print('Epoch[{}/{}],loss:{:.8f},val_loss:{:.8f},val_mae:{:.8f},val_R2:{:.8f}'\n",
    "                              .format(self.step,num_train*len_train,loss.item(),val_loss,val_mae,val_R2))\n",
    "\n",
    "                    assert (loss > stop_loss) or (val_loss > stop_loss),'Training and prediction Loss is less' \\\n",
    "                                                                        ' than cut-off Loss, so training stops'\n",
    "                elif (self.step % print_per_epoch == 0) and (self.step%val_per_train!=0):\n",
    "                    print('Epoch[{}/{}],loss:{:.8f}'.format(self.step,num_train*len_train, loss.item()))\n",
    "                    \n",
    "    def load_state_and_optimizer(self,state_path=None,optimizer_path=None):\n",
    "        if state_path is not None:\n",
    "            state_dict=torch.load(state_path)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        if optimizer_path is not None:\n",
    "            self.optimizer=torch.load(optimizer_path)\n",
    "\n",
    "    def save_param(self,path):\n",
    "        torch.save(self.model.state_dict(),path)\n",
    "\n",
    "    def save_model(self,path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def save_opt(self,path):\n",
    "        torch.save(self.optimizer,path)\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.state_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Then, modify the data type and device type'''\n",
    "device=torch.device('cpu')\n",
    "dtype=torch.float32\n",
    "model=model.to(dtype)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finally, using the trainer, training 20 times from a 5e-4 learning rate'''\n",
    "trainer=Trainer(model,train_loader=trainloader,val_loader=valloader,loss_function=l2loss,lr=5e-4,weight_decay=0,optimizer='AdamW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/1943],loss:40.64371872,val_loss:35.94981003,val_mae:5.94719601,val_R2:-4478.80810547\n",
      "Epoch[10/1943],loss:1.77643800\n",
      "Epoch[20/1943],loss:0.77030885\n",
      "Epoch[30/1943],loss:1.44904077\n",
      "Epoch[40/1943],loss:0.74103355\n",
      "Epoch[50/1943],loss:0.43585786,val_loss:0.44911265,val_mae:0.48343807,val_R2:-71.99038696\n",
      "Epoch[60/1943],loss:0.43952227\n",
      "Epoch[70/1943],loss:0.47705412\n",
      "Epoch[80/1943],loss:0.62001318\n",
      "Epoch[90/1943],loss:0.75179982\n",
      "Epoch[100/1943],loss:0.21752797,val_loss:0.31865183,val_mae:0.47467422,val_R2:-65.66029358\n",
      "Epoch[110/1943],loss:0.51658762\n",
      "Epoch[120/1943],loss:0.37469721\n",
      "Epoch[130/1943],loss:0.32675081\n",
      "Epoch[140/1943],loss:0.28695488\n",
      "Epoch[150/1943],loss:0.24882981,val_loss:0.19998977,val_mae:0.41605532,val_R2:-75.63143158\n",
      "Epoch[160/1943],loss:0.28906411\n",
      "Epoch[170/1943],loss:0.20008424\n",
      "Epoch[180/1943],loss:0.26013213\n",
      "Epoch[190/1943],loss:0.12305635\n",
      "Epoch[200/1943],loss:0.16629326,val_loss:0.12651776,val_mae:0.67349136,val_R2:-123.11518860\n",
      "Epoch[210/1943],loss:0.17127825\n",
      "Epoch[220/1943],loss:0.22645321\n",
      "Epoch[230/1943],loss:0.13800158\n",
      "Epoch[240/1943],loss:0.12417160\n",
      "Epoch[250/1943],loss:0.14925571,val_loss:0.09646172,val_mae:0.50623661,val_R2:-107.37990570\n",
      "Epoch[260/1943],loss:0.10862610\n",
      "Epoch[270/1943],loss:0.11937743\n",
      "Epoch[280/1943],loss:0.08926064\n",
      "Epoch[290/1943],loss:0.11529677\n",
      "Epoch[300/1943],loss:0.13032550,val_loss:0.10637447,val_mae:0.62230784,val_R2:-123.64662933\n",
      "Epoch[310/1943],loss:0.10928188\n",
      "Epoch[320/1943],loss:0.12922218\n",
      "Epoch[330/1943],loss:0.13753834\n",
      "Epoch[340/1943],loss:0.07944651\n",
      "Epoch[350/1943],loss:0.09064382,val_loss:0.10263257,val_mae:0.61019826,val_R2:-99.61072540\n",
      "Epoch[360/1943],loss:0.06596884\n",
      "Epoch[370/1943],loss:0.19327997\n",
      "Epoch[380/1943],loss:0.14602688\n",
      "Epoch[390/1943],loss:0.11407091\n",
      "Epoch[400/1943],loss:0.07281062,val_loss:0.10910320,val_mae:0.63853383,val_R2:-113.47810364\n",
      "Epoch[410/1943],loss:0.10757092\n",
      "Epoch[420/1943],loss:0.06044591\n",
      "Epoch[430/1943],loss:0.09517556\n",
      "Epoch[440/1943],loss:0.10902885\n",
      "Epoch[450/1943],loss:0.13243932,val_loss:0.08200807,val_mae:0.57616872,val_R2:-115.49714661\n",
      "Epoch[460/1943],loss:0.08822216\n",
      "Epoch[470/1943],loss:0.13114007\n",
      "Epoch[480/1943],loss:0.05919786\n",
      "Epoch[490/1943],loss:0.07836424\n",
      "Epoch[500/1943],loss:0.06006313,val_loss:0.08418857,val_mae:0.51179475,val_R2:-126.66485596\n",
      "Epoch[510/1943],loss:0.08252943\n",
      "Epoch[520/1943],loss:0.06510618\n",
      "Epoch[530/1943],loss:0.16250534\n",
      "Epoch[540/1943],loss:0.10270858\n",
      "Epoch[550/1943],loss:0.10586289,val_loss:0.12295383,val_mae:0.62546682,val_R2:-105.90377045\n",
      "Epoch[560/1943],loss:0.13656801\n",
      "Epoch[570/1943],loss:0.07853115\n",
      "Epoch[580/1943],loss:0.08794317\n",
      "Epoch[590/1943],loss:0.11490293\n",
      "Epoch[600/1943],loss:0.09810080,val_loss:0.06556948,val_mae:0.59703529,val_R2:-112.88378906\n",
      "Epoch[610/1943],loss:0.11275273\n",
      "Epoch[620/1943],loss:0.07802986\n",
      "Epoch[630/1943],loss:0.14307038\n",
      "Epoch[640/1943],loss:0.07864546\n",
      "Epoch[650/1943],loss:0.08290585,val_loss:0.05952262,val_mae:0.47998518,val_R2:-115.31187439\n",
      "Epoch[660/1943],loss:0.09911720\n",
      "Epoch[670/1943],loss:0.08246695\n",
      "Epoch[680/1943],loss:0.07394223\n",
      "Epoch[690/1943],loss:0.07109252\n",
      "Epoch[700/1943],loss:0.07958861,val_loss:0.10624028,val_mae:0.58569753,val_R2:-107.39884186\n",
      "Epoch[710/1943],loss:0.06728585\n",
      "Epoch[720/1943],loss:0.09772031\n",
      "Epoch[730/1943],loss:0.08768016\n",
      "Epoch[740/1943],loss:0.07107674\n",
      "Epoch[750/1943],loss:0.07431345,val_loss:0.07022543,val_mae:0.56085300,val_R2:-112.41916656\n",
      "Epoch[760/1943],loss:0.08480129\n",
      "Epoch[770/1943],loss:0.06984406\n",
      "Epoch[780/1943],loss:0.05773471\n",
      "Epoch[790/1943],loss:0.13205759\n",
      "Epoch[800/1943],loss:0.08527106,val_loss:0.08322271,val_mae:0.53700912,val_R2:-108.08552551\n",
      "Epoch[810/1943],loss:0.07896978\n",
      "Epoch[820/1943],loss:0.08265813\n",
      "Epoch[830/1943],loss:0.07032618\n",
      "Epoch[840/1943],loss:0.05103741\n",
      "Epoch[850/1943],loss:0.07022192,val_loss:0.07205098,val_mae:0.68527603,val_R2:-117.05035400\n",
      "Epoch[860/1943],loss:0.05666807\n",
      "Epoch[870/1943],loss:0.05052866\n",
      "Epoch[880/1943],loss:0.05723096\n",
      "Epoch[890/1943],loss:0.09501686\n",
      "Epoch[900/1943],loss:0.04090232,val_loss:0.05962566,val_mae:0.60505736,val_R2:-111.89254761\n",
      "Epoch[910/1943],loss:0.08969562\n",
      "Epoch[920/1943],loss:0.04894501\n",
      "Epoch[930/1943],loss:0.10201229\n",
      "Epoch[940/1943],loss:0.10491512\n",
      "Epoch[950/1943],loss:0.05470816,val_loss:0.09595355,val_mae:0.52090162,val_R2:-115.46067810\n",
      "Epoch[960/1943],loss:0.05837809\n",
      "Epoch[970/1943],loss:0.08790228\n",
      "Epoch[980/1943],loss:0.05297194\n",
      "Epoch[990/1943],loss:0.07486749\n",
      "Epoch[1000/1943],loss:0.06242231,val_loss:0.07633171,val_mae:0.61431479,val_R2:-108.35909271\n",
      "Epoch[1010/1943],loss:0.05932046\n",
      "Epoch[1020/1943],loss:0.05225364\n",
      "Epoch[1030/1943],loss:0.06306441\n",
      "Epoch[1040/1943],loss:0.06345060\n",
      "Epoch[1050/1943],loss:0.05018706,val_loss:0.05015725,val_mae:0.67656153,val_R2:-125.99533081\n",
      "Epoch[1060/1943],loss:0.06587934\n",
      "Epoch[1070/1943],loss:0.08168520\n",
      "Epoch[1080/1943],loss:0.05491317\n",
      "Epoch[1090/1943],loss:0.07175165\n",
      "Epoch[1100/1943],loss:0.09015775,val_loss:0.05190625,val_mae:0.59841806,val_R2:-123.37969208\n",
      "Epoch[1110/1943],loss:0.05229579\n",
      "Epoch[1120/1943],loss:0.07695742\n",
      "Epoch[1130/1943],loss:0.06144746\n",
      "Epoch[1140/1943],loss:0.06062733\n",
      "Epoch[1150/1943],loss:0.04886153,val_loss:0.07746649,val_mae:0.55562913,val_R2:-99.11212158\n",
      "Epoch[1160/1943],loss:0.06627811\n",
      "Epoch[1170/1943],loss:0.05398956\n",
      "Epoch[1180/1943],loss:0.05171508\n",
      "Epoch[1190/1943],loss:0.04250681\n",
      "Epoch[1200/1943],loss:0.05718262,val_loss:0.07568486,val_mae:0.54065025,val_R2:-105.06527710\n",
      "Epoch[1210/1943],loss:0.11063255\n",
      "Epoch[1220/1943],loss:0.05754907\n",
      "Epoch[1230/1943],loss:0.06858232\n",
      "Epoch[1240/1943],loss:0.06428962\n",
      "Epoch[1250/1943],loss:0.04159166,val_loss:0.06052419,val_mae:0.80709440,val_R2:-113.13036346\n",
      "Epoch[1260/1943],loss:0.05051351\n",
      "Epoch[1270/1943],loss:0.04970261\n",
      "Epoch[1280/1943],loss:0.05838767\n",
      "Epoch[1290/1943],loss:0.05260650\n",
      "Epoch[1300/1943],loss:0.05920516,val_loss:0.05605950,val_mae:0.67941159,val_R2:-132.93157959\n",
      "Epoch[1310/1943],loss:0.05306497\n",
      "Epoch[1320/1943],loss:0.07214109\n",
      "Epoch[1330/1943],loss:0.03829577\n",
      "Epoch[1340/1943],loss:0.07482126\n",
      "Epoch[1350/1943],loss:0.04783240,val_loss:0.05398746,val_mae:0.61032772,val_R2:-123.15245819\n",
      "Epoch[1360/1943],loss:0.03043179\n",
      "Epoch[1370/1943],loss:0.04834022\n",
      "Epoch[1380/1943],loss:0.05258367\n",
      "Epoch[1390/1943],loss:0.05950461\n",
      "Epoch[1400/1943],loss:0.09922378,val_loss:0.03032780,val_mae:0.49077088,val_R2:-124.51822662\n",
      "Epoch[1410/1943],loss:0.05712836\n",
      "Epoch[1420/1943],loss:0.06296495\n",
      "Epoch[1430/1943],loss:0.06642339\n",
      "Epoch[1440/1943],loss:0.04886497\n",
      "Epoch[1450/1943],loss:0.05463802,val_loss:0.04994480,val_mae:0.68041378,val_R2:-135.41929626\n",
      "Epoch[1460/1943],loss:0.07953056\n",
      "Epoch[1470/1943],loss:0.05671505\n",
      "Epoch[1480/1943],loss:0.04718108\n",
      "Epoch[1490/1943],loss:0.04088380\n",
      "Epoch[1500/1943],loss:0.05347108,val_loss:0.04849971,val_mae:0.58175343,val_R2:-116.61281586\n",
      "Epoch[1510/1943],loss:0.04778689\n",
      "Epoch[1520/1943],loss:0.06622868\n",
      "Epoch[1530/1943],loss:0.03551003\n",
      "Epoch[1540/1943],loss:0.04951038\n",
      "Epoch[1550/1943],loss:0.05750436,val_loss:0.05812138,val_mae:0.74981439,val_R2:-104.48767853\n",
      "Epoch[1560/1943],loss:0.03590007\n",
      "Epoch[1570/1943],loss:0.04777555\n",
      "Epoch[1580/1943],loss:0.04650725\n",
      "Epoch[1590/1943],loss:0.05169040\n",
      "Epoch[1600/1943],loss:0.06521706,val_loss:0.05592555,val_mae:0.49717861,val_R2:-120.56263733\n",
      "Epoch[1610/1943],loss:0.05243569\n",
      "Epoch[1620/1943],loss:0.05504657\n",
      "Epoch[1630/1943],loss:0.04782735\n",
      "Epoch[1640/1943],loss:0.04913142\n",
      "Epoch[1650/1943],loss:0.07534634,val_loss:0.07096678,val_mae:0.65235949,val_R2:-114.82145691\n",
      "Epoch[1660/1943],loss:0.03728444\n",
      "Epoch[1670/1943],loss:0.04654672\n",
      "Epoch[1680/1943],loss:0.03278196\n",
      "Epoch[1690/1943],loss:0.07708847\n",
      "Epoch[1700/1943],loss:0.04061181,val_loss:0.03750481,val_mae:0.63678360,val_R2:-133.49383545\n",
      "Epoch[1710/1943],loss:0.04826982\n",
      "Epoch[1720/1943],loss:0.04505767\n",
      "Epoch[1730/1943],loss:0.06245495\n",
      "Epoch[1740/1943],loss:0.05829726\n",
      "Epoch[1750/1943],loss:0.04188021,val_loss:0.04689106,val_mae:0.58959967,val_R2:-125.37864685\n",
      "Epoch[1760/1943],loss:0.03681473\n",
      "Epoch[1770/1943],loss:0.05748944\n",
      "Epoch[1780/1943],loss:0.05516609\n",
      "Epoch[1790/1943],loss:0.05223295\n",
      "Epoch[1800/1943],loss:0.03818469,val_loss:0.07127654,val_mae:0.60409117,val_R2:-118.15750885\n",
      "Epoch[1810/1943],loss:0.05330421\n",
      "Epoch[1820/1943],loss:0.05010245\n",
      "Epoch[1830/1943],loss:0.05031685\n",
      "Epoch[1840/1943],loss:0.03301018\n",
      "Epoch[1850/1943],loss:0.04162144,val_loss:0.05664917,val_mae:0.71691853,val_R2:-112.24141693\n",
      "Epoch[1860/1943],loss:0.02358792\n",
      "Epoch[1870/1943],loss:0.05518454\n",
      "Epoch[1880/1943],loss:0.05850020\n",
      "Epoch[1890/1943],loss:0.05839089\n",
      "Epoch[1900/1943],loss:0.04615291,val_loss:0.03721642,val_mae:0.54802120,val_R2:-116.14796448\n",
      "Epoch[1910/1943],loss:0.03034344\n",
      "Epoch[1920/1943],loss:0.05248275\n",
      "Epoch[1930/1943],loss:0.02503147\n",
      "Epoch[1940/1943],loss:0.04172691\n"
     ]
    }
   ],
   "source": [
    "trainer.train(num_train=1,targ='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Embedding.act.alpha',\n",
       "              tensor([1.0196, 1.0054, 1.0352, 1.0057, 0.9917, 1.0029, 1.0069, 0.9902, 1.0026,\n",
       "                      1.0077, 0.9943, 0.9933, 1.0125, 1.0095, 1.0116, 0.9897, 1.0007, 1.0233,\n",
       "                      1.0105, 0.9933, 1.0258, 1.0051, 0.9961, 1.0081, 1.0118, 1.0024, 1.0188,\n",
       "                      0.9934, 1.0065, 1.0074, 1.0105, 1.0080, 0.9965, 0.9911, 1.0073, 1.0173,\n",
       "                      0.9962, 1.0083, 0.9997, 0.9937, 1.0065, 0.9985, 1.0060, 0.9933, 1.0084,\n",
       "                      0.9917, 1.0135, 0.9985, 1.0126, 1.0004, 0.9913, 0.9987, 1.0053, 1.0064,\n",
       "                      0.9996, 1.0085, 0.9967, 0.9969, 0.9991, 1.0050, 0.9932, 0.9992, 1.0117,\n",
       "                      1.0020, 1.0058, 0.9917, 1.0059, 1.0073, 1.0216, 1.0068, 1.0066, 1.0233,\n",
       "                      1.0043, 1.0461, 0.9996, 0.9953, 0.9878, 0.9923, 0.9918, 1.0068, 0.9967,\n",
       "                      1.0109, 1.0100, 1.0129, 1.0056, 0.9920, 1.0080, 0.9976, 1.0000, 0.9893,\n",
       "                      1.0072, 1.0079, 1.0145, 1.0089, 0.9988, 1.0085, 0.9998, 1.0162, 0.9913,\n",
       "                      1.0119, 0.9910, 1.0459, 1.0041, 1.0023, 1.0005, 1.0096, 0.9917, 1.0186,\n",
       "                      0.9947, 0.9910, 0.9910, 1.0052, 1.0133, 1.0263, 1.0076, 1.0091, 1.0077,\n",
       "                      1.0167, 1.0127, 1.0025, 0.9875, 0.9933, 1.0077, 0.9951, 1.0148, 1.0104,\n",
       "                      0.9953, 1.0059])),\n",
       "             ('Embedding.act.beta',\n",
       "              tensor([1.7117, 1.7098, 1.7136, 1.7101, 1.7071, 1.6957, 1.7103, 1.6877, 1.6957,\n",
       "                      1.7087, 1.7035, 1.6953, 1.6936, 1.6928, 1.7018, 1.7123, 1.7051, 1.7130,\n",
       "                      1.7127, 1.6971, 1.7153, 1.6994, 1.6973, 1.6929, 1.7080, 1.6934, 1.6910,\n",
       "                      1.6938, 1.7108, 1.7110, 1.7077, 1.6921, 1.6959, 1.6912, 1.7099, 1.7111,\n",
       "                      1.6926, 1.7115, 1.7095, 1.6924, 1.6939, 1.6819, 1.6931, 1.6956, 1.6934,\n",
       "                      1.7134, 1.7147, 1.6937, 1.6725, 1.6896, 1.6928, 1.7099, 1.6944, 1.7104,\n",
       "                      1.6913, 1.6926, 1.7111, 1.6995, 1.6984, 1.6927, 1.7008, 1.6998, 1.6925,\n",
       "                      1.7099, 1.7042, 1.6979, 1.7100, 1.6964, 1.7198, 1.7108, 1.7104, 1.7133,\n",
       "                      1.7004, 1.7124, 1.7088, 1.6920, 1.6938, 1.6938, 1.7073, 1.7098, 1.6937,\n",
       "                      1.6935, 1.6993, 1.7129, 1.7096, 1.6994, 1.6946, 1.7087, 1.7000, 1.7103,\n",
       "                      1.7106, 1.7097, 1.7117, 1.6921, 1.7020, 1.6927, 1.6955, 1.7113, 1.6918,\n",
       "                      1.7137, 1.6945, 1.7149, 1.6942, 1.7016, 1.7068, 1.6928, 1.6920, 1.6950,\n",
       "                      1.6922, 1.6938, 1.7091, 1.6924, 1.7117, 1.6998, 1.7103, 1.6929, 1.7102,\n",
       "                      1.7007, 1.7135, 1.7116, 1.6930, 1.6959, 1.6930, 1.6927, 1.7102, 1.6903,\n",
       "                      1.6951, 1.7133])),\n",
       "             ('Embedding.elec_emb.weight',\n",
       "              tensor([[-9.9097e-02, -1.8619e-02,  5.7117e-02,  ..., -8.2658e-02,\n",
       "                       -1.7343e-01,  1.1240e-01],\n",
       "                      [ 6.0228e-02,  5.7199e-02, -2.1492e-01,  ..., -2.0070e-01,\n",
       "                       -7.0928e-02,  1.7871e-02],\n",
       "                      [-4.3408e-02, -1.2640e-01, -1.5386e-01,  ...,  8.0399e-02,\n",
       "                        2.0315e-01,  4.4805e-02],\n",
       "                      ...,\n",
       "                      [-9.5187e-02, -7.2999e-03, -2.0103e-01,  ...,  1.6112e-01,\n",
       "                        7.8736e-02,  1.9639e-01],\n",
       "                      [-7.2428e-02,  2.0771e-04,  6.1057e-02,  ...,  2.7964e-02,\n",
       "                       -1.0397e-02, -1.5280e-01],\n",
       "                      [ 6.2866e-02,  1.1352e-01, -2.6021e-02,  ..., -1.4967e-01,\n",
       "                       -8.9615e-02,  1.0626e-01]])),\n",
       "             ('Embedding.nuclare_emb.weight',\n",
       "              tensor([[-0.7636, -0.0716,  0.8631,  ...,  1.3771,  1.2198, -1.1919],\n",
       "                      [-0.3463,  0.2849, -0.4847,  ...,  0.2779,  2.2456, -1.4428],\n",
       "                      [ 0.3010, -0.3156, -1.7022,  ...,  0.4524, -0.9309, -1.5703],\n",
       "                      ...,\n",
       "                      [ 0.7880,  1.9228,  0.6961,  ...,  0.2866, -0.7114,  1.3779],\n",
       "                      [-0.0239, -0.1808, -0.3506,  ..., -0.4596, -0.0384,  1.0814],\n",
       "                      [ 1.6841, -0.7695, -0.2313,  ...,  0.8192,  0.4046,  0.1852]])),\n",
       "             ('Embedding.ls.weight',\n",
       "              tensor([[ 0.0486,  0.1663, -0.0326,  ...,  0.0914,  0.0756, -0.0863],\n",
       "                      [ 0.0060, -0.0120, -0.0805,  ..., -0.0928, -0.0306, -0.0959],\n",
       "                      [-0.0222, -0.1233,  0.1278,  ..., -0.0722, -0.0275,  0.0800],\n",
       "                      ...,\n",
       "                      [-0.1453,  0.0601, -0.1282,  ...,  0.0342, -0.1383, -0.1579],\n",
       "                      [ 0.0106, -0.0202, -0.0690,  ..., -0.1026, -0.0633, -0.0036],\n",
       "                      [ 0.0070, -0.0205,  0.0322,  ...,  0.0619,  0.1204, -0.0256]])),\n",
       "             ('Embedding.ls.bias',\n",
       "              tensor([ 1.5267e-02,  7.4279e-03,  2.1028e-02,  7.1259e-03,  7.8210e-03,\n",
       "                      -3.4548e-03,  7.8527e-03, -9.8016e-03,  1.1594e-03,  5.7798e-03,\n",
       "                      -4.4882e-03, -1.0623e-02, -1.8071e-03, -1.1063e-02,  7.6138e-03,\n",
       "                      -8.2599e-03,  5.5003e-03,  1.5361e-02,  1.0229e-02, -3.5609e-03,\n",
       "                       2.2753e-02,  7.9103e-03, -9.0631e-03, -9.7588e-03,  9.4651e-03,\n",
       "                      -1.0246e-02,  1.1869e-02, -7.8925e-03,  8.1857e-03,  8.7195e-03,\n",
       "                      -7.6532e-03, -8.7590e-03, -2.9325e-03, -9.8887e-03,  1.0916e-02,\n",
       "                       1.0232e-02, -8.6546e-03,  8.6595e-03,  9.7373e-04, -8.4237e-03,\n",
       "                      -1.1630e-02,  5.3822e-03, -1.4125e-02, -3.7375e-03, -1.2369e-02,\n",
       "                      -8.4504e-03,  1.0781e-02, -4.0793e-04, -9.8923e-03, -8.1858e-03,\n",
       "                      -9.1039e-03,  1.0812e-02,  4.4911e-04,  7.5096e-03, -2.5268e-03,\n",
       "                      -1.0592e-02,  1.2425e-02, -6.7672e-03, -1.5906e-04, -1.0669e-02,\n",
       "                      -2.1954e-03, -6.0607e-03, -2.9188e-03,  9.1508e-03,  8.1289e-03,\n",
       "                      -1.1780e-02,  6.8356e-03, -8.5539e-03,  1.2912e-02,  8.0488e-03,\n",
       "                       7.8179e-03,  1.7860e-02, -1.5492e-02,  1.2284e-02,  7.1509e-03,\n",
       "                      -7.8146e-03, -1.0350e-02, -8.2537e-03,  7.9228e-03,  7.4383e-03,\n",
       "                      -3.9517e-03, -5.9815e-03,  3.6668e-03,  1.2705e-02,  7.3339e-03,\n",
       "                      -9.0985e-03, -1.0765e-02,  5.0661e-03, -3.5565e-05,  7.5761e-03,\n",
       "                       7.9240e-03,  9.0935e-03,  1.0128e-02, -7.3234e-03, -5.0916e-03,\n",
       "                      -8.3080e-03, -8.6469e-03,  1.5202e-02, -8.5966e-03,  9.6305e-03,\n",
       "                      -1.0737e-02,  1.1767e-02, -1.2749e-02,  5.4618e-03,  8.4729e-03,\n",
       "                      -8.6029e-03, -8.8991e-03,  6.4526e-03, -8.3265e-03, -9.6405e-03,\n",
       "                      -1.1378e-02, -8.7672e-03,  9.7596e-03,  2.5666e-02,  8.4846e-03,\n",
       "                       3.5974e-03,  7.8148e-03, -2.7142e-03,  1.0322e-02,  7.3690e-03,\n",
       "                      -1.1741e-02, -7.4130e-03,  5.9116e-03, -9.5214e-03,  1.2524e-02,\n",
       "                       8.0459e-03, -7.5100e-03,  2.5605e-03])),\n",
       "             ('Radial.radial.alpha',\n",
       "              tensor([[-1.1308e-02,  9.9375e-01,  1.9933e+00,  2.8751e+00,  3.9855e+00,\n",
       "                        4.9877e+00,  5.9962e+00,  7.0420e+00,  8.0341e+00,  8.9842e+00,\n",
       "                        1.0026e+01,  1.0980e+01,  1.1943e+01,  1.2973e+01,  1.4010e+01,\n",
       "                        1.5006e+01,  1.5996e+01,  1.6987e+01,  1.7986e+01,  1.9000e+01,\n",
       "                        1.9994e+01,  2.0998e+01,  2.2000e+01,  2.2976e+01,  2.4002e+01,\n",
       "                        2.5001e+01,  2.6018e+01,  2.7000e+01,  2.8012e+01,  2.9005e+01,\n",
       "                        3.0000e+01,  3.1005e+01]])),\n",
       "             ('Radial.radial.beta',\n",
       "              tensor([[1.9944, 1.9999, 1.9389, 1.8781, 1.9796, 1.9735, 1.9933, 1.9942, 1.9996,\n",
       "                       1.9885, 1.9952, 1.9973, 1.9901, 1.9893, 1.9544, 1.9664, 1.9994, 1.9942,\n",
       "                       1.9871, 1.9971, 1.9914, 1.9903, 1.9572, 1.9834, 1.9892, 2.0011, 1.9973,\n",
       "                       1.9980, 1.9904, 1.9991, 1.9898, 1.9605]])),\n",
       "             ('blocks.0.message.Attention.actq.alpha',\n",
       "              tensor([1.0150, 1.0203, 1.0094, 1.1116, 1.1435, 0.9867, 1.0382, 1.0918, 1.0513,\n",
       "                      1.0125, 1.0184, 1.0062, 1.0438, 1.0002, 0.9885, 1.1563, 0.9919, 1.0247,\n",
       "                      0.9921, 1.0536, 1.0285, 1.0118, 1.0222, 0.9859, 1.0181, 1.0876, 0.9815,\n",
       "                      0.9893, 1.0332, 1.0255, 0.9728, 1.0164, 1.0106, 0.9693, 1.0157, 1.0528,\n",
       "                      0.9978, 1.0464, 1.1272, 1.0669, 1.0279, 1.0709, 1.0068, 1.0639, 1.0875,\n",
       "                      1.0157, 1.0654, 1.0363, 1.0015, 1.0344, 1.0052, 1.0949, 1.0574, 1.0440,\n",
       "                      1.0860, 1.0421, 1.0637, 1.0028, 0.9906, 1.0259, 1.0018, 1.0810, 1.0045,\n",
       "                      1.0187, 1.0448, 1.0210, 0.9980, 1.0321, 1.0624, 1.1049, 1.0002, 1.0349,\n",
       "                      1.0770, 1.1212, 0.9781, 1.0443, 1.0315, 1.0767, 1.0820, 0.9993, 1.0911,\n",
       "                      0.9871, 1.0629, 1.0760, 1.0362, 0.9743, 0.9748, 1.0205, 0.9975, 1.0246,\n",
       "                      1.0351, 1.0151, 0.9861, 1.0479, 0.9996, 1.0448, 1.0306, 0.9617, 1.0460,\n",
       "                      1.0172, 0.9935, 1.0163, 1.0005, 1.0243, 1.0492, 1.0038, 1.0376, 1.0571,\n",
       "                      0.9942, 0.9806, 1.0566, 0.9982, 1.0251, 1.0408, 1.0168, 1.0820, 1.0109,\n",
       "                      1.0106, 1.0867, 1.0116, 1.0282, 0.9855, 0.9909, 1.0748, 1.0600, 1.0317,\n",
       "                      1.0527, 1.0217])),\n",
       "             ('blocks.0.message.Attention.actq.beta',\n",
       "              tensor([1.7162, 1.7319, 1.6803, 1.7112, 1.7452, 1.7001, 1.6712, 1.7242, 1.7286,\n",
       "                      1.7385, 1.7232, 1.7048, 1.6772, 1.7069, 1.7124, 1.6565, 1.6891, 1.7119,\n",
       "                      1.7164, 1.7257, 1.6915, 1.7187, 1.6623, 1.6943, 1.7409, 1.7153, 1.6933,\n",
       "                      1.7010, 1.6819, 1.7048, 1.7177, 1.6936, 1.7048, 1.6859, 1.7228, 1.6834,\n",
       "                      1.7110, 1.6948, 1.6868, 1.6852, 1.7166, 1.6899, 1.6987, 1.7549, 1.6693,\n",
       "                      1.7031, 1.6879, 1.7288, 1.6940, 1.7431, 1.6966, 1.7537, 1.7002, 1.7461,\n",
       "                      1.6595, 1.7366, 1.7375, 1.7136, 1.6986, 1.6870, 1.7151, 1.7637, 1.7088,\n",
       "                      1.6708, 1.7108, 1.7258, 1.6953, 1.7633, 1.6876, 1.6850, 1.7174, 1.6925,\n",
       "                      1.7601, 1.7294, 1.6760, 1.7819, 1.6879, 1.7307, 1.7113, 1.7034, 1.6987,\n",
       "                      1.6907, 1.6093, 1.7180, 1.7228, 1.6850, 1.6921, 1.6913, 1.7132, 1.7744,\n",
       "                      1.7018, 1.7232, 1.7062, 1.6849, 1.6959, 1.7320, 1.7302, 1.6871, 1.7367,\n",
       "                      1.7551, 1.7075, 1.6848, 1.6829, 1.7251, 1.7140, 1.6500, 1.6941, 1.7208,\n",
       "                      1.6970, 1.7157, 1.7257, 1.7057, 1.7040, 1.6735, 1.7196, 1.7266, 1.6994,\n",
       "                      1.7063, 1.6730, 1.6899, 1.7388, 1.6923, 1.6879, 1.7074, 1.7519, 1.6721,\n",
       "                      1.6884, 1.7255])),\n",
       "             ('blocks.0.message.Attention.actk.alpha',\n",
       "              tensor([1.0838, 1.0081, 1.0049, 1.0738, 1.0035, 1.1068, 1.1608, 0.9736, 1.1077,\n",
       "                      1.0118, 1.0066, 0.9981, 1.0243, 1.0051, 1.0435, 1.0342, 1.0549, 1.0924,\n",
       "                      0.9873, 1.0402, 1.0872, 1.0707, 1.0531, 1.0797, 1.0476, 0.9981, 1.0170,\n",
       "                      1.1099, 0.9933, 1.0106, 1.0863, 0.9796, 1.0285, 1.0430, 1.0224, 1.1248,\n",
       "                      1.0963, 1.1272, 1.0250, 0.9661, 1.0277, 1.1403, 1.0765, 0.9861, 0.9972,\n",
       "                      0.9950, 1.0516, 0.9913, 1.0245, 1.0367, 0.9862, 1.1267, 1.0139, 1.0139,\n",
       "                      1.0611, 1.0915, 1.0168, 1.0958, 0.9985, 0.9914, 1.0372, 0.9619, 1.0087,\n",
       "                      1.0990, 1.0002, 0.9934, 1.0047, 1.0346, 0.9835, 1.0139, 0.9961, 1.0333,\n",
       "                      1.0562, 0.9962, 1.0022, 1.0249, 0.9902, 0.9996, 1.0401, 1.0447, 0.9955,\n",
       "                      1.0197, 1.0043, 1.0204, 1.1286, 1.0347, 1.0485, 1.0641, 1.0339, 1.0834,\n",
       "                      1.0459, 1.0332, 0.9945, 1.0361, 1.0143, 0.9724, 0.9796, 0.9667, 1.0701,\n",
       "                      1.0424, 1.0435, 1.1138, 1.0356, 1.0181, 1.0044, 1.0048, 0.9983, 0.9918,\n",
       "                      0.9789, 0.9915, 1.0725, 1.0042, 1.0869, 1.0406, 1.1368, 1.0608, 1.0012,\n",
       "                      1.0521, 1.0803, 1.0857, 1.0315, 1.0020, 1.0180, 1.1156, 1.0812, 1.1015,\n",
       "                      1.0453, 1.0058])),\n",
       "             ('blocks.0.message.Attention.actk.beta',\n",
       "              tensor([1.7034, 1.7122, 1.7017, 1.7289, 1.7038, 1.7376, 1.7391, 1.7032, 1.7490,\n",
       "                      1.6916, 1.6993, 1.7245, 1.7011, 1.6884, 1.6593, 1.6848, 1.6792, 1.7143,\n",
       "                      1.7078, 1.7220, 1.7381, 1.6883, 1.7358, 1.7500, 1.7462, 1.6984, 1.7169,\n",
       "                      1.7535, 1.6925, 1.6860, 1.7638, 1.7389, 1.7391, 1.7345, 1.7394, 1.7692,\n",
       "                      1.7403, 1.6341, 1.7299, 1.7337, 1.6379, 1.7333, 1.6311, 1.6747, 1.7039,\n",
       "                      1.6877, 1.7342, 1.7354, 1.6879, 1.7299, 1.7095, 1.7174, 1.6864, 1.7172,\n",
       "                      1.7152, 1.7259, 1.6761, 1.7464, 1.6915, 1.7000, 1.6653, 1.7146, 1.6967,\n",
       "                      1.6852, 1.7248, 1.6968, 1.6955, 1.7479, 1.6821, 1.7077, 1.7018, 1.7372,\n",
       "                      1.7438, 1.7031, 1.6758, 1.7276, 1.7183, 1.7113, 1.6816, 1.7077, 1.7259,\n",
       "                      1.7217, 1.7091, 1.7236, 1.7513, 1.6624, 1.6951, 1.7632, 1.6650, 1.6874,\n",
       "                      1.6561, 1.7061, 1.6975, 1.7189, 1.7279, 1.6776, 1.6788, 1.7119, 1.6718,\n",
       "                      1.6669, 1.7348, 1.5542, 1.6760, 1.7171, 1.7014, 1.6705, 1.6930, 1.7018,\n",
       "                      1.7230, 1.6977, 1.7092, 1.6901, 1.7168, 1.6799, 1.6514, 1.7734, 1.7390,\n",
       "                      1.6515, 1.7055, 1.7642, 1.7418, 1.6928, 1.7327, 1.7572, 1.7314, 1.7112,\n",
       "                      1.6140, 1.6979])),\n",
       "             ('blocks.0.message.Attention.actv.alpha',\n",
       "              tensor([1.0031, 1.0137, 1.0251, 1.0088, 1.0009, 0.9843, 0.9929, 1.0058, 1.0070,\n",
       "                      1.0025, 1.0115, 0.9966, 1.0020, 0.9941, 1.0005, 0.9986, 1.0176, 1.0031,\n",
       "                      0.9968, 1.0125, 1.0021, 1.0000, 1.0031, 0.9969, 1.0221, 0.9758, 1.0050,\n",
       "                      1.0003, 0.9987, 1.0019, 1.0036, 0.9903, 1.0051, 1.0380, 1.0376, 0.9963,\n",
       "                      0.9927, 1.0053, 0.9888, 1.0014, 0.9937, 1.0187, 1.0032, 1.0043, 0.9949,\n",
       "                      1.0001, 1.0027, 0.9846, 1.0126, 0.9986, 0.9974, 1.0621, 0.9903, 0.9961,\n",
       "                      1.0020, 1.0105, 1.0034, 0.9899, 0.9895, 1.0152, 1.0365, 0.9994, 1.0052,\n",
       "                      1.0105, 1.0196, 1.0208, 1.0168, 1.0319, 0.9987, 1.0218, 1.0042, 0.9922,\n",
       "                      0.9972, 1.0047, 1.0116, 1.0027, 1.0305, 1.0029, 0.9983, 1.0275, 1.0744,\n",
       "                      1.0033, 0.9979, 1.0044, 1.0062, 0.9954, 1.0023, 0.9994, 1.0140, 1.0066,\n",
       "                      0.9945, 0.9965, 0.9985, 1.0013, 0.9961, 0.9980, 0.9949, 0.9919, 1.0022,\n",
       "                      1.0010, 0.9999, 1.0022, 1.0022, 0.9887, 1.0015, 0.9999, 1.0032, 1.0006,\n",
       "                      0.9989, 0.9898, 0.9908, 0.9763, 1.0320, 1.0039, 1.0090, 1.0003, 1.0074,\n",
       "                      1.0020, 0.9914, 1.0148, 0.9976, 0.9976, 0.9938, 0.9989, 0.9991, 1.0002,\n",
       "                      0.9945, 0.9980, 1.0061, 0.9894, 0.9970, 0.9982, 0.9979, 1.0052, 1.0020,\n",
       "                      0.9902, 0.9906, 1.0034, 1.0037, 0.9976, 1.0197, 1.0021, 1.0014, 1.0313,\n",
       "                      1.0218, 0.9929, 1.0170, 1.0160, 0.9968, 1.0026, 0.9916, 1.0175, 1.0515,\n",
       "                      1.0095, 1.0185, 1.0018, 0.9941, 0.9920, 1.0292, 0.9974, 1.0018, 1.0858,\n",
       "                      1.0193, 0.9985, 1.0016, 0.9858, 0.9938, 1.0035, 1.0120, 1.0038, 1.0662,\n",
       "                      1.0003, 1.0001, 1.0362, 0.9944, 1.0520, 1.0562, 0.9875, 1.0041, 0.9876,\n",
       "                      0.9960, 1.0043, 0.9956, 1.0247, 1.0048, 1.0109, 1.0008, 1.0035, 0.9886,\n",
       "                      0.9968, 1.0336, 1.0461, 0.9981, 0.9959, 1.0288, 1.0016, 0.9957, 1.0247,\n",
       "                      1.0029, 1.0180, 1.0026, 0.9927, 0.9942, 1.0053, 1.0006, 0.9993, 0.9924,\n",
       "                      1.0900, 1.0134, 1.0023, 1.0021, 1.0076, 1.0002, 0.9872, 0.9893, 0.9818,\n",
       "                      1.0106, 1.0039, 1.0036, 1.0031, 0.9973, 1.0020, 0.9827, 0.9969, 0.9918,\n",
       "                      1.0666, 1.0511, 1.0024, 0.9932, 0.9969, 0.9935, 1.0037, 1.0007, 1.0041,\n",
       "                      1.0050, 1.0060, 0.9889, 1.0105, 1.0016, 1.0465, 1.0374, 1.0029, 1.0757,\n",
       "                      1.0046, 1.0033, 1.0078, 1.0027, 1.0347, 1.0065, 0.9972, 1.0018, 1.0137,\n",
       "                      0.9927, 1.0029, 0.9998, 1.0372])),\n",
       "             ('blocks.0.message.Attention.actv.beta',\n",
       "              tensor([1.7079, 1.6877, 1.6885, 1.7091, 1.7033, 1.7058, 1.7109, 1.6968, 1.7043,\n",
       "                      1.7036, 1.6996, 1.6940, 1.7040, 1.6936, 1.7026, 1.7015, 1.6856, 1.6984,\n",
       "                      1.7005, 1.7061, 1.7014, 1.7017, 1.7053, 1.7076, 1.6552, 1.7177, 1.6998,\n",
       "                      1.7037, 1.7041, 1.6987, 1.7062, 1.7056, 1.7146, 1.7252, 1.7186, 1.6981,\n",
       "                      1.6865, 1.7051, 1.6926, 1.7038, 1.7059, 1.6974, 1.7026, 1.7060, 1.7014,\n",
       "                      1.6994, 1.7048, 1.7132, 1.6854, 1.6982, 1.7044, 1.7751, 1.7000, 1.7065,\n",
       "                      1.7036, 1.6966, 1.7053, 1.6912, 1.7047, 1.7019, 1.6983, 1.7051, 1.7038,\n",
       "                      1.7041, 1.7164, 1.7164, 1.6838, 1.7174, 1.7004, 1.7028, 1.7038, 1.7077,\n",
       "                      1.6994, 1.6992, 1.7046, 1.6983, 1.7113, 1.7054, 1.6910, 1.7328, 1.7525,\n",
       "                      1.7049, 1.6935, 1.7068, 1.7026, 1.6974, 1.7061, 1.7021, 1.6888, 1.6997,\n",
       "                      1.7052, 1.6969, 1.7007, 1.7039, 1.6955, 1.6969, 1.7015, 1.6878, 1.6975,\n",
       "                      1.6945, 1.7039, 1.7016, 1.7046, 1.6908, 1.7032, 1.7037, 1.7020, 1.7057,\n",
       "                      1.7040, 1.7099, 1.6859, 1.7178, 1.7233, 1.6993, 1.6873, 1.7027, 1.6964,\n",
       "                      1.7034, 1.6936, 1.7183, 1.7066, 1.7168, 1.7070, 1.7018, 1.7013, 1.6987,\n",
       "                      1.7009, 1.7032, 1.6968, 1.6987, 1.7056, 1.7072, 1.6998, 1.7088, 1.7044,\n",
       "                      1.7028, 1.7092, 1.7062, 1.7007, 1.6905, 1.7214, 1.7029, 1.7036, 1.7214,\n",
       "                      1.7278, 1.7062, 1.7090, 1.7112, 1.6990, 1.7038, 1.7104, 1.7154, 1.7564,\n",
       "                      1.7047, 1.7109, 1.7000, 1.7050, 1.6987, 1.7071, 1.7034, 1.6957, 1.7426,\n",
       "                      1.7202, 1.7032, 1.7013, 1.6937, 1.6956, 1.6988, 1.6793, 1.6986, 1.7047,\n",
       "                      1.7022, 1.7028, 1.7109, 1.6973, 1.7085, 1.7357, 1.7054, 1.7021, 1.7054,\n",
       "                      1.7063, 1.6993, 1.7004, 1.7375, 1.6888, 1.7108, 1.7029, 1.7068, 1.7021,\n",
       "                      1.6930, 1.6961, 1.6956, 1.7020, 1.7192, 1.6061, 1.7041, 1.6987, 1.7100,\n",
       "                      1.7066, 1.7144, 1.7063, 1.6993, 1.7021, 1.6995, 1.7026, 1.7048, 1.6882,\n",
       "                      1.7016, 1.6931, 1.7053, 1.7011, 1.7050, 1.7047, 1.7035, 1.6967, 1.7247,\n",
       "                      1.7157, 1.7069, 1.6996, 1.7052, 1.6983, 1.6996, 1.6847, 1.7078, 1.7006,\n",
       "                      1.7280, 1.6903, 1.6995, 1.7044, 1.7134, 1.6948, 1.6980, 1.7059, 1.6983,\n",
       "                      1.7081, 1.6964, 1.7028, 1.6983, 1.7003, 1.7408, 1.7137, 1.7047, 1.7276,\n",
       "                      1.7058, 1.6976, 1.7027, 1.7063, 1.7244, 1.7036, 1.6989, 1.7028, 1.7019,\n",
       "                      1.7061, 1.7039, 1.7087, 1.7476])),\n",
       "             ('blocks.0.message.Attention.acta.alpha',\n",
       "              tensor([0.9839, 0.9929, 0.9699, 0.9624, 0.9707, 1.0158, 1.0224, 1.0035, 1.0330,\n",
       "                      1.0292, 1.0173, 0.9887, 1.0174, 1.0737, 0.9810, 0.9906, 0.9835, 1.0658,\n",
       "                      0.9787, 0.9508, 0.9894, 0.9858, 0.9679, 0.9933, 1.0548, 0.9983, 0.9644,\n",
       "                      1.0009, 0.9911, 0.9889, 0.9946, 1.0001, 1.0282, 0.9829, 1.0226, 1.0104,\n",
       "                      1.0188, 0.9811, 1.0308, 1.0387, 1.0099, 0.9930, 0.9890, 1.0298, 0.9743,\n",
       "                      1.0387, 1.0113, 0.9858, 0.9930, 0.9866, 1.0741, 0.9851, 1.0105, 1.0228,\n",
       "                      0.9500, 0.9727, 0.9998, 1.0236, 1.0403, 0.9849, 1.0301, 0.9785, 1.0545,\n",
       "                      0.9937, 1.0046, 0.9732, 1.0241, 1.0132, 0.9873, 0.9642, 1.0486, 0.9438,\n",
       "                      0.9657, 0.9382, 1.0026, 1.0069, 0.9718, 1.0185, 0.9602, 0.9680, 0.9715,\n",
       "                      0.9813, 1.0168, 1.0774, 0.9789, 0.9927, 1.0083, 1.0165, 1.0237, 0.9743,\n",
       "                      1.0238, 1.0011, 1.0114, 1.0307, 1.0050, 0.9776, 1.0548, 1.0292, 1.0157,\n",
       "                      0.9680, 1.0715, 1.0041, 0.9887, 1.0455, 1.0151, 1.0218, 1.0103, 1.0332,\n",
       "                      0.9735, 1.0231, 0.9902, 1.0048, 0.9868, 0.9557, 0.9887, 1.0211, 1.0010,\n",
       "                      0.9842, 1.0358, 0.9824, 0.9861, 0.9852, 1.0177, 0.9771, 1.0232, 0.9657,\n",
       "                      1.0056, 0.9936, 1.0160, 0.9906, 1.0109, 1.0017, 0.9869, 0.9978, 1.0140,\n",
       "                      0.9862, 0.9958, 1.0018, 1.0145, 1.0000, 1.0097, 1.0003, 1.0017, 1.0062,\n",
       "                      1.0281, 1.0017, 1.0076, 1.0092, 1.0032, 1.0349, 1.0222, 1.0136, 0.9983,\n",
       "                      1.0125, 1.0030, 1.0002, 1.0035, 1.0091, 1.0040, 1.0061, 0.9912, 1.0322,\n",
       "                      1.0061, 1.0027, 0.9996, 1.0144, 1.0064, 1.0174, 1.0169, 1.0216, 1.0063,\n",
       "                      1.0054, 0.9966, 0.9987, 1.0009, 1.0013, 0.9945, 1.0081, 1.0083, 0.9995,\n",
       "                      1.0035, 0.9962, 1.0060, 0.9931, 0.9945, 1.0030, 1.0167, 1.0074, 0.9978,\n",
       "                      1.0038, 1.0058, 1.0027, 1.0031, 1.0053, 1.0183, 1.0021, 0.9974, 1.0066,\n",
       "                      1.0082, 1.0062, 1.0134, 0.9971, 1.0220, 1.0091, 1.0103, 1.0094, 1.0398,\n",
       "                      1.0000, 1.0036, 1.0068, 0.9986, 1.0008, 1.0023, 1.0019, 1.0041, 1.0057,\n",
       "                      0.9989, 1.0021, 1.0095, 1.0150, 1.0105, 0.9918, 1.0305, 1.0131, 1.0052,\n",
       "                      1.0167, 1.0099, 1.0032, 1.0059, 1.0408, 1.0226, 1.0245, 1.0020, 1.0195,\n",
       "                      1.0038, 0.9920, 0.9973, 0.9991, 1.0005, 1.0089, 1.0010, 1.0238, 1.0137,\n",
       "                      1.0018, 1.0065, 1.0315, 1.0006, 1.0042, 1.0001, 1.0083, 0.9883, 1.0061,\n",
       "                      1.0016, 0.9935, 1.0067, 1.0036])),\n",
       "             ('blocks.0.message.Attention.acta.beta',\n",
       "              tensor([1.7210, 1.7097, 1.7044, 1.6644, 1.6872, 1.7516, 1.6533, 1.7224, 1.6929,\n",
       "                      1.6617, 1.7450, 1.7010, 1.6579, 1.7143, 1.6840, 1.6776, 1.6564, 1.7592,\n",
       "                      1.6741, 1.6974, 1.6858, 1.7038, 1.7114, 1.7062, 1.7871, 1.7338, 1.7267,\n",
       "                      1.6903, 1.7233, 1.6899, 1.6792, 1.7451, 1.7338, 1.6860, 1.6828, 1.6818,\n",
       "                      1.7004, 1.6958, 1.6683, 1.7466, 1.6420, 1.6807, 1.7122, 1.6951, 1.6839,\n",
       "                      1.7693, 1.7206, 1.6749, 1.7429, 1.6616, 1.7744, 1.6653, 1.7167, 1.6966,\n",
       "                      1.6963, 1.7141, 1.7622, 1.7377, 1.6498, 1.7270, 1.7042, 1.6918, 1.6500,\n",
       "                      1.6888, 1.7274, 1.6526, 1.7159, 1.6893, 1.7334, 1.6845, 1.6444, 1.7303,\n",
       "                      1.6379, 1.6590, 1.6836, 1.6741, 1.6760, 1.6205, 1.6768, 1.7027, 1.7046,\n",
       "                      1.7235, 1.7514, 1.7305, 1.6748, 1.7210, 1.7443, 1.7015, 1.7101, 1.7234,\n",
       "                      1.6642, 1.6688, 1.6725, 1.7178, 1.7117, 1.7006, 1.7323, 1.7771, 1.6945,\n",
       "                      1.7071, 1.7128, 1.7461, 1.6803, 1.7282, 1.7545, 1.6636, 1.7226, 1.6593,\n",
       "                      1.7090, 1.7120, 1.7198, 1.7305, 1.6786, 1.7005, 1.7192, 1.6496, 1.6808,\n",
       "                      1.7108, 1.7009, 1.6930, 1.6927, 1.7145, 1.7273, 1.6584, 1.7076, 1.6876,\n",
       "                      1.7067, 1.7023, 1.6805, 1.6705, 1.6432, 1.6978, 1.7174, 1.6982, 1.6962,\n",
       "                      1.7001, 1.7005, 1.7042, 1.6708, 1.6892, 1.6616, 1.7240, 1.7035, 1.6867,\n",
       "                      1.6824, 1.6726, 1.6841, 1.6887, 1.7144, 1.6412, 1.6711, 1.6520, 1.6763,\n",
       "                      1.7091, 1.6964, 1.7030, 1.6969, 1.6689, 1.6486, 1.7135, 1.6953, 1.6633,\n",
       "                      1.7175, 1.7047, 1.7170, 1.7030, 1.6822, 1.7089, 1.6800, 1.7524, 1.7132,\n",
       "                      1.7000, 1.7108, 1.7060, 1.6982, 1.6971, 1.6716, 1.7178, 1.7113, 1.7030,\n",
       "                      1.7180, 1.6696, 1.7120, 1.6850, 1.7030, 1.6935, 1.7138, 1.7359, 1.6972,\n",
       "                      1.7028, 1.6977, 1.7015, 1.7110, 1.6788, 1.7210, 1.6931, 1.6638, 1.6761,\n",
       "                      1.7127, 1.7199, 1.7109, 1.6833, 1.7180, 1.6242, 1.7029, 1.7014, 1.7098,\n",
       "                      1.6736, 1.6996, 1.7020, 1.6902, 1.6831, 1.7025, 1.6932, 1.7071, 1.6986,\n",
       "                      1.7015, 1.7050, 1.7141, 1.6748, 1.7236, 1.7093, 1.6721, 1.6781, 1.7190,\n",
       "                      1.6621, 1.6983, 1.7032, 1.6997, 1.7264, 1.7391, 1.7227, 1.7013, 1.7532,\n",
       "                      1.7071, 1.7035, 1.7131, 1.7120, 1.6871, 1.7201, 1.6974, 1.7326, 1.6840,\n",
       "                      1.7010, 1.7298, 1.6701, 1.7015, 1.7083, 1.6924, 1.6887, 1.6869, 1.6976,\n",
       "                      1.7016, 1.6803, 1.6956, 1.7168])),\n",
       "             ('blocks.0.message.Attention.lq.weight',\n",
       "              tensor([[-0.1126,  0.0507,  0.0713,  ...,  0.0882, -0.1011, -0.0690],\n",
       "                      [-0.0082,  0.0734, -0.2022,  ..., -0.0409,  0.0985, -0.0795],\n",
       "                      [-0.0367, -0.0120,  0.0676,  ..., -0.1399,  0.1116,  0.0924],\n",
       "                      ...,\n",
       "                      [ 0.1986,  0.0503, -0.0542,  ...,  0.0148, -0.0269, -0.0841],\n",
       "                      [-0.0932, -0.1261,  0.0632,  ...,  0.0453, -0.0199,  0.2305],\n",
       "                      [ 0.0490,  0.1037,  0.0916,  ...,  0.1542,  0.1934, -0.0787]])),\n",
       "             ('blocks.0.message.Attention.lq.bias',\n",
       "              tensor([ 0.0200,  0.0259, -0.0094,  0.0501,  0.0375, -0.0104,  0.0283,  0.0459,\n",
       "                       0.0498, -0.0020, -0.0085,  0.0210,  0.0068, -0.0004, -0.0172,  0.0323,\n",
       "                       0.0140,  0.0224, -0.0286,  0.0329,  0.0285, -0.0175, -0.0311, -0.0062,\n",
       "                       0.0304,  0.0231, -0.0140, -0.0063, -0.0028,  0.0043, -0.0080, -0.0238,\n",
       "                       0.0123, -0.0340, -0.0041,  0.0049, -0.0157,  0.0148,  0.0521,  0.0090,\n",
       "                       0.0455,  0.0249, -0.0050,  0.0264,  0.0459, -0.0040,  0.0389,  0.0051,\n",
       "                       0.0458,  0.0268,  0.0078,  0.0437, -0.0023,  0.0214,  0.0393,  0.0278,\n",
       "                       0.0429, -0.0016, -0.0080,  0.0256, -0.0059,  0.0394,  0.0222, -0.0017,\n",
       "                       0.0469,  0.0236, -0.0021, -0.0064,  0.0241,  0.0269, -0.0282, -0.0068,\n",
       "                       0.0448,  0.1189, -0.0271,  0.0402,  0.0283,  0.0144,  0.0234,  0.0022,\n",
       "                       0.0638, -0.0056, -0.0107,  0.0557,  0.0233, -0.0200, -0.0210,  0.0019,\n",
       "                      -0.0160,  0.0336,  0.0173, -0.0674, -0.0268, -0.0056, -0.0137,  0.0101,\n",
       "                       0.0261, -0.0056,  0.0306,  0.0167, -0.0087,  0.0002,  0.0091,  0.0261,\n",
       "                       0.0165,  0.0169,  0.0402,  0.0299, -0.0113, -0.0161,  0.0362, -0.0025,\n",
       "                      -0.0036, -0.0174,  0.0192,  0.0547,  0.0162, -0.0137,  0.0291, -0.0036,\n",
       "                       0.0292, -0.0302, -0.0012,  0.0327,  0.0093, -0.0107,  0.0391, -0.0097])),\n",
       "             ('blocks.0.message.Attention.lk.weight',\n",
       "              tensor([[ 0.1850, -0.1483,  0.1567,  ..., -0.0490,  0.1168, -0.1755],\n",
       "                      [ 0.0622,  0.0998, -0.0453,  ...,  0.0126, -0.1251,  0.0541],\n",
       "                      [-0.0265,  0.1143,  0.1423,  ..., -0.1363, -0.0501, -0.1331],\n",
       "                      ...,\n",
       "                      [-0.0224, -0.1032,  0.3657,  ..., -0.1964,  0.0234,  0.1297],\n",
       "                      [-0.1345, -0.0163,  0.0042,  ..., -0.1118, -0.0723, -0.0830],\n",
       "                      [-0.1730, -0.1223, -0.0442,  ..., -0.0081,  0.0365, -0.0810]])),\n",
       "             ('blocks.0.message.Attention.lk.bias',\n",
       "              tensor([ 0.0326,  0.0110,  0.0009,  0.0479,  0.0025,  0.0924,  0.1358, -0.0077,\n",
       "                       0.0895, -0.0175, -0.0201,  0.0086,  0.0036, -0.0017, -0.0080,  0.0049,\n",
       "                      -0.0027,  0.0580, -0.0267,  0.0336,  0.0731,  0.0580,  0.0381,  0.0798,\n",
       "                       0.0481, -0.0039,  0.0140,  0.0722, -0.0064,  0.0206,  0.0509, -0.0363,\n",
       "                       0.0120,  0.0614,  0.0359,  0.0903,  0.0531,  0.0694,  0.0281,  0.0106,\n",
       "                      -0.0200,  0.1038, -0.0146,  0.0027,  0.0074, -0.0062,  0.0397, -0.0144,\n",
       "                      -0.0088,  0.0286, -0.0062,  0.0953,  0.0149,  0.0160,  0.0389,  0.0396,\n",
       "                       0.0157,  0.0246, -0.0129,  0.0110, -0.0023,  0.0066,  0.0088, -0.0153,\n",
       "                       0.0076, -0.0089, -0.0128,  0.0331, -0.0157, -0.0028, -0.0032,  0.0441,\n",
       "                       0.0432, -0.0155, -0.0149,  0.0209, -0.0362, -0.0053,  0.0511,  0.0083,\n",
       "                      -0.0300,  0.0055,  0.0056,  0.0215,  0.1168, -0.0070,  0.0395,  0.0431,\n",
       "                      -0.0044,  0.0246, -0.0010,  0.0388, -0.0052,  0.0191, -0.0302, -0.0140,\n",
       "                      -0.0100,  0.0021,  0.0199,  0.0159,  0.0475,  0.0070,  0.0275,  0.0146,\n",
       "                       0.0060,  0.0032, -0.0105, -0.0134, -0.0207,  0.0159,  0.0357,  0.0058,\n",
       "                       0.0250, -0.0020,  0.0681,  0.0464,  0.0236, -0.0008,  0.0528,  0.0746,\n",
       "                       0.0268,  0.0060,  0.0019,  0.0709,  0.0334,  0.0506,  0.0184, -0.0084])),\n",
       "             ('blocks.0.message.Attention.lv.weight',\n",
       "              tensor([[-0.0973, -0.0731, -0.0542,  ..., -0.0722,  0.0732, -0.1253],\n",
       "                      [-0.0480,  0.0984,  0.0291,  ...,  0.0418, -0.0765, -0.0728],\n",
       "                      [ 0.0117, -0.0304,  0.0329,  ..., -0.0488, -0.1239, -0.0180],\n",
       "                      ...,\n",
       "                      [ 0.1041,  0.0061,  0.0816,  ..., -0.0271, -0.0371, -0.0296],\n",
       "                      [-0.1197, -0.0237,  0.1036,  ..., -0.0316, -0.0776, -0.0409],\n",
       "                      [ 0.1500, -0.1008,  0.0097,  ..., -0.1190,  0.0731,  0.1207]])),\n",
       "             ('blocks.0.message.Attention.lv.bias',\n",
       "              tensor([ 2.2853e-03, -1.1259e-02,  2.5351e-03,  5.1770e-03,  1.4848e-03,\n",
       "                       6.7840e-03, -7.4533e-03, -1.6019e-02,  5.2480e-04,  5.7333e-03,\n",
       "                       6.5398e-03, -8.4026e-03,  2.8259e-03, -8.9695e-03,  1.6839e-03,\n",
       "                      -1.1529e-02, -1.0025e-02, -3.1834e-03, -1.0060e-03, -5.6633e-03,\n",
       "                      -1.2164e-02,  1.2245e-03,  4.4192e-03, -3.4266e-04, -5.9711e-03,\n",
       "                       8.4183e-03,  1.1743e-04, -8.0807e-03, -3.5026e-03, -5.1659e-03,\n",
       "                       5.1446e-03, -5.2520e-03,  1.1225e-02,  2.2007e-02,  2.5889e-02,\n",
       "                      -3.6887e-03, -6.2906e-03,  2.5210e-03, -1.1751e-02,  2.9910e-03,\n",
       "                       2.7723e-03,  1.7580e-02,  2.1460e-02,  4.4294e-03,  9.8136e-04,\n",
       "                       2.8209e-04,  3.6122e-03, -6.8880e-03, -2.0577e-02,  2.3396e-03,\n",
       "                      -5.0397e-03,  2.7864e-02, -7.3302e-03,  6.7516e-03,  3.5053e-03,\n",
       "                       1.7989e-02, -1.6342e-03, -1.0566e-02, -1.2271e-02,  2.1891e-03,\n",
       "                       2.6298e-02,  4.4529e-03,  2.7884e-03,  7.5478e-03,  3.5718e-02,\n",
       "                       7.7492e-03, -9.2223e-03,  1.0996e-02, -1.0169e-03,  1.1712e-02,\n",
       "                       5.6320e-03, -1.1059e-04, -1.1313e-02, -1.1405e-02, -1.6998e-03,\n",
       "                      -1.0478e-02,  1.1117e-02,  3.2788e-03, -7.3428e-04,  1.6078e-02,\n",
       "                       4.8136e-02,  4.3896e-03, -2.4673e-03,  5.7127e-03,  8.2148e-05,\n",
       "                      -2.0426e-03,  3.8278e-03, -6.5758e-04, -8.9387e-03,  5.2416e-03,\n",
       "                      -5.4518e-03, -1.5831e-02,  1.2280e-03,  2.6105e-03, -6.6576e-03,\n",
       "                      -2.4705e-03,  2.4593e-03,  1.2000e-03, -1.2871e-02,  1.0457e-02,\n",
       "                       1.1464e-03, -1.5346e-02,  3.7351e-03, -8.9948e-03,  1.5568e-03,\n",
       "                      -6.9189e-03, -2.6614e-03,  1.2610e-03, -1.0508e-02,  1.1737e-03,\n",
       "                      -1.0644e-02,  4.9081e-04,  3.1319e-02, -1.2320e-02, -5.0344e-03,\n",
       "                       5.6901e-03,  1.2490e-04,  2.8247e-03, -1.1907e-02,  1.1791e-02,\n",
       "                       4.6244e-03,  6.0571e-03,  2.2797e-03,  6.3919e-04,  8.7510e-04,\n",
       "                      -5.0592e-03,  3.2509e-03, -7.7626e-03, -1.0894e-02, -2.1235e-03,\n",
       "                       3.1713e-04, -9.7614e-04, -4.2896e-04,  5.1118e-03,  3.2219e-03,\n",
       "                      -6.2250e-03,  4.2237e-03,  3.8423e-03, -6.2167e-03,  5.2667e-03,\n",
       "                      -5.0501e-03,  3.3918e-03,  2.7414e-03,  6.6127e-02,  2.3287e-02,\n",
       "                      -6.1773e-03,  3.2268e-03, -1.3274e-05, -1.7946e-03,  3.0046e-03,\n",
       "                       3.5980e-03,  8.7641e-03,  5.6879e-02,  6.9591e-03,  8.6172e-03,\n",
       "                      -7.0914e-03, -5.2772e-03, -7.5598e-03,  4.9321e-03,  5.1451e-03,\n",
       "                       3.2628e-03,  5.9966e-02,  1.6432e-02, -1.7394e-03,  1.5443e-03,\n",
       "                      -7.4899e-03, -7.9854e-03, -1.8435e-03, -1.0972e-02, -1.4371e-03,\n",
       "                       4.6086e-03,  1.9944e-03,  4.7150e-04,  9.1422e-03, -7.0855e-03,\n",
       "                      -6.5621e-04,  5.0278e-02, -1.5054e-03, -1.3192e-02,  9.0749e-03,\n",
       "                       4.6130e-03, -9.7064e-03,  2.1372e-03,  1.3844e-02,  6.3275e-03,\n",
       "                       9.5220e-03,  1.0117e-03,  4.3973e-03, -4.8511e-03,  5.2559e-03,\n",
       "                       1.5390e-02,  2.1132e-02, -6.0532e-04,  6.1773e-04,  1.2633e-02,\n",
       "                       3.4926e-03, -5.9821e-03,  9.1419e-03,  4.6623e-03,  4.8925e-03,\n",
       "                       3.4446e-03,  1.5837e-03, -1.1015e-03, -1.0551e-02,  1.2980e-03,\n",
       "                       2.2879e-04, -8.5541e-03,  1.9080e-02,  7.3137e-03,  3.7428e-03,\n",
       "                      -2.5589e-03,  5.9333e-03,  1.7410e-03, -3.6259e-03, -1.1934e-02,\n",
       "                       1.1601e-02,  8.3129e-03,  4.9703e-03, -6.4731e-03,  3.9226e-03,\n",
       "                      -2.7304e-03, -2.8672e-03, -7.9315e-03,  3.3686e-03, -6.2091e-03,\n",
       "                       2.0024e-02,  7.9992e-03, -1.0230e-02,  1.0870e-03, -4.6745e-03,\n",
       "                      -6.1042e-03, -3.4174e-03,  1.4016e-02,  1.2081e-03,  3.5382e-03,\n",
       "                      -1.0527e-02,  5.9369e-03,  6.6097e-03, -5.0357e-03,  3.4384e-02,\n",
       "                       1.7984e-02,  4.0125e-03,  9.4877e-02,  5.4958e-03, -1.4103e-03,\n",
       "                      -1.8826e-03,  4.0192e-03,  5.4763e-03, -3.6945e-03, -2.0585e-03,\n",
       "                       7.3538e-03, -8.5863e-03,  1.6860e-03,  3.4527e-03, -3.4920e-03,\n",
       "                       1.2295e-02])),\n",
       "             ('blocks.0.message.Attention.la.weight',\n",
       "              tensor([[-0.0929,  0.0015, -0.0249,  ..., -0.0421, -0.0977,  0.0389],\n",
       "                      [-0.1258,  0.0541, -0.0314,  ...,  0.0889, -0.0705,  0.0221],\n",
       "                      [-0.0283,  0.1221,  0.1096,  ...,  0.0372,  0.1066,  0.0316],\n",
       "                      ...,\n",
       "                      [-0.0434,  0.0350, -0.1141,  ..., -0.0535,  0.0908, -0.0714],\n",
       "                      [ 0.0440, -0.1161, -0.0570,  ...,  0.1034, -0.0214, -0.0262],\n",
       "                      [-0.0277, -0.1206, -0.0890,  ...,  0.0596,  0.1054,  0.0014]])),\n",
       "             ('blocks.0.message.Attention.la.bias',\n",
       "              tensor([-1.3545e-02, -6.2046e-03, -1.1680e-03, -2.5379e-02,  5.0150e-04,\n",
       "                       4.1605e-03, -1.4523e-02,  3.3635e-02,  3.9722e-02, -6.0241e-02,\n",
       "                      -2.4156e-02,  6.0767e-03, -4.5200e-02,  4.0129e-02, -1.1114e-02,\n",
       "                      -1.4961e-02, -2.5557e-02, -3.0940e-02, -2.5962e-02,  1.3719e-03,\n",
       "                       2.3093e-02, -2.0380e-02, -2.2125e-03,  2.1857e-02,  9.1777e-02,\n",
       "                       1.2828e-02,  1.9936e-02, -3.5120e-03,  7.2195e-03, -9.0602e-03,\n",
       "                       3.1698e-03,  2.3159e-02,  4.9288e-02, -1.5474e-02,  5.6809e-02,\n",
       "                      -2.2844e-03, -2.2042e-03,  4.2561e-02, -4.0704e-02,  3.7710e-02,\n",
       "                      -4.3380e-02,  2.1375e-02, -2.6387e-02,  1.6142e-02, -7.3592e-03,\n",
       "                       4.5967e-02,  3.0947e-03, -5.8248e-02,  5.1988e-03, -3.9175e-02,\n",
       "                       8.7348e-02,  2.2550e-02,  5.7817e-02, -6.1877e-02, -1.7499e-04,\n",
       "                      -8.5483e-03, -1.9505e-02, -4.2614e-02, -1.1511e-01, -3.6028e-02,\n",
       "                       2.5033e-03,  1.7559e-02, -4.3024e-02,  3.8469e-03,  1.8227e-02,\n",
       "                      -4.2974e-02, -2.4473e-02, -2.4575e-02,  3.1051e-02,  2.6995e-02,\n",
       "                      -7.6133e-02, -2.0485e-03, -9.1308e-03, -1.3788e-02, -1.6335e-02,\n",
       "                       3.7782e-02, -2.1060e-03, -4.2391e-02, -1.0603e-02,  7.8912e-03,\n",
       "                      -5.5216e-03, -6.6069e-03,  8.0654e-02,  7.7617e-02,  4.2340e-03,\n",
       "                      -1.5863e-02,  2.5341e-02, -3.3248e-02,  1.6611e-02,  1.9479e-02,\n",
       "                      -5.6363e-02, -4.9224e-02, -4.3006e-02,  5.7131e-02, -2.8479e-02,\n",
       "                      -1.4334e-02,  5.3645e-02,  5.1902e-02, -2.3721e-02, -3.5772e-02,\n",
       "                       4.3461e-02,  2.9911e-03, -4.5945e-02,  4.4374e-02,  3.3254e-02,\n",
       "                      -7.0334e-02,  1.5127e-02, -5.8910e-02, -4.0867e-03,  5.4720e-02,\n",
       "                       3.1257e-02, -6.4047e-03,  1.2057e-03,  1.6816e-03, -2.9146e-02,\n",
       "                      -7.0221e-02,  1.0087e-02, -3.3867e-02,  3.1764e-02,  9.7595e-03,\n",
       "                      -3.5562e-02, -3.6753e-04,  1.0093e-02, -2.7619e-02,  2.9597e-02,\n",
       "                       1.5747e-02, -3.8198e-02,  5.5925e-03,  8.2462e-03,  6.7683e-03,\n",
       "                      -8.5430e-03, -6.4323e-03,  1.4714e-03,  4.8964e-03, -5.6332e-03,\n",
       "                       4.7103e-03,  2.0069e-04, -6.3458e-03, -4.8005e-03, -5.9152e-03,\n",
       "                      -6.5285e-03, -1.5619e-03,  5.6439e-03, -8.6005e-03, -2.3831e-03,\n",
       "                      -5.0256e-03, -7.7491e-03, -7.9626e-03,  5.8659e-03,  6.9894e-03,\n",
       "                      -1.4616e-02, -5.5834e-03, -4.2779e-03, -1.2763e-02, -8.0721e-03,\n",
       "                       4.3863e-03, -6.6058e-03, -5.4701e-03, -6.3827e-03, -4.6533e-03,\n",
       "                      -6.4668e-03, -4.8088e-03, -6.9894e-03, -5.7624e-03, -6.9488e-03,\n",
       "                       5.7318e-03, -5.0325e-03,  5.6602e-03, -7.0166e-03,  5.6306e-03,\n",
       "                       4.6626e-03, -2.9808e-03,  1.5004e-03,  6.0345e-03, -3.3857e-03,\n",
       "                       5.8293e-03,  8.7937e-03,  5.3774e-03, -1.1860e-02,  5.3566e-03,\n",
       "                      -5.9079e-03,  4.1156e-03,  3.4333e-03, -6.4853e-03,  8.1933e-03,\n",
       "                      -5.7093e-03, -1.3936e-03,  5.5928e-03,  6.7234e-03,  6.6337e-03,\n",
       "                       7.0170e-03, -5.1154e-03, -8.0430e-06, -7.0144e-03, -8.0722e-03,\n",
       "                      -2.9249e-03, -3.5077e-03, -7.2130e-03, -1.1880e-03,  5.8641e-03,\n",
       "                       8.5682e-03, -5.9326e-03, -1.1323e-02, -6.0081e-03,  7.1941e-03,\n",
       "                       7.9727e-03, -1.2124e-02, -5.4026e-03,  6.7872e-03, -8.4904e-03,\n",
       "                      -7.5511e-03, -4.6267e-03,  5.8337e-03, -4.7842e-03, -8.4987e-03,\n",
       "                      -7.3437e-03, -5.1024e-03,  2.6326e-03,  7.0584e-03, -7.2230e-03,\n",
       "                       6.0987e-03, -1.0021e-02,  5.4249e-04,  1.3440e-02,  6.1522e-03,\n",
       "                      -5.4190e-03,  1.3368e-03, -7.1297e-03, -8.1787e-03, -6.5951e-03,\n",
       "                       4.1779e-03,  3.2893e-03, -6.5508e-03,  6.2724e-03, -9.5485e-03,\n",
       "                       3.4293e-03, -6.0644e-03, -4.4434e-03, -4.4591e-03,  2.9160e-03,\n",
       "                      -5.6762e-03, -4.5156e-03, -7.3930e-03,  4.5314e-03,  8.2975e-03,\n",
       "                      -9.1351e-03, -5.7015e-03,  7.8637e-03, -3.6324e-03, -4.1762e-03,\n",
       "                      -2.9220e-03, -5.8379e-03,  5.5522e-03, -5.4513e-03, -7.9614e-03,\n",
       "                       4.0948e-03])),\n",
       "             ('blocks.0.message.Attention.lrbf.weight',\n",
       "              tensor([[ 5.7821e-02, -1.2425e-02,  4.5610e-05,  ..., -5.8093e-02,\n",
       "                       -1.2558e-01,  1.4274e-01],\n",
       "                      [-1.1846e-01, -4.5460e-02,  2.5321e-02,  ...,  1.2106e-01,\n",
       "                        1.2352e-01,  2.3091e-01],\n",
       "                      [ 1.0458e-01,  1.8160e-01, -1.8597e-01,  ..., -5.2092e-02,\n",
       "                        8.0422e-02,  1.2245e-02],\n",
       "                      ...,\n",
       "                      [-4.5953e-02,  5.3422e-02, -8.8853e-02,  ...,  1.0616e-01,\n",
       "                        1.5224e-01, -9.2740e-02],\n",
       "                      [ 6.2014e-03,  1.0600e-01, -2.2192e-01,  ..., -2.6907e-03,\n",
       "                        6.0783e-02,  1.3406e-01],\n",
       "                      [-1.8633e-01, -2.3449e-02,  4.2040e-02,  ..., -1.0535e-01,\n",
       "                        1.8875e-01,  1.8699e-01]])),\n",
       "             ('blocks.0.message.Attention.lkrbf.weight',\n",
       "              tensor([[ 0.1700,  0.0410,  0.1475,  ...,  0.0431, -0.0167, -0.1721],\n",
       "                      [-0.0739, -0.0515,  0.0110,  ..., -0.1903, -0.0832, -0.0337],\n",
       "                      [ 0.0604, -0.0221,  0.0797,  ..., -0.1370,  0.0978, -0.0798],\n",
       "                      ...,\n",
       "                      [ 0.2389, -0.0886,  0.0854,  ...,  0.0878,  0.0071, -0.2074],\n",
       "                      [-0.0688, -0.0678, -0.0760,  ..., -0.1648,  0.0265, -0.0992],\n",
       "                      [ 0.0860,  0.0712, -0.0256,  ...,  0.0762,  0.0940,  0.1343]])),\n",
       "             ('blocks.0.message.Attention.lvrbf.weight',\n",
       "              tensor([[ 0.1219,  0.0183, -0.0689,  ..., -0.0984, -0.0122,  0.1257],\n",
       "                      [ 0.0872, -0.0097,  0.0794,  ..., -0.0444,  0.0218,  0.0662],\n",
       "                      [-0.1045,  0.1225,  0.0845,  ..., -0.0287,  0.0890, -0.0319],\n",
       "                      ...,\n",
       "                      [ 0.0219, -0.0115,  0.0657,  ...,  0.0374,  0.0556,  0.0645],\n",
       "                      [-0.1103, -0.0514,  0.0386,  ..., -0.0391,  0.0938,  0.0172],\n",
       "                      [-0.0268,  0.0276, -0.1184,  ...,  0.0363, -0.0299,  0.0082]])),\n",
       "             ('blocks.0.message.tp.weight',\n",
       "              tensor([ 1.3191e+00, -1.3556e-01,  7.6437e-01, -4.0521e-01, -4.8800e-01,\n",
       "                      -7.4319e-01, -9.3660e-01,  1.0668e+00, -1.3085e+00,  5.4348e-01,\n",
       "                       1.1361e+00, -2.5235e+00, -1.7109e-01, -6.3271e-02,  8.0200e-02,\n",
       "                       2.0631e+00, -2.0083e-01, -8.4842e-01, -2.5424e-01,  1.3460e-01,\n",
       "                       7.7865e-01,  1.0731e+00,  1.6426e+00,  4.0044e-01,  5.4676e-01,\n",
       "                      -1.2856e+00,  1.1005e-01,  6.0872e-01, -9.9504e-01, -1.1380e+00,\n",
       "                       1.1625e+00, -9.1932e-01, -1.2034e+00,  8.0245e-02, -8.1146e-01,\n",
       "                      -4.2469e-01, -1.6033e+00,  5.6944e-01,  1.9908e+00,  1.4783e-01,\n",
       "                      -1.0105e+00,  1.0370e+00, -6.6935e-01,  1.8471e+00, -7.6221e-02,\n",
       "                      -1.7621e+00, -4.8651e-01,  2.0173e-01, -1.6371e+00, -6.6057e-01,\n",
       "                      -7.2547e-01, -2.3407e-01, -7.5385e-01, -5.2301e-01, -3.6518e-01,\n",
       "                      -1.3355e-01,  4.2673e-01,  2.5652e-01, -2.1143e-01,  4.4762e-01,\n",
       "                      -8.1413e-01, -1.8582e+00,  7.6784e-01, -6.1422e-01, -3.6431e-02,\n",
       "                      -8.8033e-01, -1.0463e+00, -8.1266e-01, -1.1694e+00,  6.7625e-01,\n",
       "                       7.8825e-01,  1.6487e+00, -1.0729e+00,  1.2271e+00, -6.8767e-01,\n",
       "                      -3.7598e-01,  5.1401e-01, -8.7427e-01, -8.8156e-01,  6.1316e-01,\n",
       "                       1.0958e+00,  1.4642e-01, -6.5040e-02, -1.4629e+00, -1.4341e-01,\n",
       "                       7.2083e-01,  1.9611e+00, -3.6152e-01, -8.2477e-02,  5.2206e-01,\n",
       "                      -5.7521e-01,  2.8731e-02, -2.5857e-01, -5.3570e-02, -1.9898e+00,\n",
       "                       9.3000e-01,  1.8061e+00,  5.4545e-01, -1.0532e+00,  7.2527e-01,\n",
       "                      -7.3740e-01, -5.7436e-01, -2.2571e-01,  2.6696e-03,  6.7707e-01,\n",
       "                       4.1773e-01,  6.4687e-01, -2.3359e-01,  5.3668e-01,  1.2432e+00,\n",
       "                      -5.6995e-01,  1.2852e+00, -3.1787e-01, -6.6152e-01, -3.9432e-01,\n",
       "                       2.6819e-01,  1.0739e+00, -1.1571e+00,  4.3674e-01,  7.9254e-01,\n",
       "                       1.4127e+00,  2.9558e-01,  9.0049e-01,  6.0694e-02, -1.6696e-01,\n",
       "                       3.6699e-02,  1.9298e-01, -6.1995e-01, -7.2873e-01,  2.1805e+00,\n",
       "                       1.2234e-01,  4.7676e-01, -8.2630e-01, -3.0731e-02,  1.3672e+00,\n",
       "                       9.6279e-01,  5.1325e-01,  1.6986e+00, -8.4890e-01,  1.2385e+00,\n",
       "                       1.6888e+00,  2.3782e-01, -1.5109e+00, -6.4332e-01,  1.8566e+00,\n",
       "                       1.1377e-01, -3.9339e-01, -7.7827e-01,  1.8211e-01, -9.2314e-01,\n",
       "                      -1.4386e+00,  1.9016e+00, -5.6691e-01, -2.2442e+00,  8.1844e-01,\n",
       "                       8.7712e-01, -6.9239e-01, -3.8535e-01,  3.5312e-01, -8.1559e-01,\n",
       "                       8.8872e-01, -3.3236e-02, -4.2989e-01,  5.7466e-01,  8.2070e-01,\n",
       "                       2.6783e+00, -9.9808e-01, -8.9251e-02,  1.4665e+00, -1.9663e+00,\n",
       "                      -1.0365e+00,  4.3498e-01, -6.8236e-01,  2.7184e-01,  1.5547e+00,\n",
       "                      -4.3160e-01, -5.4703e-01,  1.4281e-01,  1.2985e+00, -1.4580e+00,\n",
       "                       8.1160e-01,  8.3026e-01,  1.0880e-01, -2.7511e-01,  3.5128e-01,\n",
       "                       5.5282e-01,  7.5026e-01,  2.3087e-01,  3.5956e-01, -8.0996e-01,\n",
       "                       8.8002e-01, -1.8099e+00, -8.9795e-01,  1.6440e-01, -3.8178e-02,\n",
       "                       1.3577e+00,  1.7022e+00, -1.2372e+00, -2.1192e-02,  1.0840e+00,\n",
       "                       3.7036e-01,  3.7963e-01, -1.1225e-01, -1.1951e-01,  5.6085e-02,\n",
       "                       1.0208e-01, -3.2889e-01, -1.2040e+00,  4.4728e-01,  9.0754e-02,\n",
       "                      -1.4352e+00,  4.2980e-01,  1.0101e+00, -2.8786e-01,  8.2840e-01,\n",
       "                       2.5802e-01,  1.7582e+00,  6.7178e-01,  6.3129e-01, -9.1959e-01,\n",
       "                       1.8797e+00,  1.5388e+00,  8.7516e-01, -1.7782e+00,  6.9712e-01,\n",
       "                       4.5007e-02, -8.4071e-01, -1.1138e+00,  6.9188e-01, -4.0117e-01,\n",
       "                      -1.0725e+00, -6.0955e-02,  1.1190e+00, -8.1298e-01,  2.7561e-02,\n",
       "                       8.9998e-01,  4.5159e-01, -1.3801e+00,  1.9379e+00,  7.5337e-01,\n",
       "                       7.2508e-01,  6.7931e-01, -7.6619e-01, -8.2712e-02, -1.7272e+00,\n",
       "                       1.9973e-01,  1.3591e+00,  1.4680e+00, -1.3328e-01, -7.6995e-01,\n",
       "                      -5.0782e-01, -1.4840e+00, -2.1451e+00, -1.1639e-01, -1.7386e+00,\n",
       "                      -5.5282e-01, -2.4672e-01, -1.2833e+00, -2.9595e-02, -2.4380e-01,\n",
       "                       2.2811e-01,  1.6894e-01, -8.0017e-01,  6.8280e-01,  6.6363e-01,\n",
       "                      -1.7258e+00, -1.5341e+00,  1.3550e-01, -1.5420e+00,  1.1454e-01,\n",
       "                       3.9200e-02,  3.4088e-01, -1.1613e+00, -1.9226e-01, -1.0974e+00,\n",
       "                      -1.0879e+00,  1.4958e+00, -1.1291e-01, -2.0897e+00, -1.2111e+00,\n",
       "                      -8.2316e-01,  2.1257e-01,  5.9461e-01,  1.8765e-01, -9.5300e-01,\n",
       "                      -9.7569e-01, -8.5263e-01,  4.8091e-01, -7.5076e-01,  4.6234e-01,\n",
       "                       8.6727e-01, -1.1580e-01,  4.3529e-01, -2.0934e+00,  1.0884e+00,\n",
       "                       1.0504e+00, -3.6372e-01,  4.0530e-01, -1.1005e+00,  4.2842e-01,\n",
       "                      -9.6548e-01, -4.7125e-01, -6.7160e-01,  6.4767e-01, -1.0615e-01,\n",
       "                      -2.9890e-03, -8.2914e-01,  1.2155e+00, -6.8075e-01, -1.2213e+00,\n",
       "                      -5.8643e-01,  1.6253e+00,  1.3727e+00,  3.0761e-01,  6.4502e-01,\n",
       "                       1.3179e+00,  4.9775e-03,  1.7852e+00,  1.9829e-01,  9.7998e-01,\n",
       "                      -4.0877e-01, -9.3486e-01, -1.8735e+00,  2.2416e-02,  6.5066e-01,\n",
       "                      -1.1518e+00,  5.2179e-02,  1.1216e+00,  3.8062e-02,  3.4145e-01,\n",
       "                      -1.8148e-01, -6.1908e-02, -1.3634e+00,  1.7260e-02, -1.4133e+00,\n",
       "                      -1.0166e+00, -1.0961e-01,  1.7641e+00,  1.2821e+00,  6.6469e-03,\n",
       "                       1.3061e-01, -7.2289e-01,  1.1979e+00, -1.5358e+00,  2.4623e-01,\n",
       "                       1.5875e+00,  3.3495e-01,  1.1427e-01, -1.4562e+00,  3.7938e-01,\n",
       "                       5.5425e-01,  8.1569e-01,  8.4334e-01,  8.2550e-02,  3.2939e-01,\n",
       "                       1.4830e+00,  5.2290e-03,  5.2169e-01,  4.3300e-01, -1.6864e+00,\n",
       "                       1.6639e+00,  9.7182e-01, -3.7314e-01,  1.5641e+00,  2.1643e+00,\n",
       "                       6.7014e-02, -9.3971e-01,  2.7444e+00,  5.5345e-01, -1.9885e+00,\n",
       "                      -2.5713e-01, -6.2457e-01,  8.2213e-01, -3.9848e-01,  1.7405e+00,\n",
       "                       5.3126e-01,  1.5600e+00, -1.1789e+00,  4.2566e-01, -2.8775e-01,\n",
       "                       1.7775e-01, -6.3041e-01,  6.0313e-01,  2.3594e-01])),\n",
       "             ('blocks.0.message.tp.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.actu.alpha',\n",
       "              tensor([1.0154, 1.0018, 0.9855, 0.9971, 1.0183, 1.0068, 1.0002, 1.0242, 1.0037,\n",
       "                      1.0276, 0.9871, 0.9985, 0.9992, 0.9990, 0.9904, 0.9906, 1.0060, 0.9983,\n",
       "                      1.0032, 0.9958, 0.9994, 1.0020, 1.0345, 0.9972, 1.0154, 1.0060, 1.0052,\n",
       "                      1.0090, 1.0011, 1.0021, 1.0056, 1.0034, 1.0054, 0.9968, 1.0018, 1.0111,\n",
       "                      1.0077, 0.9955, 1.0088, 1.0231, 0.9939, 1.0212, 0.9979, 0.9942, 1.0067,\n",
       "                      1.1070, 0.9992, 0.9962, 1.0084, 1.0123, 1.0120, 1.0077, 1.0139, 0.9978,\n",
       "                      0.9943, 0.9995, 1.0218, 1.0140, 1.0317, 1.0064, 0.9835, 0.9929, 1.0255,\n",
       "                      1.0016, 1.0087, 1.0019, 1.0065, 1.0047, 0.9814, 0.9942, 0.9941, 1.0080,\n",
       "                      0.9932, 1.0054, 0.9762, 1.0075, 1.0070, 1.0248, 1.0017, 1.0019, 1.0050,\n",
       "                      1.0065, 1.0065, 1.0114, 0.9931, 1.0182, 0.9962, 0.9939, 0.9864, 1.0032,\n",
       "                      0.9940, 0.9991, 1.0201, 1.0196, 1.0133, 1.0048, 1.0087, 1.0062, 0.9990,\n",
       "                      1.0026, 1.0039, 0.9969, 1.0176, 0.9975, 0.9895, 1.0115, 1.0101, 1.0041,\n",
       "                      1.0344, 1.0099, 1.0182, 1.0155, 0.9859, 1.0127, 1.0024, 1.0128, 1.0012,\n",
       "                      1.0026, 0.9931, 1.0058, 0.9989, 1.0020, 1.0285, 1.0126, 0.9921, 1.0284,\n",
       "                      1.0017, 1.0033])),\n",
       "             ('blocks.0.update.actu.beta',\n",
       "              tensor([1.7106, 1.7187, 1.6755, 1.7023, 1.7189, 1.6693, 1.7032, 1.7136, 1.6965,\n",
       "                      1.7000, 1.6893, 1.6993, 1.6780, 1.6796, 1.6933, 1.7196, 1.7068, 1.7157,\n",
       "                      1.6877, 1.6925, 1.6964, 1.6989, 1.6744, 1.7039, 1.7271, 1.7027, 1.6961,\n",
       "                      1.6995, 1.7013, 1.6994, 1.7005, 1.7014, 1.7021, 1.6665, 1.7024, 1.7033,\n",
       "                      1.7029, 1.6978, 1.7049, 1.6890, 1.7007, 1.7303, 1.7010, 1.7066, 1.7082,\n",
       "                      1.7284, 1.7010, 1.6797, 1.7363, 1.7141, 1.6968, 1.7011, 1.6909, 1.7042,\n",
       "                      1.6830, 1.7038, 1.7120, 1.6920, 1.6762, 1.7018, 1.6932, 1.6871, 1.6967,\n",
       "                      1.7036, 1.6990, 1.6730, 1.7027, 1.6985, 1.7149, 1.6959, 1.6974, 1.6962,\n",
       "                      1.6967, 1.6900, 1.7516, 1.7025, 1.7005, 1.6824, 1.7051, 1.6987, 1.6964,\n",
       "                      1.6999, 1.6827, 1.7157, 1.6976, 1.7369, 1.6915, 1.6992, 1.6877, 1.7031,\n",
       "                      1.7028, 1.6821, 1.6958, 1.7381, 1.6877, 1.7032, 1.7138, 1.7087, 1.6962,\n",
       "                      1.7023, 1.7016, 1.6909, 1.7007, 1.7103, 1.6865, 1.6922, 1.6899, 1.6997,\n",
       "                      1.6837, 1.6913, 1.6982, 1.6698, 1.6952, 1.6944, 1.7049, 1.7001, 1.7006,\n",
       "                      1.7018, 1.7139, 1.7010, 1.7037, 1.6650, 1.6796, 1.6901, 1.6871, 1.7121,\n",
       "                      1.7248, 1.6941])),\n",
       "             ('blocks.0.update.outt.weight',\n",
       "              tensor([ 1.3499,  0.0755,  0.7279,  ...,  0.7514, -0.2360, -0.7767])),\n",
       "             ('blocks.0.update.outt.bias', tensor([])),\n",
       "             ('blocks.0.update.outt.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.outs.weight',\n",
       "              tensor([[-0.0067, -0.0353, -0.1231,  ..., -0.1065, -0.1435,  0.0263],\n",
       "                      [-0.1315,  0.0185,  0.1000,  ...,  0.0470, -0.0110, -0.0525],\n",
       "                      [ 0.1376,  0.0804, -0.0195,  ...,  0.0023, -0.0167, -0.0774],\n",
       "                      ...,\n",
       "                      [-0.1394, -0.1028, -0.0242,  ...,  0.0584,  0.0018,  0.1783],\n",
       "                      [ 0.0536,  0.0219, -0.0644,  ..., -0.1289,  0.1505, -0.1138],\n",
       "                      [ 0.0680, -0.1454, -0.0489,  ..., -0.0913, -0.1405, -0.1544]])),\n",
       "             ('blocks.0.update.outs.bias',\n",
       "              tensor([ 0.0100,  0.0077,  0.0110,  0.0070,  0.0094, -0.0060,  0.0069, -0.0079,\n",
       "                      -0.0025, -0.0068,  0.0016, -0.0088, -0.0103, -0.0101,  0.0026, -0.0061,\n",
       "                       0.0073,  0.0104,  0.0097, -0.0039,  0.0106,  0.0082, -0.0056, -0.0102,\n",
       "                       0.0086, -0.0096,  0.0096, -0.0085,  0.0080,  0.0083, -0.0082, -0.0097,\n",
       "                      -0.0054, -0.0101,  0.0083,  0.0081, -0.0093,  0.0083,  0.0077, -0.0091,\n",
       "                      -0.0090,  0.0053, -0.0110, -0.0034, -0.0094, -0.0093,  0.0094, -0.0081,\n",
       "                      -0.0135, -0.0098, -0.0089,  0.0093, -0.0050,  0.0069, -0.0054, -0.0097,\n",
       "                       0.0097, -0.0064, -0.0041, -0.0103, -0.0030, -0.0092, -0.0077,  0.0085,\n",
       "                       0.0053, -0.0051,  0.0071, -0.0095,  0.0083,  0.0071,  0.0081,  0.0110,\n",
       "                      -0.0051,  0.0047,  0.0078, -0.0104, -0.0095, -0.0079,  0.0073,  0.0060,\n",
       "                      -0.0036, -0.0097, -0.0060,  0.0092,  0.0078, -0.0021, -0.0095,  0.0092,\n",
       "                       0.0005,  0.0081,  0.0076,  0.0088,  0.0043, -0.0077, -0.0039, -0.0099,\n",
       "                      -0.0081,  0.0102, -0.0094,  0.0092, -0.0092,  0.0092, -0.0084, -0.0030,\n",
       "                       0.0088, -0.0076, -0.0085, -0.0083, -0.0085, -0.0079,  0.0088, -0.0082,\n",
       "                       0.0095, -0.0041,  0.0073, -0.0102,  0.0079, -0.0054,  0.0097,  0.0078,\n",
       "                      -0.0101, -0.0076,  0.0014, -0.0089,  0.0077, -0.0112, -0.0068,  0.0100])),\n",
       "             ('blocks.0.update.uattn.lq.weight',\n",
       "              tensor([ 0.2518, -0.7742, -0.0237,  ..., -0.2055,  1.2128,  1.0184])),\n",
       "             ('blocks.0.update.uattn.lq.bias', tensor([])),\n",
       "             ('blocks.0.update.uattn.lq.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.uattn.lk.weight',\n",
       "              tensor([-1.2096,  0.0357,  1.2330,  ...,  0.2092, -1.9685, -0.0237])),\n",
       "             ('blocks.0.update.uattn.lk.bias', tensor([])),\n",
       "             ('blocks.0.update.uattn.lk.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.uattn.lv.weight',\n",
       "              tensor([ 0.7619,  1.3262, -1.0034,  ..., -0.9944,  0.8602, -1.0089])),\n",
       "             ('blocks.0.update.uattn.lv.bias', tensor([])),\n",
       "             ('blocks.0.update.uattn.lv.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.uattn.ls.weight',\n",
       "              tensor([[-0.1012, -0.0539, -0.1035,  ...,  0.0548,  0.0612,  0.0276],\n",
       "                      [-0.0039,  0.0827, -0.1251,  ...,  0.0890,  0.0022, -0.0017],\n",
       "                      [-0.0484, -0.1004,  0.0633,  ...,  0.0636, -0.0562,  0.1381],\n",
       "                      ...,\n",
       "                      [-0.0823,  0.1142,  0.0889,  ...,  0.0361, -0.0721, -0.0157],\n",
       "                      [-0.1334, -0.0729, -0.0773,  ...,  0.0852, -0.0535,  0.1296],\n",
       "                      [ 0.0672,  0.0745, -0.0222,  ...,  0.0266,  0.0203,  0.0813]])),\n",
       "             ('blocks.0.update.uattn.ls.bias',\n",
       "              tensor([ 7.0629e-03, -6.7314e-03, -1.7596e-02,  3.5120e-03,  5.4057e-03,\n",
       "                      -7.3407e-03, -8.3227e-03,  6.2736e-03, -4.4351e-03, -1.3754e-02,\n",
       "                       2.1267e-02,  7.6122e-03, -5.0453e-03, -1.4056e-02, -1.3181e-02,\n",
       "                      -1.5675e-03,  2.0156e-03,  5.6089e-03, -8.9731e-03, -1.0914e-02,\n",
       "                       1.9910e-02,  5.2055e-03, -5.2208e-03,  9.3433e-03, -8.7600e-03,\n",
       "                      -8.0423e-03, -4.5501e-03,  8.6008e-03,  6.8328e-03, -1.0469e-02,\n",
       "                      -1.9978e-03,  9.1583e-03,  2.9966e-03, -1.1368e-02,  4.8561e-03,\n",
       "                      -9.5381e-03,  9.1727e-03,  6.1424e-03,  1.1254e-02,  8.6036e-03,\n",
       "                      -7.7097e-03,  5.3104e-03,  1.0465e-02, -6.3889e-03,  6.7562e-03,\n",
       "                       1.0476e-02,  1.4407e-02,  8.7128e-03,  7.4738e-03, -1.3640e-02,\n",
       "                      -9.2105e-03, -9.2967e-03, -2.4855e-03,  7.9900e-03, -4.2384e-03,\n",
       "                       9.7111e-03, -1.1422e-02,  3.1662e-02, -3.2827e-03, -9.9638e-03,\n",
       "                      -1.4383e-02,  4.1706e-03,  1.1386e-02,  7.8311e-03,  1.0038e-02,\n",
       "                      -2.5426e-03, -1.1619e-03,  1.1475e-03,  1.3356e-02,  9.4507e-03,\n",
       "                      -5.1841e-03, -1.2064e-02, -6.5254e-03,  5.5882e-02, -7.0784e-03,\n",
       "                       7.1866e-03,  9.0963e-03,  1.2079e-02,  6.5909e-03,  4.0397e-03,\n",
       "                       3.4373e-03, -6.9210e-03, -1.0124e-02,  5.6422e-03,  6.1990e-03,\n",
       "                       3.7845e-03, -1.4800e-02, -2.2031e-04, -8.7469e-03, -8.7752e-03,\n",
       "                       5.6297e-03,  2.8953e-02, -5.7519e-03, -6.4323e-03,  4.7576e-03,\n",
       "                      -8.0836e-03, -1.2883e-02,  8.6960e-03,  8.5528e-03, -6.5600e-03,\n",
       "                       1.0103e-02, -2.7518e-03, -9.9282e-03, -1.8294e-02,  4.0412e-02,\n",
       "                       9.7278e-03, -1.4061e-04,  7.7914e-03, -2.9873e-04, -7.3422e-03,\n",
       "                      -7.0757e-03,  9.5290e-03,  1.3997e-02, -7.0855e-03, -9.5635e-03,\n",
       "                       1.3076e-02,  7.2267e-03,  6.8631e-03, -1.2889e-02,  4.2461e-03,\n",
       "                      -8.5430e-03,  1.2625e-02,  2.3124e-02,  9.3698e-03,  4.0611e-02,\n",
       "                      -4.6252e-03, -3.3277e-03,  1.3054e-02,  4.6350e-03,  2.5459e-03,\n",
       "                       3.2228e-03,  3.3151e-03,  4.0253e-03,  3.8338e-03,  2.5737e-03,\n",
       "                       8.2972e-04, -8.4649e-04,  1.2019e-03,  2.0182e-03,  1.5690e-03,\n",
       "                       2.3444e-03,  1.8515e-03,  2.2092e-03, -8.1365e-04,  2.0610e-03,\n",
       "                       1.8324e-03,  2.9282e-03,  7.4543e-04,  3.4139e-03,  1.0858e-03,\n",
       "                       4.1087e-03,  3.9418e-03,  4.0844e-03,  1.7138e-03,  4.2681e-03,\n",
       "                       2.3795e-03,  8.5189e-04,  4.6693e-03,  1.4041e-03,  2.3903e-03,\n",
       "                       1.3290e-03,  1.7410e-03,  3.4353e-03,  8.3895e-03,  1.3428e-03,\n",
       "                       2.3109e-03,  1.9053e-03,  1.7479e-03,  3.5957e-03,  3.5045e-03,\n",
       "                      -4.6026e-04, -2.9133e-04,  3.5496e-03,  1.4684e-03,  1.6396e-03,\n",
       "                       1.0231e-03,  3.1482e-03,  9.6255e-04,  3.1634e-03,  2.5839e-03,\n",
       "                       4.0916e-03,  3.2296e-03,  1.5303e-03,  5.1422e-03,  3.5305e-03,\n",
       "                       2.1081e-03,  3.4863e-03,  3.0756e-03,  1.8412e-03, -5.3073e-05,\n",
       "                       2.5379e-03,  3.3626e-03,  5.3121e-04,  2.0255e-03,  2.4549e-03,\n",
       "                       3.3162e-03,  2.7384e-03,  4.2590e-03,  2.6862e-03,  5.3547e-04,\n",
       "                       2.7787e-03,  4.4245e-03,  1.3072e-03,  3.7402e-03,  2.6344e-03,\n",
       "                       2.4257e-03,  4.4827e-03,  1.6620e-03,  4.0833e-03,  1.0041e-03,\n",
       "                       3.5357e-03,  2.8426e-03,  3.1490e-03,  2.0229e-03,  2.3161e-03,\n",
       "                       6.0801e-04,  3.0984e-03,  2.7631e-03,  2.5576e-03,  2.8213e-03,\n",
       "                       3.4315e-03,  7.7879e-04,  1.0684e-03,  2.9114e-04,  1.6840e-03,\n",
       "                       3.1656e-03,  2.4138e-03,  3.3247e-03,  1.5724e-04,  1.9290e-03,\n",
       "                       2.2563e-03,  1.4010e-03,  2.1671e-03,  3.1347e-03,  2.3267e-03,\n",
       "                      -1.2779e-03,  3.0405e-03,  3.3463e-03,  3.7434e-03,  2.1338e-03,\n",
       "                       2.5843e-03,  1.7675e-03,  1.3711e-03,  2.9562e-03,  2.2659e-03,\n",
       "                       2.4699e-03,  2.6856e-03,  2.3133e-03,  3.2419e-03,  1.4136e-03,\n",
       "                       2.6905e-03,  2.7998e-03,  3.1545e-03,  2.2255e-03,  3.1761e-03,\n",
       "                       1.4738e-03])),\n",
       "             ('blocks.0.update.uattn.lvs.weight',\n",
       "              tensor([[ 0.1351,  0.0806, -0.1170,  ...,  0.1223, -0.1062,  0.0617],\n",
       "                      [-0.0914, -0.0898, -0.0341,  ..., -0.0592,  0.0593, -0.0763],\n",
       "                      [-0.0781,  0.1252,  0.0312,  ..., -0.1172, -0.1116,  0.1053],\n",
       "                      ...,\n",
       "                      [-0.0581,  0.0273, -0.0413,  ...,  0.0101, -0.0766,  0.0169],\n",
       "                      [-0.0619, -0.1055,  0.1073,  ..., -0.0591, -0.0880,  0.1355],\n",
       "                      [-0.1132,  0.0982,  0.1018,  ..., -0.0982, -0.1221,  0.1542]])),\n",
       "             ('blocks.0.update.uattn.lvs.bias',\n",
       "              tensor([ 0.0083,  0.0054,  0.0073,  0.0071,  0.0078, -0.0074,  0.0174, -0.0095,\n",
       "                       0.0039, -0.0093,  0.0117, -0.0105, -0.0100, -0.0109, -0.0012, -0.0043,\n",
       "                       0.0054,  0.0109,  0.0099, -0.0059,  0.0132,  0.0349, -0.0082,  0.0021,\n",
       "                       0.0103, -0.0090,  0.0112, -0.0084,  0.0105,  0.0071, -0.0068, -0.0117,\n",
       "                      -0.0081, -0.0106,  0.0081,  0.0082, -0.0087,  0.0084,  0.0082, -0.0101,\n",
       "                      -0.0077,  0.0061, -0.0082, -0.0046, -0.0102,  0.0022,  0.0108, -0.0067,\n",
       "                      -0.0158, -0.0105, -0.0088, -0.0141, -0.0086,  0.0092, -0.0044, -0.0069,\n",
       "                       0.0078,  0.0036, -0.0054, -0.0093, -0.0097, -0.0088, -0.0060,  0.0100,\n",
       "                       0.0457, -0.0079,  0.0054, -0.0057,  0.0104,  0.0088,  0.0067,  0.0059,\n",
       "                      -0.0050,  0.0239,  0.0065, -0.0111, -0.0093,  0.0103,  0.0080,  0.0063,\n",
       "                      -0.0045, -0.0079, -0.0073,  0.0085,  0.0079, -0.0005, -0.0110,  0.0033,\n",
       "                      -0.0027,  0.0081,  0.0079,  0.0128, -0.0135, -0.0081, -0.0035, -0.0090,\n",
       "                      -0.0120,  0.0110, -0.0027,  0.0123,  0.0049,  0.0131, -0.0099, -0.0076,\n",
       "                       0.0109, -0.0095, -0.0066, -0.0074, -0.0082, -0.0080,  0.0031, -0.0074,\n",
       "                       0.0369, -0.0063,  0.0057, -0.0042,  0.0094, -0.0036,  0.0078,  0.0062,\n",
       "                      -0.0088,  0.0086,  0.0142,  0.0089,  0.0132, -0.0106, -0.0051,  0.0076])),\n",
       "             ('blocks.0.update.uattn.tp1.weight', tensor([])),\n",
       "             ('blocks.0.update.uattn.tp1.output_mask',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('blocks.0.update.uattn.tp2.weight',\n",
       "              tensor([ 8.6010e-01, -2.5332e-01, -7.5832e-01,  1.7367e+00, -2.8850e-02,\n",
       "                      -6.6764e-01, -1.0204e+00,  7.5059e-01,  1.5383e+00, -1.0339e+00,\n",
       "                       1.9204e-01, -1.2008e+00, -4.0107e-02,  6.6379e-01, -5.8462e-01,\n",
       "                      -6.2740e-01, -1.1566e+00, -3.8878e-02, -1.3130e-01, -4.9823e-01,\n",
       "                      -7.5855e-01,  2.5517e+00,  1.4964e+00,  7.6846e-01, -2.4984e+00,\n",
       "                      -1.8118e+00,  8.2149e-01, -2.0677e-01, -6.7564e-01,  1.8769e+00,\n",
       "                       1.5128e-01, -1.3390e-01, -6.9540e-01,  2.1261e+00,  3.6796e-02,\n",
       "                      -3.2630e+00,  6.1123e-01, -3.6127e-01,  1.0662e+00, -9.1538e-01,\n",
       "                      -1.8594e+00, -2.8444e+00,  9.2619e-01, -7.0384e-01,  4.5370e-01,\n",
       "                       7.5894e-01, -8.9728e-01,  7.3495e-02, -4.3293e-01, -9.7794e-02,\n",
       "                       8.1511e-02, -6.0883e-01,  2.0720e+00, -1.4070e+00,  4.9278e-01,\n",
       "                      -9.3540e-01, -1.7419e+00, -6.0407e-01, -5.3138e-01, -2.3351e-01,\n",
       "                      -1.0283e+00, -4.4127e-01,  1.4652e+00,  4.8741e-01,  1.7040e-01,\n",
       "                       1.2226e-01, -8.0587e-01,  8.8748e-02, -1.3357e-01,  4.7582e-01,\n",
       "                       2.3026e-02,  1.4588e+00, -1.7336e-01,  2.8585e-02, -4.2383e-01,\n",
       "                      -1.0335e+00, -9.7223e-02, -8.6477e-01,  1.2642e+00, -1.4035e+00,\n",
       "                      -1.7136e+00, -8.4859e-01, -6.4365e-01,  8.9343e-02,  2.0100e-01,\n",
       "                       9.9578e-01,  4.7600e-01,  1.2494e+00, -7.6768e-01, -4.6985e-01,\n",
       "                       9.3230e-01, -1.5364e-02,  8.8643e-01,  8.0513e-01,  9.7147e-01,\n",
       "                       1.2376e+00, -7.6652e-01, -6.6717e-01,  1.5722e+00,  6.9243e-02,\n",
       "                      -1.4254e+00, -3.5089e-01, -6.9377e-01,  1.3427e+00, -1.2198e+00,\n",
       "                      -7.1064e-01,  9.5605e-01,  1.1373e+00, -7.4161e-01,  2.6898e-01,\n",
       "                       9.8713e-01,  1.0958e+00, -2.2017e-01, -1.6702e+00,  1.9663e+00,\n",
       "                       1.9561e+00, -9.6795e-02, -7.7262e-02,  7.9144e-01, -4.8638e-01,\n",
       "                      -1.6833e+00,  4.0412e-01, -2.7455e-01, -7.9097e-01, -3.2954e-01,\n",
       "                      -3.0553e-01,  1.0811e+00,  2.7076e-01,  8.0287e-01, -2.3007e-01,\n",
       "                       1.3389e+00, -1.4110e+00,  1.3319e+00, -1.5810e+00, -1.3984e+00,\n",
       "                      -1.6385e+00, -1.0452e-01,  9.4318e-01,  1.2942e-01,  1.5667e+00,\n",
       "                      -9.3288e-01, -4.2634e-01, -2.9955e-01, -1.8586e+00,  8.5352e-01,\n",
       "                      -3.1923e-01,  6.5611e-01, -1.3387e+00,  2.1561e+00, -1.3667e-01,\n",
       "                       6.0006e-01, -8.2078e-01,  4.8700e-01,  1.3741e+00, -4.1388e-01,\n",
       "                       1.2692e-01,  6.7282e-01, -7.9231e-01, -2.8885e-01,  5.8905e-02,\n",
       "                      -6.1756e-01, -5.1895e-02, -8.1352e-01, -4.0675e-01,  6.7635e-01,\n",
       "                      -4.3595e-01, -1.0250e+00,  2.6904e-01,  2.8391e-01, -9.1905e-02,\n",
       "                      -4.5270e-01, -7.6499e-01, -4.2423e-01, -4.9607e-02, -6.4623e-02,\n",
       "                       6.6275e-01,  8.6686e-01,  3.4745e-01, -1.0743e+00,  8.6439e-01,\n",
       "                       4.5377e-01,  4.0632e-01, -3.5235e-01,  5.4961e-01,  1.4245e+00,\n",
       "                       5.7953e-01, -4.2909e-01, -6.5550e-01, -6.5904e-01, -2.3305e+00,\n",
       "                      -1.4464e-02, -5.3701e-01,  1.1574e+00, -7.6067e-02,  1.3036e+00,\n",
       "                      -1.6374e+00, -5.0087e-01, -4.9232e-01,  6.4309e-01, -8.2777e-01,\n",
       "                       7.7291e-01,  1.3299e+00, -8.0769e-01,  1.0305e+00, -4.5070e-01,\n",
       "                       9.3450e-01, -2.0227e-02,  1.4715e+00,  4.3937e-02,  1.5059e-01,\n",
       "                       4.2061e-01, -1.1473e+00,  1.9265e-01, -1.2219e+00, -1.1319e+00,\n",
       "                       2.1732e-01, -1.2187e-01, -8.4078e-01, -4.4698e-01, -9.3877e-01,\n",
       "                      -5.7107e-01,  9.6030e-01,  2.0234e+00, -8.0496e-01, -1.6227e+00,\n",
       "                       7.4225e-01,  1.9200e-01,  7.1182e-01,  5.1361e-01,  1.8699e+00,\n",
       "                       3.6471e-01, -1.6383e+00, -7.1225e-02,  2.7380e-02,  8.8797e-01,\n",
       "                       1.6817e+00, -6.0635e-01,  7.8822e-01,  3.0768e-01,  8.4880e-01,\n",
       "                       6.7836e-01,  2.5230e-01, -1.1493e+00,  1.1899e-01, -4.7749e-01,\n",
       "                      -7.6113e-01, -1.6193e-01, -3.6180e-03,  5.6163e-01,  4.1956e-01,\n",
       "                      -1.0344e+00,  1.7092e+00,  5.2763e-01, -5.3393e-01,  1.2476e+00,\n",
       "                       3.9193e-01,  6.8713e-01, -7.3327e-01,  1.9225e+00,  1.5733e-01,\n",
       "                      -1.1291e+00,  4.9512e-01, -7.9234e-02, -3.0869e-01,  6.9013e-02,\n",
       "                       1.7341e-01, -1.6580e+00, -1.1429e-01, -1.6761e+00, -2.2232e-01,\n",
       "                      -2.0308e+00, -1.2186e+00, -8.5120e-01, -4.0900e-01,  7.8444e-01,\n",
       "                       1.3652e+00, -6.1194e-01,  1.5993e-01, -1.2667e+00,  1.1588e+00,\n",
       "                       2.3230e-01,  1.3894e+00, -1.2411e+00, -5.3633e-01,  1.1583e+00,\n",
       "                       1.6763e-01,  1.8665e+00,  7.9700e-01, -3.2100e-01,  8.9162e-01,\n",
       "                      -3.6731e-01,  4.8587e-03,  1.6269e+00,  1.1831e+00, -9.4318e-01,\n",
       "                      -1.6170e+00, -6.5693e-01,  2.1768e+00,  2.9032e-01, -1.2245e+00,\n",
       "                      -3.4057e-03,  9.4627e-01, -1.7794e+00, -7.5186e-01, -9.7618e-01,\n",
       "                       2.3320e+00,  1.2648e+00,  1.4210e-01, -6.8161e-01, -8.6732e-01,\n",
       "                      -1.4988e+00, -5.2137e-01, -1.5340e+00,  7.9661e-01, -5.1273e-01,\n",
       "                      -1.9244e-01,  1.5446e+00, -6.6685e-01, -1.0645e+00,  1.4133e-01,\n",
       "                      -7.0426e-01,  3.2002e-02, -3.8061e-01,  3.1781e-01, -1.0288e+00,\n",
       "                      -9.5669e-01, -1.0497e+00, -1.0839e+00, -4.3526e-01, -1.2213e+00,\n",
       "                       7.4969e-01,  5.5443e-01,  3.9445e-01,  3.1981e-01, -2.1433e+00,\n",
       "                       6.6914e-02,  1.0332e-02,  2.5743e-03,  5.3897e-01, -1.4905e+00,\n",
       "                       4.1418e-01, -1.9529e+00,  1.0234e+00, -1.4818e-01, -3.9465e-03,\n",
       "                      -5.9948e-02, -1.3429e+00, -3.8722e-01,  4.2048e-01,  1.6299e+00,\n",
       "                       6.9552e-01,  7.5236e-02, -9.1075e-01, -2.3071e-02, -9.0372e-01,\n",
       "                      -1.7233e+00, -1.2074e+00,  4.9355e-01, -1.9681e+00, -5.1509e-01,\n",
       "                      -6.0384e-01, -6.6625e-01,  1.2057e+00, -9.3344e-01,  1.8279e-01,\n",
       "                       8.0676e-01,  8.9563e-01,  1.4740e-01, -2.9006e+00,  5.8325e-02,\n",
       "                       3.7773e-01,  1.1945e+00, -2.9502e-01, -1.8177e+00,  3.3113e+00,\n",
       "                       4.6365e-01, -4.8839e-01,  2.3690e-01, -1.2894e+00,  1.0499e+00,\n",
       "                       9.8881e-01, -2.7145e+00, -4.9799e-01,  4.5865e-02])),\n",
       "             ('blocks.0.update.uattn.tp2.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.0.update.uattn.actlvs.alpha',\n",
       "              tensor([1.0070, 0.9972, 0.9844, 1.0048, 1.0055, 0.9927, 0.9914, 1.0020, 0.9963,\n",
       "                      0.9861, 1.0213, 1.0075, 0.9886, 0.9845, 0.9886, 0.9986, 1.0022, 1.0051,\n",
       "                      0.9929, 0.9899, 1.0172, 1.0439, 0.9948, 1.0219, 0.9931, 0.9919, 0.9974,\n",
       "                      1.0085, 1.0120, 0.9882, 0.9988, 1.0090, 1.0027, 0.9888, 1.0047, 0.9904,\n",
       "                      1.0090, 1.0064, 1.0108, 1.0085, 0.9923, 1.0004, 1.0095, 0.9935, 1.0065,\n",
       "                      1.0135, 1.0138, 1.0085, 1.0070, 0.9854, 0.9915, 0.9905, 0.9933, 1.0112,\n",
       "                      0.9977, 1.0095, 0.9847, 1.0239, 0.9965, 0.9899, 0.9852, 1.0049, 1.0109,\n",
       "                      1.0082, 1.0137, 0.9935, 0.9988, 0.9975, 1.0110, 1.0090, 0.9945, 0.9872,\n",
       "                      0.9919, 1.0675, 0.9920, 1.0070, 1.0088, 1.0127, 1.0066, 1.0043, 1.0034,\n",
       "                      0.9932, 0.9909, 1.0057, 1.0062, 1.0038, 0.9869, 0.9988, 0.9920, 0.9911,\n",
       "                      1.0056, 1.0147, 0.9936, 0.9936, 1.0048, 0.9919, 0.9867, 1.0089, 1.0082,\n",
       "                      0.9937, 1.0100, 0.9965, 0.9861, 0.9882, 1.0364, 1.0094, 1.0015, 1.0076,\n",
       "                      0.9994, 0.9927, 0.9931, 1.0103, 1.0155, 0.9932, 0.9902, 1.0094, 1.0072,\n",
       "                      1.0080, 0.9876, 1.0031, 0.9915, 1.0132, 1.0252, 1.0092, 1.0376, 0.9980,\n",
       "                      0.9964, 0.9920])),\n",
       "             ('blocks.0.update.uattn.actlvs.beta',\n",
       "              tensor([1.7129, 1.7081, 1.7177, 1.7121, 1.7082, 1.6950, 1.7094, 1.6978, 1.6951,\n",
       "                      1.6844, 1.7104, 1.6952, 1.6715, 1.6925, 1.6918, 1.6966, 1.7098, 1.7092,\n",
       "                      1.7113, 1.6987, 1.7126, 1.7359, 1.7064, 1.6979, 1.7103, 1.6953, 1.7099,\n",
       "                      1.6934, 1.7126, 1.7095, 1.7104, 1.6931, 1.6963, 1.6909, 1.7055, 1.7109,\n",
       "                      1.6935, 1.7087, 1.7111, 1.6892, 1.6940, 1.6739, 1.6938, 1.6967, 1.6943,\n",
       "                      1.6944, 1.7154, 1.6930, 1.6751, 1.6916, 1.6928, 1.7118, 1.7038, 1.7129,\n",
       "                      1.6885, 1.6921, 1.7092, 1.6955, 1.6990, 1.6914, 1.7016, 1.6909, 1.6898,\n",
       "                      1.7115, 1.7084, 1.7009, 1.7087, 1.6989, 1.7128, 1.7115, 1.7095, 1.7152,\n",
       "                      1.6938, 1.7185, 1.7112, 1.6927, 1.6949, 1.6955, 1.7094, 1.7018, 1.7001,\n",
       "                      1.6941, 1.6941, 1.7100, 1.7096, 1.7050, 1.6984, 1.7102, 1.7014, 1.7113,\n",
       "                      1.7095, 1.7122, 1.6993, 1.6942, 1.6945, 1.6927, 1.6976, 1.7141, 1.6934,\n",
       "                      1.7111, 1.6949, 1.7127, 1.6841, 1.7045, 1.7014, 1.6926, 1.6881, 1.6946,\n",
       "                      1.6919, 1.6949, 1.7092, 1.6873, 1.7121, 1.6972, 1.7113, 1.6932, 1.7097,\n",
       "                      1.7014, 1.7082, 1.7070, 1.6932, 1.6937, 1.7088, 1.6928, 1.7097, 1.6971,\n",
       "                      1.6956, 1.7189])),\n",
       "             ('blocks.1.message.Attention.actq.alpha',\n",
       "              tensor([1.0050, 1.0552, 1.1002, 1.0189, 1.0070, 0.9899, 1.0211, 1.0093, 1.0440,\n",
       "                      1.0211, 1.0450, 1.0068, 0.9661, 1.0675, 1.0316, 1.0366, 0.9985, 1.0252,\n",
       "                      1.0414, 1.0390, 1.0239, 1.0434, 0.9756, 0.9969, 1.0111, 1.0044, 1.0061,\n",
       "                      1.0143, 1.0128, 1.1066, 1.0294, 1.0532, 1.0064, 1.0446, 1.0255, 1.0248,\n",
       "                      1.0230, 1.0804, 1.0042, 0.9870, 1.0581, 1.0124, 1.0671, 1.0328, 1.0618,\n",
       "                      1.0706, 1.0129, 0.9887, 0.9979, 1.0389, 0.9702, 1.0664, 1.0310, 1.0045,\n",
       "                      1.0015, 0.9975, 1.0967, 1.0417, 1.0117, 1.0328, 1.0889, 1.0187, 1.0125,\n",
       "                      1.0380, 1.0261, 1.0189, 1.0230, 1.0296, 0.9975, 0.9763, 1.0280, 0.9993,\n",
       "                      1.0110, 1.0405, 1.0186, 1.0082, 1.0166, 1.0745, 0.9863, 0.9864, 1.0349,\n",
       "                      0.9816, 1.0254, 1.0214, 0.9941, 1.0492, 1.0135, 1.0151, 1.0145, 0.9991,\n",
       "                      1.0029, 1.0165, 1.0492, 1.0069, 0.9954, 1.0252, 1.0057, 1.0559, 1.0205,\n",
       "                      1.0656, 1.0866, 1.0031, 0.9918, 0.9930, 1.0541, 1.0272, 1.0229, 1.0441,\n",
       "                      1.0387, 1.0370, 1.0010, 1.0322, 1.0647, 0.9812, 0.9755, 1.0005, 1.0046,\n",
       "                      1.0119, 1.0278, 1.0076, 1.0046, 1.0122, 1.0131, 1.0234, 1.0450, 0.9828,\n",
       "                      1.0450, 1.0309])),\n",
       "             ('blocks.1.message.Attention.actq.beta',\n",
       "              tensor([1.7023, 1.7362, 1.7060, 1.7041, 1.6978, 1.7122, 1.6868, 1.7051, 1.7059,\n",
       "                      1.6913, 1.7277, 1.7234, 1.7861, 1.7211, 1.6806, 1.7158, 1.6990, 1.7036,\n",
       "                      1.7440, 1.7320, 1.6908, 1.7478, 1.6832, 1.6965, 1.6872, 1.7005, 1.6971,\n",
       "                      1.6706, 1.6877, 1.6875, 1.7160, 1.7052, 1.7092, 1.7403, 1.6893, 1.7070,\n",
       "                      1.7280, 1.7204, 1.7075, 1.7072, 1.7327, 1.7137, 1.7173, 1.7438, 1.7214,\n",
       "                      1.7130, 1.6826, 1.6951, 1.7052, 1.6596, 1.7320, 1.6638, 1.6829, 1.7019,\n",
       "                      1.6853, 1.6837, 1.6707, 1.6994, 1.7137, 1.6494, 1.6822, 1.7103, 1.6985,\n",
       "                      1.6809, 1.7010, 1.7448, 1.7008, 1.7243, 1.7269, 1.7367, 1.6823, 1.7009,\n",
       "                      1.7061, 1.6392, 1.6920, 1.7312, 1.6717, 1.6510, 1.6927, 1.6960, 1.6905,\n",
       "                      1.6980, 1.6677, 1.6954, 1.7021, 1.7455, 1.6987, 1.6858, 1.6974, 1.7052,\n",
       "                      1.6792, 1.7295, 1.7078, 1.7011, 1.6983, 1.7285, 1.7024, 1.7376, 1.7069,\n",
       "                      1.7508, 1.7186, 1.7068, 1.7046, 1.7179, 1.7372, 1.6882, 1.7149, 1.7317,\n",
       "                      1.7270, 1.6965, 1.7009, 1.7300, 1.7171, 1.6884, 1.7030, 1.6576, 1.6921,\n",
       "                      1.7141, 1.6819, 1.6962, 1.7160, 1.6980, 1.6897, 1.7328, 1.7035, 1.7228,\n",
       "                      1.6788, 1.7438])),\n",
       "             ('blocks.1.message.Attention.actk.alpha',\n",
       "              tensor([1.0101, 1.0006, 1.0330, 1.0117, 1.0054, 1.1635, 1.0058, 1.0356, 1.0758,\n",
       "                      1.0084, 0.9999, 1.0026, 1.0070, 1.0634, 0.9834, 1.0074, 1.0157, 0.9902,\n",
       "                      1.0805, 0.9874, 0.9999, 0.9989, 0.9959, 1.0188, 1.0261, 1.0155, 1.0239,\n",
       "                      1.0118, 1.0794, 1.0442, 1.0042, 1.0434, 1.0456, 1.0537, 1.0711, 0.9910,\n",
       "                      1.0113, 1.0030, 0.9911, 0.9974, 1.0730, 1.1132, 1.0682, 1.0135, 1.0137,\n",
       "                      1.0120, 0.9990, 1.0754, 1.0051, 0.9988, 1.0099, 1.0209, 0.9605, 1.0102,\n",
       "                      1.0517, 1.0058, 1.0123, 1.0016, 1.0033, 1.0126, 0.9760, 1.0028, 1.0167,\n",
       "                      0.9837, 1.0027, 1.0557, 1.0678, 1.0569, 0.9869, 1.0183, 0.9807, 1.0110,\n",
       "                      1.1053, 1.0016, 1.0539, 1.0504, 1.1071, 0.9798, 1.0049, 1.0787, 0.9882,\n",
       "                      1.0887, 1.0123, 1.0599, 1.0435, 1.0117, 1.0033, 1.0105, 1.0264, 1.0007,\n",
       "                      0.9995, 1.0615, 1.0703, 1.0094, 1.0606, 1.0339, 1.0129, 1.0406, 1.0152,\n",
       "                      1.0403, 0.9997, 1.0466, 1.0084, 0.9898, 1.0775, 1.0206, 1.0135, 1.0241,\n",
       "                      0.9938, 1.0731, 0.9918, 1.0280, 1.0057, 1.0166, 1.0049, 0.9869, 1.0159,\n",
       "                      1.0047, 0.9802, 0.9942, 0.9837, 0.9925, 1.0519, 0.9975, 1.0293, 0.9906,\n",
       "                      1.0019, 1.0355])),\n",
       "             ('blocks.1.message.Attention.actk.beta',\n",
       "              tensor([1.7043, 1.7004, 1.6744, 1.7064, 1.7009, 1.7205, 1.7135, 1.7190, 1.7334,\n",
       "                      1.7140, 1.6907, 1.6996, 1.7125, 1.7486, 1.7002, 1.7007, 1.7019, 1.6978,\n",
       "                      1.7121, 1.6966, 1.7144, 1.6970, 1.6968, 1.6940, 1.7245, 1.7008, 1.7070,\n",
       "                      1.7021, 1.6914, 1.7195, 1.7059, 1.7318, 1.6981, 1.7065, 1.7397, 1.6876,\n",
       "                      1.6931, 1.6982, 1.7119, 1.7001, 1.7187, 1.7198, 1.7650, 1.6892, 1.7202,\n",
       "                      1.6969, 1.6800, 1.7831, 1.7044, 1.6955, 1.6841, 1.6832, 1.7218, 1.7039,\n",
       "                      1.6951, 1.6933, 1.6992, 1.6934, 1.6965, 1.7133, 1.7422, 1.7228, 1.7018,\n",
       "                      1.6515, 1.6901, 1.7309, 1.7544, 1.7599, 1.7094, 1.7268, 1.6915, 1.7053,\n",
       "                      1.7131, 1.7062, 1.7156, 1.6975, 1.7157, 1.6839, 1.7084, 1.6976, 1.7278,\n",
       "                      1.7095, 1.6885, 1.7317, 1.7141, 1.7217, 1.6993, 1.7087, 1.6953, 1.6949,\n",
       "                      1.7210, 1.7121, 1.7345, 1.7316, 1.7184, 1.7234, 1.6905, 1.7483, 1.7300,\n",
       "                      1.7212, 1.7071, 1.6863, 1.7096, 1.7132, 1.7228, 1.7191, 1.7063, 1.7220,\n",
       "                      1.7197, 1.6997, 1.6825, 1.7068, 1.7045, 1.7118, 1.6822, 1.6903, 1.7044,\n",
       "                      1.6891, 1.6849, 1.7064, 1.6929, 1.7050, 1.7159, 1.7032, 1.7022, 1.6920,\n",
       "                      1.6988, 1.7159])),\n",
       "             ('blocks.1.message.Attention.actv.alpha',\n",
       "              tensor([1.0001, 1.0033, 1.0141, 1.0205, 1.0055, 1.0238, 1.0201, 1.0081, 0.9945,\n",
       "                      0.9942, 1.0015, 1.0128, 1.0049, 1.0036, 0.9987, 1.0790, 1.0048, 1.0012,\n",
       "                      1.0281, 0.9970, 0.9927, 1.0011, 1.0000, 1.0030, 1.0029, 0.9948, 0.9967,\n",
       "                      0.9995, 1.0024, 1.0015, 1.0044, 1.0032, 0.9956, 1.0015, 1.0018, 1.0022,\n",
       "                      1.0033, 1.0025, 0.9929, 0.9986, 1.0048, 1.0016, 1.0028, 0.9971, 1.0064,\n",
       "                      1.0002, 1.0031, 0.9987, 0.9987, 1.0009, 1.0385, 1.0034, 0.9999, 1.0016,\n",
       "                      1.0369, 1.0020, 0.9997, 1.0128, 1.0012, 1.0030, 0.9924, 1.0018, 1.0018,\n",
       "                      1.0184, 0.9842, 1.0021, 1.0041, 1.0240, 1.0035, 1.0039, 0.9943, 0.9866,\n",
       "                      0.9957, 1.0046, 1.0137, 1.0006, 1.0032, 1.0474, 0.9968, 1.0042, 0.9916,\n",
       "                      0.9954, 1.0040, 1.0028, 1.0025, 1.0116, 1.0063, 1.0078, 1.0010, 1.0319,\n",
       "                      0.9977, 1.0211, 1.0038, 1.0028, 0.9888, 0.9942, 0.9886, 1.0022, 0.9929,\n",
       "                      0.9967, 0.9972, 1.0019, 1.0035, 1.0407, 1.0065, 1.0020, 1.0044, 0.9956,\n",
       "                      1.0027, 1.0009, 1.0029, 0.9921, 0.9850, 1.0028, 0.9987, 0.9982, 0.9928,\n",
       "                      1.0006, 1.0143, 1.0063, 1.0183, 1.0134, 1.0263, 0.9926, 0.9969, 0.9956,\n",
       "                      1.0216, 0.9955, 1.0052, 0.9983, 1.0114, 0.9986, 0.9934, 1.0001, 1.0107,\n",
       "                      0.9974, 1.0035, 0.9989, 1.0022, 1.0021, 1.0034, 1.0004, 1.0026, 0.9966,\n",
       "                      0.9981, 0.9981, 0.9987, 1.0035, 1.0109, 0.9898, 1.0063, 1.0002, 1.0010,\n",
       "                      1.0000, 0.9968, 0.9926, 1.0104, 1.0001, 1.0035, 1.0025, 1.0064, 0.9983,\n",
       "                      0.9893, 1.0024, 0.9966, 0.9898, 1.0031, 1.0678, 0.9953, 1.0044, 0.9918,\n",
       "                      0.9993, 1.0021, 1.0011, 0.9934, 1.0083, 1.0025, 0.9799, 0.9867, 1.0018,\n",
       "                      0.9997, 0.9945, 0.9753, 1.0012, 0.9927, 1.0055, 0.9980, 1.0021, 0.9954,\n",
       "                      1.0024, 0.9998, 1.0056, 1.0029, 1.0016, 1.0011, 1.0013, 0.9924, 0.9910,\n",
       "                      1.0213, 1.0107, 1.0046, 1.0042, 1.0014, 1.0118, 1.0049, 0.9871, 1.0047,\n",
       "                      1.0041, 0.9901, 1.0047, 1.0152, 0.9931, 1.0028, 0.9916, 0.9818, 1.0021,\n",
       "                      1.0021, 0.9998, 0.9975, 0.9912, 1.0049, 1.0057, 1.0241, 1.0101, 1.0039,\n",
       "                      1.0083, 1.0512, 1.0001, 0.9964, 0.9954, 0.9915, 0.9958, 1.0019, 1.0038,\n",
       "                      0.9882, 1.0014, 1.0025, 0.9847, 0.9862, 1.0028, 0.9869, 0.9986, 1.0023,\n",
       "                      1.0152, 1.0027, 0.9989, 1.0191, 1.0038, 1.0052, 1.0426, 1.0230, 0.9969,\n",
       "                      0.9984, 1.0321, 0.9955, 1.0281])),\n",
       "             ('blocks.1.message.Attention.actv.beta',\n",
       "              tensor([1.7040, 1.7036, 1.6988, 1.7254, 1.6989, 1.6842, 1.7082, 1.7044, 1.6991,\n",
       "                      1.7137, 1.7054, 1.7052, 1.7073, 1.6982, 1.7018, 1.6997, 1.6993, 1.7033,\n",
       "                      1.6980, 1.7048, 1.7035, 1.7047, 1.7031, 1.7003, 1.7005, 1.7102, 1.6952,\n",
       "                      1.7005, 1.7041, 1.7036, 1.6960, 1.7060, 1.6859, 1.7028, 1.7021, 1.7065,\n",
       "                      1.7062, 1.7058, 1.7144, 1.6923, 1.6989, 1.7021, 1.7036, 1.7041, 1.6977,\n",
       "                      1.7026, 1.6997, 1.6931, 1.7031, 1.7032, 1.7090, 1.7058, 1.7023, 1.6979,\n",
       "                      1.7450, 1.7043, 1.7008, 1.7277, 1.7083, 1.7070, 1.6993, 1.7088, 1.7037,\n",
       "                      1.7196, 1.6896, 1.7045, 1.7082, 1.7099, 1.7022, 1.7001, 1.7018, 1.7230,\n",
       "                      1.7022, 1.7014, 1.7042, 1.6998, 1.7053, 1.7092, 1.6888, 1.7000, 1.6936,\n",
       "                      1.7007, 1.7070, 1.7051, 1.7000, 1.7089, 1.7039, 1.7034, 1.7028, 1.7458,\n",
       "                      1.7016, 1.7294, 1.7042, 1.7105, 1.6778, 1.7119, 1.7180, 1.7047, 1.7077,\n",
       "                      1.7049, 1.7136, 1.7018, 1.7064, 1.7246, 1.6981, 1.7040, 1.6977, 1.7131,\n",
       "                      1.7024, 1.7026, 1.7060, 1.6954, 1.7092, 1.7062, 1.7022, 1.7100, 1.6928,\n",
       "                      1.7051, 1.7244, 1.7146, 1.7010, 1.6906, 1.7476, 1.7066, 1.6957, 1.6960,\n",
       "                      1.7064, 1.7038, 1.6991, 1.7319, 1.7023, 1.7024, 1.6948, 1.7019, 1.6988,\n",
       "                      1.7021, 1.7044, 1.7044, 1.7050, 1.7016, 1.7044, 1.7026, 1.7021, 1.6928,\n",
       "                      1.7035, 1.7097, 1.6747, 1.6994, 1.7095, 1.6996, 1.7085, 1.7011, 1.7021,\n",
       "                      1.7028, 1.6865, 1.6927, 1.6999, 1.6983, 1.7036, 1.7050, 1.7260, 1.7066,\n",
       "                      1.7018, 1.7044, 1.6999, 1.7088, 1.7041, 1.7157, 1.7038, 1.7004, 1.7100,\n",
       "                      1.6979, 1.7057, 1.7051, 1.7010, 1.6984, 1.7053, 1.7030, 1.6920, 1.7076,\n",
       "                      1.7010, 1.7063, 1.6859, 1.7022, 1.7057, 1.7084, 1.6984, 1.7039, 1.6956,\n",
       "                      1.7034, 1.7061, 1.7064, 1.7050, 1.7045, 1.7055, 1.7029, 1.6948, 1.7160,\n",
       "                      1.7033, 1.7014, 1.6991, 1.7030, 1.7048, 1.6986, 1.7027, 1.7128, 1.7015,\n",
       "                      1.7065, 1.7071, 1.6981, 1.6998, 1.6940, 1.7032, 1.7120, 1.6845, 1.7008,\n",
       "                      1.7046, 1.6998, 1.6998, 1.7093, 1.7036, 1.7067, 1.7078, 1.7006, 1.7023,\n",
       "                      1.7086, 1.7137, 1.7025, 1.6975, 1.7063, 1.6937, 1.7176, 1.7013, 1.6999,\n",
       "                      1.7170, 1.7031, 1.7058, 1.7042, 1.6894, 1.6943, 1.7086, 1.7040, 1.7036,\n",
       "                      1.6986, 1.7050, 1.7065, 1.7106, 1.7058, 1.6993, 1.7027, 1.7089, 1.6970,\n",
       "                      1.6997, 1.7222, 1.6957, 1.6868])),\n",
       "             ('blocks.1.message.Attention.acta.alpha',\n",
       "              tensor([0.9816, 1.0358, 1.0404, 0.9970, 1.0066, 1.0256, 1.0234, 0.9555, 0.9887,\n",
       "                      1.0149, 1.0153, 1.0507, 0.9976, 1.0447, 1.0306, 0.9441, 1.0409, 1.0142,\n",
       "                      1.0004, 1.0209, 0.9777, 1.0042, 1.0047, 0.9842, 1.0264, 1.0544, 1.0408,\n",
       "                      1.0120, 0.9890, 1.0007, 1.0134, 0.9799, 1.0286, 0.9830, 1.0156, 1.0005,\n",
       "                      0.9750, 1.0077, 0.9840, 0.9755, 0.9740, 1.0310, 1.0359, 0.9716, 0.9630,\n",
       "                      1.0115, 1.0319, 0.9798, 1.0453, 0.9970, 1.0192, 1.0672, 1.0335, 1.0172,\n",
       "                      1.0057, 1.0143, 0.9872, 1.0742, 1.0107, 1.0123, 0.9598, 1.0062, 0.9382,\n",
       "                      0.9597, 0.9932, 1.0315, 0.9570, 1.0273, 0.9988, 1.0135, 0.9872, 0.9981,\n",
       "                      0.9813, 1.0069, 1.0351, 1.0200, 1.0182, 0.9736, 1.0429, 0.9935, 0.9955,\n",
       "                      1.0292, 1.0058, 0.9689, 0.9858, 1.0175, 0.9782, 0.9812, 0.9895, 1.0279,\n",
       "                      1.0344, 0.9919, 1.0170, 1.0280, 0.9839, 1.0497, 1.0031, 1.0004, 0.9959,\n",
       "                      1.0012, 0.9996, 1.0410, 1.0202, 0.9947, 1.0494, 0.9241, 0.9969, 0.9650,\n",
       "                      0.9950, 0.9862, 1.0127, 1.0024, 1.0098, 0.9808, 1.0014, 1.0646, 1.0498,\n",
       "                      1.0122, 0.9798, 1.0042, 1.0209, 0.9970, 0.9681, 0.9459, 1.0034, 0.9981,\n",
       "                      1.0346, 0.9992, 1.0017, 1.0026, 1.0014, 0.9919, 0.9943, 1.0033, 1.0024,\n",
       "                      1.0119, 1.0170, 1.0035, 1.0027, 0.9992, 1.0036, 0.9954, 1.0012, 1.0052,\n",
       "                      1.0007, 1.0006, 1.0015, 0.9991, 0.9988, 1.0353, 0.9976, 1.0067, 1.0015,\n",
       "                      0.9907, 0.9959, 0.9980, 0.9979, 1.0032, 0.9999, 1.0127, 1.0221, 1.0018,\n",
       "                      0.9900, 1.0022, 1.0055, 1.0176, 1.0165, 1.0033, 0.9983, 1.0014, 1.0130,\n",
       "                      0.9989, 1.0009, 0.9851, 1.0067, 0.9929, 1.0072, 1.0028, 1.0101, 0.9943,\n",
       "                      0.9983, 0.9993, 1.0269, 1.0028, 1.0058, 0.9964, 1.0048, 1.0034, 1.0007,\n",
       "                      1.0022, 0.9973, 1.0065, 1.0018, 1.0022, 0.9915, 1.0055, 1.0056, 1.0048,\n",
       "                      1.0041, 1.0085, 0.9994, 1.0081, 1.0028, 0.9991, 0.9962, 1.0018, 1.0066,\n",
       "                      1.0014, 1.0145, 0.9981, 1.0013, 1.0086, 1.0000, 0.9992, 1.0029, 1.0025,\n",
       "                      1.0008, 1.0011, 1.0000, 1.0103, 1.0033, 0.9983, 1.0025, 0.9992, 1.0027,\n",
       "                      1.0022, 0.9813, 0.9993, 1.0012, 1.0005, 1.0040, 0.9973, 1.0061, 0.9752,\n",
       "                      1.0040, 1.0053, 0.9981, 1.0096, 1.0039, 1.0145, 0.9988, 1.0027, 1.0024,\n",
       "                      1.0021, 1.0029, 1.0037, 1.0051, 0.9913, 0.9967, 0.9953, 0.9961, 1.0097,\n",
       "                      1.0007, 1.0062, 1.0078, 1.0069])),\n",
       "             ('blocks.1.message.Attention.acta.beta',\n",
       "              tensor([1.6957, 1.7772, 1.7400, 1.6735, 1.7043, 1.7058, 1.6751, 1.6636, 1.7017,\n",
       "                      1.7427, 1.5989, 1.7962, 1.7017, 1.6569, 1.7566, 1.7503, 1.7041, 1.6791,\n",
       "                      1.6544, 1.7373, 1.6635, 1.7259, 1.7406, 1.7592, 1.8049, 1.6525, 1.6834,\n",
       "                      1.7233, 1.6670, 1.7173, 1.7374, 1.7275, 1.6915, 1.7201, 1.7270, 1.6841,\n",
       "                      1.6727, 1.6847, 1.6945, 1.6888, 1.7295, 1.6566, 1.6969, 1.6704, 1.6762,\n",
       "                      1.7195, 1.7230, 1.7074, 1.7150, 1.6591, 1.7260, 1.6632, 1.7087, 1.7218,\n",
       "                      1.6998, 1.6729, 1.6860, 1.8365, 1.6884, 1.7413, 1.6688, 1.7446, 1.6952,\n",
       "                      1.7018, 1.6920, 1.6858, 1.6968, 1.7040, 1.6441, 1.7323, 1.7133, 1.7267,\n",
       "                      1.6841, 1.7127, 1.6673, 1.7337, 1.7407, 1.6566, 1.7463, 1.7276, 1.6886,\n",
       "                      1.7140, 1.7068, 1.7275, 1.7041, 1.6957, 1.7212, 1.6827, 1.6707, 1.6309,\n",
       "                      1.6898, 1.7621, 1.6977, 1.7735, 1.6937, 1.7256, 1.6951, 1.7026, 1.7498,\n",
       "                      1.6692, 1.7223, 1.7051, 1.6650, 1.7194, 1.6981, 1.7556, 1.6867, 1.7449,\n",
       "                      1.6974, 1.6797, 1.7214, 1.6878, 1.7312, 1.6791, 1.7314, 1.7559, 1.7070,\n",
       "                      1.6525, 1.6667, 1.7305, 1.6872, 1.6975, 1.7691, 1.7413, 1.7357, 1.7100,\n",
       "                      1.7179, 1.6813, 1.7041, 1.7020, 1.7044, 1.6858, 1.7097, 1.7019, 1.7075,\n",
       "                      1.7016, 1.7157, 1.7057, 1.7052, 1.7071, 1.7102, 1.6747, 1.6921, 1.6782,\n",
       "                      1.6989, 1.6902, 1.7282, 1.7020, 1.6824, 1.6952, 1.7030, 1.7026, 1.7043,\n",
       "                      1.7032, 1.6950, 1.7011, 1.6930, 1.6935, 1.6947, 1.7136, 1.7272, 1.6946,\n",
       "                      1.6909, 1.7017, 1.7090, 1.6951, 1.7256, 1.7062, 1.7298, 1.7016, 1.6833,\n",
       "                      1.7140, 1.7041, 1.7011, 1.7021, 1.6710, 1.7258, 1.7063, 1.6948, 1.6772,\n",
       "                      1.7368, 1.7037, 1.7019, 1.7077, 1.7034, 1.7226, 1.7008, 1.7041, 1.7074,\n",
       "                      1.7028, 1.6984, 1.7134, 1.6881, 1.7020, 1.6974, 1.7186, 1.7001, 1.7058,\n",
       "                      1.7030, 1.7525, 1.7006, 1.6989, 1.7076, 1.6979, 1.6976, 1.6976, 1.7045,\n",
       "                      1.7031, 1.6888, 1.7046, 1.7027, 1.6844, 1.7088, 1.6964, 1.7026, 1.6969,\n",
       "                      1.7046, 1.6906, 1.7005, 1.7055, 1.7007, 1.7123, 1.7041, 1.6982, 1.7045,\n",
       "                      1.7002, 1.7002, 1.6866, 1.6838, 1.7026, 1.6986, 1.7087, 1.7107, 1.6607,\n",
       "                      1.6926, 1.6832, 1.6895, 1.6990, 1.7104, 1.6989, 1.7072, 1.7042, 1.6976,\n",
       "                      1.7009, 1.6951, 1.6994, 1.7551, 1.6746, 1.6921, 1.6938, 1.6975, 1.6876,\n",
       "                      1.7041, 1.7105, 1.7148, 1.6893])),\n",
       "             ('blocks.1.message.Attention.lq.weight',\n",
       "              tensor([[-0.0332, -0.0709, -0.0784,  ..., -0.2015, -0.1211, -0.0444],\n",
       "                      [ 0.1054,  0.0874, -0.0852,  ..., -0.1511, -0.1212,  0.1195],\n",
       "                      [ 0.1445,  0.1261, -0.1680,  ..., -0.1229, -0.1300,  0.0720],\n",
       "                      ...,\n",
       "                      [-0.0819,  0.0261, -0.1379,  ..., -0.0506, -0.0281, -0.1259],\n",
       "                      [ 0.1171,  0.1026, -0.0520,  ..., -0.0655, -0.1208, -0.0978],\n",
       "                      [ 0.0446, -0.0233,  0.0624,  ..., -0.0094,  0.0062, -0.0206]])),\n",
       "             ('blocks.1.message.Attention.lq.bias',\n",
       "              tensor([-0.0013,  0.0419,  0.0349,  0.0200, -0.0019, -0.0196, -0.0058, -0.0017,\n",
       "                       0.0223, -0.0192,  0.0225,  0.0525,  0.0041,  0.0515, -0.0096,  0.0310,\n",
       "                      -0.0010, -0.0135,  0.0081, -0.0209,  0.0018,  0.0474, -0.0115, -0.0064,\n",
       "                      -0.0051,  0.0031,  0.0085, -0.0156, -0.0011,  0.0283, -0.0101,  0.0321,\n",
       "                       0.0088,  0.0434,  0.0184,  0.0358,  0.0339,  0.0603, -0.0010, -0.0162,\n",
       "                       0.0405, -0.0172,  0.0034,  0.0608,  0.0477,  0.0178,  0.0177, -0.0017,\n",
       "                       0.0003, -0.0147, -0.0215,  0.0022,  0.0028,  0.0080, -0.0064, -0.0140,\n",
       "                       0.0191,  0.0029,  0.0023, -0.0201,  0.0557,  0.0208,  0.0068, -0.0038,\n",
       "                      -0.0033,  0.0362, -0.0145,  0.0226, -0.0405, -0.0277,  0.0103,  0.0024,\n",
       "                       0.0167, -0.0049, -0.0090, -0.0338, -0.0088,  0.0099, -0.0134, -0.0165,\n",
       "                       0.0120,  0.0043,  0.0184,  0.0255,  0.0002,  0.0271, -0.0024,  0.0146,\n",
       "                      -0.0190, -0.0071, -0.0090,  0.0228,  0.0124,  0.0011,  0.0008,  0.0271,\n",
       "                       0.0003,  0.0412,  0.0186,  0.0392,  0.0287,  0.0013,  0.0037, -0.0003,\n",
       "                       0.0480,  0.0131,  0.0150,  0.0454,  0.0383,  0.0159,  0.0018,  0.0338,\n",
       "                       0.0129,  0.0040, -0.0153,  0.0010,  0.0251, -0.0140, -0.0225, -0.0013,\n",
       "                       0.0076, -0.0055,  0.0017,  0.0300,  0.0309, -0.0129,  0.0362,  0.0275])),\n",
       "             ('blocks.1.message.Attention.lk.weight',\n",
       "              tensor([[ 0.1248,  0.0973,  0.1106,  ..., -0.0554,  0.0651, -0.0854],\n",
       "                      [ 0.0420, -0.0554,  0.0405,  ..., -0.0189, -0.0032, -0.0407],\n",
       "                      [ 0.0543, -0.1690, -0.1536,  ...,  0.0373, -0.1015, -0.0808],\n",
       "                      ...,\n",
       "                      [-0.0693,  0.1341, -0.1063,  ...,  0.0979, -0.0103,  0.0908],\n",
       "                      [ 0.1105,  0.1696, -0.1089,  ..., -0.0258, -0.0826,  0.0294],\n",
       "                      [-0.1101, -0.0952,  0.0816,  ...,  0.1140, -0.0775, -0.0453]])),\n",
       "             ('blocks.1.message.Attention.lk.bias',\n",
       "              tensor([ 0.0034, -0.0005, -0.0046,  0.0040,  0.0045,  0.0919, -0.0081,  0.0339,\n",
       "                      -0.0109,  0.0108, -0.0055,  0.0111,  0.0067,  0.0098, -0.0085,  0.0033,\n",
       "                       0.0062,  0.0095,  0.0397, -0.0111, -0.0020, -0.0058, -0.0038, -0.0015,\n",
       "                       0.0227,  0.0212, -0.0029,  0.0137,  0.0145,  0.0844,  0.0039,  0.0274,\n",
       "                       0.0074,  0.0081,  0.0628, -0.0190,  0.0065, -0.0145,  0.0154, -0.0028,\n",
       "                       0.0361,  0.0561,  0.0702, -0.0063, -0.0170,  0.0070, -0.0061,  0.0732,\n",
       "                      -0.0061, -0.0078,  0.0025,  0.0129, -0.0175,  0.0052,  0.0316,  0.0069,\n",
       "                       0.0269, -0.0010,  0.0036,  0.0097, -0.0219,  0.0091,  0.0037, -0.0096,\n",
       "                      -0.0029,  0.0608,  0.0099,  0.0355, -0.0088,  0.0208, -0.0187,  0.0025,\n",
       "                       0.0695,  0.0018,  0.0287,  0.0080,  0.0497, -0.0161,  0.0026,  0.0399,\n",
       "                      -0.0188,  0.0317, -0.0168,  0.0267,  0.0293,  0.0060, -0.0001,  0.0044,\n",
       "                       0.0072,  0.0005,  0.0120,  0.0573,  0.0537,  0.0177,  0.0399,  0.0561,\n",
       "                      -0.0148,  0.0152,  0.0305,  0.0351, -0.0014,  0.0197,  0.0005, -0.0025,\n",
       "                       0.0500,  0.0163,  0.0054,  0.0047,  0.0039,  0.0103, -0.0116,  0.0245,\n",
       "                       0.0026,  0.0167, -0.0048, -0.0099,  0.0168,  0.0065, -0.0154, -0.0001,\n",
       "                      -0.0112, -0.0041,  0.0030,  0.0010,  0.0140, -0.0077,  0.0049,  0.0205])),\n",
       "             ('blocks.1.message.Attention.lv.weight',\n",
       "              tensor([[ 0.0529, -0.0818,  0.0633,  ...,  0.0707, -0.1049, -0.0438],\n",
       "                      [-0.0392,  0.0416, -0.0220,  ...,  0.0249,  0.0644,  0.1211],\n",
       "                      [ 0.1311,  0.0004,  0.1849,  ..., -0.0672, -0.0638, -0.1035],\n",
       "                      ...,\n",
       "                      [-0.0061,  0.1324,  0.0636,  ..., -0.0101,  0.1338, -0.0795],\n",
       "                      [-0.0112,  0.0298, -0.0652,  ...,  0.1180, -0.0915, -0.1021],\n",
       "                      [ 0.0174,  0.0261, -0.1403,  ...,  0.0825, -0.1368,  0.0489]])),\n",
       "             ('blocks.1.message.Attention.lv.bias',\n",
       "              tensor([ 3.2073e-03,  2.5565e-03, -6.3702e-03,  9.3986e-03, -7.9488e-03,\n",
       "                       8.0826e-03, -3.5066e-03,  6.8460e-03, -4.2403e-03, -1.0204e-02,\n",
       "                       2.7179e-03,  1.9402e-02,  6.5173e-03, -7.2860e-03, -1.7543e-03,\n",
       "                       7.0138e-02,  2.7799e-03,  1.5491e-03, -8.1838e-04, -7.7592e-04,\n",
       "                      -9.2231e-03,  3.1115e-03,  2.3050e-03, -7.0234e-03, -1.2438e-02,\n",
       "                      -9.7329e-03, -4.9300e-03, -8.4956e-03,  4.6468e-03,  2.6595e-03,\n",
       "                      -8.8414e-03,  4.4154e-03, -1.7662e-03,  2.3312e-03,  3.9762e-03,\n",
       "                       4.2263e-03,  3.4298e-03,  4.2684e-03,  2.4688e-03,  6.0888e-03,\n",
       "                      -9.3784e-03, -1.0409e-02,  1.7804e-03, -5.7129e-03, -1.3863e-02,\n",
       "                       3.4398e-03, -1.1333e-02, -4.1809e-05,  2.0390e-03,  2.6739e-03,\n",
       "                       4.8724e-03,  4.3493e-03,  2.6124e-03, -1.6578e-03,  3.0991e-02,\n",
       "                       2.3243e-03, -7.2984e-03,  9.9051e-03,  4.0447e-03,  3.9583e-03,\n",
       "                      -2.4773e-03,  3.4171e-03,  2.3023e-03,  8.7440e-03, -1.0361e-02,\n",
       "                       3.0185e-03,  6.0588e-03,  2.8826e-02,  2.1435e-03,  5.7058e-03,\n",
       "                       3.4061e-03,  2.4849e-03, -9.9286e-03, -8.2466e-03, -8.5960e-03,\n",
       "                       1.4151e-03,  4.5473e-03,  5.0471e-03, -1.4463e-02, -1.2579e-02,\n",
       "                      -8.3964e-03,  1.5959e-03,  4.9221e-03,  4.6013e-03, -1.4094e-02,\n",
       "                       2.5966e-03, -6.7991e-03,  5.2006e-03,  1.3446e-03,  1.8606e-02,\n",
       "                      -6.3208e-03,  1.0262e-02,  4.1898e-03,  3.4359e-02, -1.4865e-02,\n",
       "                      -7.3190e-04, -1.0520e-02,  2.3372e-03,  9.6428e-03,  4.9654e-03,\n",
       "                       1.0519e-02, -1.8983e-02,  4.3414e-03,  4.4005e-03, -9.6805e-03,\n",
       "                       2.8261e-03, -4.9340e-03,  7.7932e-03,  3.4364e-03,  2.1314e-03,\n",
       "                       4.5033e-03, -5.2106e-03,  4.7999e-03,  3.1418e-03,  2.3792e-03,\n",
       "                      -3.2746e-03, -1.3324e-02,  4.1567e-03,  6.1188e-03,  9.7909e-03,\n",
       "                       1.7844e-02, -1.5654e-02,  1.3775e-02,  5.2661e-03, -1.2663e-03,\n",
       "                      -6.7880e-03,  5.8426e-02,  7.3752e-04, -1.3288e-02,  1.5268e-02,\n",
       "                      -4.0185e-03, -6.2513e-03, -7.1445e-03, -5.0858e-03,  6.0886e-03,\n",
       "                      -3.8203e-03,  3.8502e-03,  4.9634e-03,  3.7803e-03, -7.4925e-04,\n",
       "                       4.3829e-03,  2.0273e-03, -6.8834e-03, -9.4208e-03, -4.0132e-03,\n",
       "                       2.4429e-03, -1.8615e-02, -1.9787e-02, -9.6144e-04, -6.6460e-03,\n",
       "                       5.2489e-03,  3.3970e-03,  1.4767e-03,  4.6800e-03, -6.7350e-03,\n",
       "                      -9.9425e-03,  1.0396e-02, -1.3261e-03, -1.0353e-02,  3.6858e-03,\n",
       "                      -6.9954e-04,  8.9693e-03, -5.5423e-03,  2.8159e-03, -7.9957e-03,\n",
       "                      -4.8754e-03,  4.3160e-03,  5.9875e-02, -8.0844e-04, -1.2603e-02,\n",
       "                       6.3745e-03,  5.7665e-03,  3.7194e-03,  2.6324e-03, -7.9347e-03,\n",
       "                       2.0267e-02,  2.2902e-03, -4.5932e-03, -5.7564e-03,  4.3502e-03,\n",
       "                      -1.4148e-02,  6.3425e-03,  3.0827e-03, -1.1694e-02,  1.0269e-02,\n",
       "                       8.4188e-03, -2.1141e-03,  3.9072e-03,  3.4923e-03,  4.0939e-03,\n",
       "                       2.4117e-03,  4.1152e-03,  3.5959e-03,  2.5019e-03,  4.2146e-03,\n",
       "                       1.2933e-03, -7.3123e-03, -9.8919e-03, -5.4168e-03, -9.3723e-03,\n",
       "                      -1.6326e-02,  4.1972e-03, -4.0449e-03, -5.1226e-03, -5.3171e-03,\n",
       "                       4.4279e-03, -8.9762e-03,  5.1881e-03,  7.9517e-03, -9.9162e-03,\n",
       "                       1.8571e-02, -7.9818e-03,  6.2822e-03,  2.8995e-04, -9.5961e-03,\n",
       "                       3.1444e-03,  2.2769e-03,  1.0028e-02, -1.0635e-03,  1.1394e-02,\n",
       "                       2.8933e-03,  5.5712e-03,  4.9915e-03,  9.2629e-03, -2.0577e-02,\n",
       "                       7.6243e-03,  2.9064e-03,  9.4914e-04,  2.5974e-03,  7.5205e-03,\n",
       "                      -4.3355e-03, -4.8673e-03, -1.0116e-02, -7.0646e-03,  6.1374e-03,\n",
       "                       6.2580e-04,  4.1002e-03, -8.9548e-03, -9.6018e-03,  1.4940e-03,\n",
       "                      -2.2498e-03, -5.4793e-03,  4.6597e-03,  3.5558e-03,  8.7417e-03,\n",
       "                       9.0651e-04,  4.1658e-02,  4.5801e-03, -1.1231e-02,  2.8721e-02,\n",
       "                       5.9298e-03, -3.8455e-04, -1.3723e-03,  6.7121e-02, -5.5116e-03,\n",
       "                      -4.9060e-04])),\n",
       "             ('blocks.1.message.Attention.la.weight',\n",
       "              tensor([[ 0.0932, -0.0183,  0.0599,  ...,  0.0503, -0.0182,  0.0036],\n",
       "                      [-0.0981, -0.0209, -0.0267,  ...,  0.0685,  0.0360,  0.0954],\n",
       "                      [ 0.1159,  0.0418, -0.1179,  ..., -0.0083,  0.1135,  0.0309],\n",
       "                      ...,\n",
       "                      [-0.0800,  0.1008,  0.0729,  ...,  0.1105, -0.0746,  0.0186],\n",
       "                      [-0.0831, -0.0737,  0.0545,  ..., -0.0281, -0.0850, -0.0285],\n",
       "                      [-0.0268, -0.0137,  0.1127,  ..., -0.0156, -0.1004, -0.0665]])),\n",
       "             ('blocks.1.message.Attention.la.bias',\n",
       "              tensor([-0.0005,  0.0204,  0.0540, -0.0415,  0.0133,  0.0377, -0.0689, -0.0003,\n",
       "                       0.0161,  0.0109, -0.0048,  0.0230, -0.0263, -0.1041,  0.0339, -0.0050,\n",
       "                       0.0167,  0.0300, -0.0266, -0.0263, -0.0217, -0.0195,  0.0163,  0.0239,\n",
       "                       0.0449, -0.0892,  0.0222, -0.0145,  0.0192,  0.0675,  0.0593,  0.0363,\n",
       "                       0.0386,  0.0260,  0.0315, -0.0228,  0.0084, -0.0191, -0.0272,  0.0199,\n",
       "                       0.0643,  0.0087,  0.0369, -0.0387,  0.0167,  0.0198,  0.0362,  0.0075,\n",
       "                       0.0252, -0.0040,  0.0888,  0.0280,  0.0290,  0.0110,  0.0255,  0.0308,\n",
       "                      -0.0411,  0.0657, -0.0230,  0.0277, -0.0449,  0.0326,  0.0061,  0.0058,\n",
       "                      -0.0190,  0.0009,  0.0165,  0.0107, -0.0644,  0.0418, -0.0596,  0.0080,\n",
       "                      -0.0494, -0.0165, -0.0340,  0.0211, -0.0023,  0.0138, -0.0115,  0.0590,\n",
       "                      -0.0259, -0.0581, -0.0081,  0.0187,  0.0562,  0.0119, -0.0041, -0.0094,\n",
       "                      -0.0097, -0.0243,  0.0548,  0.0360,  0.0117, -0.0016,  0.0128,  0.0504,\n",
       "                       0.0066,  0.0462, -0.0203,  0.0242, -0.0503,  0.0318,  0.0175, -0.0090,\n",
       "                       0.0622,  0.0326, -0.0143,  0.0084, -0.0615,  0.0185,  0.0475, -0.0833,\n",
       "                      -0.0295, -0.0067,  0.0256,  0.1378,  0.0366,  0.0173, -0.0337,  0.0515,\n",
       "                      -0.0343, -0.0486, -0.0295, -0.0082, -0.0385,  0.0191, -0.0748, -0.0368,\n",
       "                       0.0062,  0.0052,  0.0039, -0.0028,  0.0048,  0.0068, -0.0066,  0.0080,\n",
       "                       0.0097,  0.0069,  0.0060, -0.0060,  0.0070, -0.0070,  0.0046, -0.0090,\n",
       "                      -0.0078,  0.0055,  0.0041, -0.0020, -0.0068, -0.0197, -0.0010, -0.0080,\n",
       "                      -0.0052, -0.0013, -0.0021, -0.0068, -0.0055, -0.0060,  0.0072,  0.0092,\n",
       "                      -0.0130, -0.0050, -0.0045,  0.0054,  0.0062,  0.0005, -0.0106, -0.0064,\n",
       "                       0.0027,  0.0056, -0.0108,  0.0054, -0.0063, -0.0075,  0.0091, -0.0060,\n",
       "                       0.0082,  0.0071, -0.0003,  0.0066,  0.0092,  0.0046,  0.0097, -0.0063,\n",
       "                       0.0076,  0.0025,  0.0082, -0.0100, -0.0056, -0.0062, -0.0069, -0.0096,\n",
       "                      -0.0077, -0.0058,  0.0035, -0.0130,  0.0073,  0.0081, -0.0046,  0.0034,\n",
       "                       0.0062,  0.0069,  0.0068,  0.0058, -0.0061, -0.0046, -0.0026, -0.0058,\n",
       "                       0.0088,  0.0018,  0.0058,  0.0042,  0.0044, -0.0054,  0.0065, -0.0073,\n",
       "                       0.0050,  0.0081, -0.0037,  0.0091,  0.0065,  0.0045, -0.0040, -0.0066,\n",
       "                       0.0060,  0.0062, -0.0015,  0.0012, -0.0078,  0.0054, -0.0072,  0.0076,\n",
       "                       0.0082, -0.0071, -0.0109, -0.0035,  0.0061, -0.0016,  0.0069, -0.0096,\n",
       "                       0.0029, -0.0072, -0.0054,  0.0062,  0.0075, -0.0075,  0.0098,  0.0020,\n",
       "                      -0.0053,  0.0066,  0.0040, -0.0157, -0.0066, -0.0090,  0.0046, -0.0090])),\n",
       "             ('blocks.1.message.Attention.lrbf.weight',\n",
       "              tensor([[ 0.0200, -0.0012, -0.0507,  ..., -0.1374, -0.1746, -0.0573],\n",
       "                      [-0.1020,  0.0403, -0.1015,  ..., -0.1456,  0.0682,  0.0848],\n",
       "                      [-0.1492, -0.1303, -0.1811,  ..., -0.0244, -0.1263, -0.1349],\n",
       "                      ...,\n",
       "                      [ 0.1925, -0.0436,  0.1322,  ...,  0.1078, -0.0575,  0.1570],\n",
       "                      [ 0.1575, -0.0802,  0.1780,  ..., -0.0144,  0.0045,  0.1255],\n",
       "                      [-0.1280, -0.0011,  0.1552,  ...,  0.1839,  0.0085, -0.0380]])),\n",
       "             ('blocks.1.message.Attention.lkrbf.weight',\n",
       "              tensor([[ 0.0214,  0.0802,  0.0835,  ...,  0.1673,  0.0866, -0.0345],\n",
       "                      [ 0.1150, -0.0655,  0.1471,  ..., -0.0922,  0.1391,  0.0502],\n",
       "                      [ 0.1643,  0.0824,  0.1565,  ...,  0.0897, -0.2399, -0.1961],\n",
       "                      ...,\n",
       "                      [ 0.1444, -0.1032,  0.0537,  ...,  0.0711, -0.0098,  0.0823],\n",
       "                      [ 0.1096,  0.0102, -0.0385,  ...,  0.1534,  0.0614,  0.0957],\n",
       "                      [-0.1022, -0.0328,  0.1244,  ..., -0.0377,  0.1561,  0.0101]])),\n",
       "             ('blocks.1.message.Attention.lvrbf.weight',\n",
       "              tensor([[ 0.1170,  0.1053, -0.0557,  ..., -0.1086,  0.0567, -0.0673],\n",
       "                      [ 0.0355, -0.0112, -0.0845,  ...,  0.0907, -0.0593, -0.0523],\n",
       "                      [-0.0567,  0.0213, -0.0902,  ..., -0.1048, -0.0080, -0.0507],\n",
       "                      ...,\n",
       "                      [ 0.0960, -0.0454,  0.1122,  ...,  0.0955,  0.1211, -0.0186],\n",
       "                      [ 0.0321,  0.0677, -0.0683,  ...,  0.1080, -0.0360, -0.1202],\n",
       "                      [ 0.0691,  0.0391,  0.0962,  ...,  0.0797,  0.0739, -0.0197]])),\n",
       "             ('blocks.1.message.tp.weight',\n",
       "              tensor([-6.3934e-01,  1.7346e+00, -9.3038e-01, -3.8243e-01,  1.7723e+00,\n",
       "                      -6.9520e-02,  5.1409e-01,  2.9667e-01, -7.3604e-01,  9.7140e-01,\n",
       "                      -1.2057e+00,  9.6693e-01, -1.6400e+00,  1.9028e-01,  1.4598e+00,\n",
       "                       1.2648e+00, -7.4983e-01,  3.6858e-01,  1.2337e+00, -1.2986e+00,\n",
       "                      -1.4680e-01,  1.2897e+00, -1.3929e-01,  8.8287e-01,  1.6876e+00,\n",
       "                       7.0158e-01, -1.6786e+00, -4.7531e-01,  1.0664e+00, -7.1692e-01,\n",
       "                      -5.0407e-01, -6.6452e-01, -3.9627e-01, -2.8210e-01, -1.3521e+00,\n",
       "                       1.9831e-01,  6.3290e-01,  1.2244e+00,  7.6985e-01,  7.9735e-01,\n",
       "                      -2.9234e-01,  1.5627e+00,  5.7796e-01,  8.2441e-01,  4.2801e-01,\n",
       "                       1.0635e+00,  2.8760e-02,  2.0076e-01, -6.1287e-01,  1.4259e+00,\n",
       "                       1.3560e-01, -1.7538e-01, -1.4757e-01, -7.1012e-01, -1.5890e+00,\n",
       "                       3.8177e+00,  1.7517e-01,  9.4161e-01,  6.1433e-01,  3.0957e-01,\n",
       "                      -1.5851e+00,  1.2200e+00,  7.7783e-01, -8.5430e-01, -7.9030e-01,\n",
       "                      -6.7166e-02, -3.5636e-02,  9.5688e-02,  1.8855e-01,  3.9335e-01,\n",
       "                       2.7580e-01, -1.3280e+00,  2.4618e-01, -7.9931e-01, -1.0984e+00,\n",
       "                      -1.7869e+00, -1.5092e-01,  2.7463e-01, -1.5225e-01,  7.5442e-02,\n",
       "                      -2.7931e+00, -1.0287e+00, -2.4987e-01,  6.7243e-01,  5.3372e-02,\n",
       "                      -1.3602e-01,  1.7037e+00,  1.4346e+00,  1.2237e+00, -1.2100e+00,\n",
       "                      -1.0789e-01,  9.5232e-01,  1.8613e+00,  1.0490e+00, -1.2019e+00,\n",
       "                      -7.5526e-01, -9.9842e-01,  2.8782e+00,  3.0205e-01, -2.8649e-01,\n",
       "                      -1.5080e-02,  1.0991e+00,  1.0192e+00, -1.0524e+00,  1.7738e+00,\n",
       "                      -1.1410e-02,  3.7622e-01, -2.0719e-02, -2.6281e-01,  1.0169e+00,\n",
       "                       1.6253e-01,  2.7231e-02,  4.2476e-01,  1.4339e+00,  3.5595e-03,\n",
       "                      -1.0142e+00,  3.7980e-01,  2.4173e+00,  3.4790e-01,  1.1713e-01,\n",
       "                       3.7327e-01,  4.4852e-01, -4.5596e-01,  1.1500e+00, -1.6342e+00,\n",
       "                       1.1025e+00,  3.9785e-02,  1.2967e+00,  1.5148e+00, -8.0101e-01,\n",
       "                       3.3642e-01, -9.7131e-01, -7.3556e-01, -7.0110e-01, -2.4913e+00,\n",
       "                       1.4241e-01, -6.9999e-01, -3.2566e-01,  2.8709e-01,  7.1873e-01,\n",
       "                       4.2231e-02,  2.1037e+00,  1.1728e+00,  1.4985e-01,  2.3326e+00,\n",
       "                       1.1204e+00,  1.0464e+00,  1.5146e+00, -3.6476e-01, -5.6768e-02,\n",
       "                      -5.4801e-01,  3.1647e-01,  1.3515e+00,  7.6815e-01,  2.5728e-01,\n",
       "                       1.9054e-01,  3.8117e-02,  1.4017e+00, -3.7373e-01, -1.3386e+00,\n",
       "                       8.9119e-01, -6.8192e-01,  1.3341e+00,  1.8144e+00,  3.2087e-01,\n",
       "                      -2.5302e-01, -8.7798e-01,  9.7124e-01, -1.0290e+00, -3.4317e-01,\n",
       "                       9.7340e-01, -1.6500e+00,  1.0374e+00,  5.4076e-01,  1.5139e+00,\n",
       "                       7.9538e-01, -4.0742e-01,  2.9195e-01,  6.9905e-01,  7.6592e-03,\n",
       "                       9.9316e-01, -1.8600e+00, -1.4192e+00,  4.5141e-01, -6.3298e-01,\n",
       "                      -6.9758e-01, -4.7871e-01, -6.9118e-01,  1.2450e+00,  1.0166e+00,\n",
       "                       2.1214e+00,  7.2762e-02,  3.6805e-01,  1.1946e+00,  7.5382e-01,\n",
       "                      -1.1135e+00, -5.2312e-01,  1.5960e+00, -1.6742e+00,  1.3523e+00,\n",
       "                      -1.0693e+00, -3.4019e-01,  1.4230e+00,  7.6073e-02,  6.2359e-02,\n",
       "                       1.6241e+00, -9.3731e-01,  1.1751e+00, -1.0765e+00, -4.0384e-02,\n",
       "                      -6.0240e-01,  3.4347e-02, -4.6728e-01, -1.1885e+00, -8.9082e-01,\n",
       "                      -1.3529e+00, -7.6437e-02,  2.8169e-01,  2.6167e-01, -1.9580e+00,\n",
       "                       1.8862e+00,  5.8931e-01,  4.9968e-01, -3.6226e-01, -2.9402e-01,\n",
       "                      -1.7081e+00, -9.9136e-01,  1.6399e+00,  1.2315e+00, -6.6794e-01,\n",
       "                      -3.9582e-01, -1.2762e+00,  9.9586e-01,  8.8402e-01, -5.5518e-01,\n",
       "                       7.3669e-02, -1.0446e+00, -9.1126e-01, -8.0557e-01,  4.5294e-01,\n",
       "                      -9.3337e-01,  3.7877e-01,  1.3704e+00,  1.3933e+00,  8.6611e-01,\n",
       "                      -4.3281e-02, -5.9583e-01,  7.7828e-01,  4.1700e-01,  1.4141e+00,\n",
       "                       2.1227e+00, -2.3114e-01,  2.3187e-01,  3.0973e-01, -5.9911e-01,\n",
       "                      -2.2574e+00, -1.0912e+00,  5.6656e-01,  1.4825e+00,  5.4860e-01,\n",
       "                      -9.5114e-01,  5.5878e-01, -4.7989e-01, -1.3414e+00,  3.2441e-01,\n",
       "                      -6.5477e-02,  9.1834e-01, -6.5984e-02, -1.5732e+00, -3.5140e-01,\n",
       "                       7.1616e-01, -1.6619e+00, -5.3083e-02,  8.9198e-01,  2.5386e-01,\n",
       "                      -2.4338e+00, -1.0950e+00, -3.3869e-01,  1.9091e+00,  1.1139e+00,\n",
       "                      -9.6499e-01, -1.4826e+00, -2.0153e-01, -8.2692e-01, -7.7460e-01,\n",
       "                       1.4770e+00,  5.7520e-01,  6.1030e-01,  1.3607e+00,  2.0398e+00,\n",
       "                      -1.9452e-01,  1.7910e-01, -5.6949e-01,  1.4721e+00,  1.9160e+00,\n",
       "                      -5.1160e-01, -2.3715e+00, -6.5606e-01,  3.7417e-01,  6.3660e-02,\n",
       "                      -8.3263e-01, -1.2836e-01, -1.3657e-01, -1.0807e+00,  5.6523e-02,\n",
       "                       4.3334e-01,  1.0517e+00,  1.4516e-01,  1.4111e+00, -7.9555e-01,\n",
       "                      -2.9829e-02,  1.4982e+00,  5.1327e-02,  1.9114e-02, -1.9036e+00,\n",
       "                       1.7138e+00, -3.5766e-01,  9.6762e-01,  1.0469e+00,  8.7162e-01,\n",
       "                       1.5650e+00, -6.5410e-01, -9.7118e-02,  1.7889e-01, -4.8458e-01,\n",
       "                       3.9778e-01, -7.4517e-01,  1.0443e+00,  9.0492e-01,  9.8407e-01,\n",
       "                       2.7376e-01,  6.7847e-01, -1.3629e+00,  2.2044e+00, -5.4194e-02,\n",
       "                       1.0455e+00,  2.0343e+00, -1.0781e+00,  1.3228e-01, -2.1489e-01,\n",
       "                      -1.0604e-01,  2.2425e+00,  1.2840e+00, -4.4461e-01,  1.5086e+00,\n",
       "                      -4.0242e-01, -8.5408e-02,  3.1659e-01, -5.5726e-01, -2.1976e-01,\n",
       "                       1.2731e+00, -8.7620e-01,  5.8674e-01,  8.2520e-02,  3.4306e-02,\n",
       "                      -1.5308e+00, -1.4742e-01,  4.8090e-01,  1.9197e-01,  8.0898e-01,\n",
       "                      -1.1001e-01, -1.7333e+00, -7.3375e-01, -5.1127e-01, -1.0916e+00,\n",
       "                       2.1584e+00, -2.1362e-01,  3.3342e-01, -1.4827e+00,  1.1481e+00,\n",
       "                      -9.0988e-01, -7.7714e-02, -1.7640e+00,  6.5624e-01, -1.0071e-01,\n",
       "                      -5.1493e-01,  1.1013e+00, -1.4138e+00,  7.0807e-01, -1.8452e+00,\n",
       "                      -1.1078e-01,  2.6222e-01,  1.3139e+00,  1.4181e+00])),\n",
       "             ('blocks.1.message.tp.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.actu.alpha',\n",
       "              tensor([1.0194, 1.0026, 0.9831, 0.9990, 1.0037, 1.0036, 0.9999, 1.0099, 1.0035,\n",
       "                      1.0166, 0.9798, 0.9940, 1.0056, 1.0033, 1.0013, 0.9802, 1.0010, 1.0018,\n",
       "                      1.0025, 0.9936, 1.0006, 0.9846, 1.0085, 1.0026, 1.0041, 1.0045, 0.9966,\n",
       "                      1.0071, 0.9980, 1.0002, 0.9923, 1.0054, 0.9991, 1.0327, 1.0015, 1.0010,\n",
       "                      1.0049, 0.9855, 1.0242, 1.0059, 1.0023, 1.0111, 1.0048, 0.9993, 1.0001,\n",
       "                      1.0153, 1.0023, 1.0098, 0.9960, 0.9936, 1.0037, 1.0049, 0.9939, 1.0015,\n",
       "                      0.9974, 0.9988, 1.0052, 1.0012, 1.0217, 1.0041, 0.9900, 1.0011, 1.0133,\n",
       "                      0.9972, 0.9806, 0.9988, 0.9941, 1.0010, 0.9987, 0.9988, 1.0015, 0.9995,\n",
       "                      1.0088, 1.0019, 0.9970, 0.9964, 1.0039, 1.0096, 1.0044, 1.0029, 0.9924,\n",
       "                      1.0044, 1.0099, 1.0017, 1.0011, 1.0042, 1.0035, 0.9850, 0.9966, 1.0010,\n",
       "                      0.9997, 1.0043, 0.9986, 0.9857, 0.9908, 1.0046, 0.9884, 0.9960, 1.0064,\n",
       "                      1.0048, 1.0039, 0.9873, 0.9926, 1.0011, 1.0024, 1.0019, 1.0116, 1.0035,\n",
       "                      1.0716, 0.9988, 1.0111, 1.0230, 0.9971, 0.9981, 0.9995, 1.0043, 1.0009,\n",
       "                      1.0047, 0.9791, 1.0026, 1.0059, 1.0158, 1.0219, 1.0092, 0.9950, 1.0155,\n",
       "                      1.0089, 1.0063])),\n",
       "             ('blocks.1.update.actu.beta',\n",
       "              tensor([1.7197, 1.7035, 1.6962, 1.7000, 1.7000, 1.6944, 1.6984, 1.6985, 1.6921,\n",
       "                      1.7077, 1.7066, 1.6938, 1.7010, 1.7010, 1.6955, 1.7073, 1.6992, 1.7069,\n",
       "                      1.7023, 1.6829, 1.7037, 1.6974, 1.7093, 1.7046, 1.7170, 1.7012, 1.6987,\n",
       "                      1.7009, 1.7009, 1.7018, 1.6859, 1.7020, 1.7133, 1.6911, 1.7013, 1.6999,\n",
       "                      1.7010, 1.7006, 1.7621, 1.6953, 1.7120, 1.7198, 1.7234, 1.6981, 1.7019,\n",
       "                      1.7007, 1.7041, 1.7052, 1.7053, 1.7098, 1.7005, 1.7055, 1.6927, 1.7018,\n",
       "                      1.6962, 1.7011, 1.7000, 1.6991, 1.6471, 1.7021, 1.6883, 1.7016, 1.6967,\n",
       "                      1.6997, 1.6840, 1.7029, 1.7027, 1.6987, 1.7019, 1.6999, 1.7018, 1.6972,\n",
       "                      1.7156, 1.6710, 1.7054, 1.6970, 1.7011, 1.6788, 1.7017, 1.7139, 1.6941,\n",
       "                      1.7020, 1.6869, 1.7058, 1.6993, 1.6950, 1.7018, 1.6945, 1.6973, 1.7028,\n",
       "                      1.7000, 1.7046, 1.7030, 1.6868, 1.7087, 1.6975, 1.7047, 1.6978, 1.6862,\n",
       "                      1.7048, 1.7010, 1.6905, 1.6969, 1.6835, 1.6992, 1.6888, 1.6893, 1.7018,\n",
       "                      1.6919, 1.6905, 1.7121, 1.6904, 1.7118, 1.7015, 1.7010, 1.7029, 1.7030,\n",
       "                      1.6959, 1.7042, 1.7031, 1.7011, 1.6819, 1.5995, 1.6997, 1.6999, 1.6836,\n",
       "                      1.7062, 1.6893])),\n",
       "             ('blocks.1.update.outt.weight',\n",
       "              tensor([-0.1123, -0.3194,  0.8857,  ...,  0.8462, -1.2407,  1.4545])),\n",
       "             ('blocks.1.update.outt.bias', tensor([])),\n",
       "             ('blocks.1.update.outt.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.outs.weight',\n",
       "              tensor([[ 1.6277e-01,  1.4315e-01, -6.9762e-02,  ..., -9.1769e-03,\n",
       "                       -5.1683e-02, -9.1839e-02],\n",
       "                      [ 2.8623e-05,  7.6889e-02, -6.8867e-02,  ...,  5.0701e-02,\n",
       "                       -1.0533e-01,  7.4048e-02],\n",
       "                      [ 6.4611e-02, -1.3391e-02,  6.0035e-02,  ...,  4.3300e-02,\n",
       "                        4.1975e-02,  9.1057e-02],\n",
       "                      ...,\n",
       "                      [ 7.0554e-02,  9.1214e-02, -1.6226e-01,  ..., -8.6526e-02,\n",
       "                        1.0366e-01,  3.7535e-02],\n",
       "                      [-2.7113e-02,  8.6591e-02,  1.3463e-01,  ..., -1.0287e-01,\n",
       "                       -8.1616e-03,  3.3468e-02],\n",
       "                      [ 1.0378e-01,  1.2083e-01, -5.1845e-02,  ...,  2.2760e-02,\n",
       "                        1.3799e-01, -3.7283e-02]])),\n",
       "             ('blocks.1.update.outs.bias',\n",
       "              tensor([ 9.7967e-03,  7.6105e-03,  1.0931e-02,  7.0785e-03,  7.5527e-03,\n",
       "                      -6.3194e-03,  8.1280e-03, -5.4756e-03, -2.5228e-03, -1.4229e-02,\n",
       "                       3.4181e-03, -8.2257e-03, -1.0155e-02, -1.1122e-02,  2.6232e-03,\n",
       "                      -7.0970e-03,  6.2485e-03,  1.0601e-02,  9.0679e-03, -3.2807e-03,\n",
       "                       9.7371e-03,  8.2735e-03, -1.1089e-02, -1.0237e-02,  8.4462e-03,\n",
       "                      -1.0289e-02,  9.6214e-03, -8.5268e-03,  7.5322e-03,  7.4430e-03,\n",
       "                      -8.6950e-03, -1.0993e-02, -6.6782e-03, -9.7892e-03,  8.4341e-03,\n",
       "                       8.6817e-03, -9.8759e-03,  8.7685e-03,  8.0647e-03, -9.8483e-03,\n",
       "                      -8.3334e-03,  3.1104e-03, -1.0662e-02, -1.7918e-03, -1.0825e-02,\n",
       "                      -6.9051e-03,  9.2898e-03, -7.5284e-03, -1.2619e-02, -1.0104e-02,\n",
       "                      -9.0847e-03,  9.0012e-03, -9.0931e-03,  7.0241e-03, -5.1909e-03,\n",
       "                      -1.0099e-02,  8.7937e-03, -7.3494e-03, -7.7992e-03, -1.0307e-02,\n",
       "                      -1.7132e-03, -9.1021e-03, -8.8445e-03,  1.0334e-02,  5.0584e-03,\n",
       "                      -7.5125e-03,  6.3698e-03, -9.9865e-03,  1.1180e-02,  7.0599e-03,\n",
       "                       6.9314e-03,  1.0489e-02, -8.1036e-03,  4.2443e-03,  9.4699e-03,\n",
       "                      -9.8223e-03, -9.4749e-03, -7.8073e-03,  7.8501e-03,  7.7290e-03,\n",
       "                      -4.6682e-03, -9.4632e-03, -4.7519e-03,  8.5801e-03,  7.7223e-03,\n",
       "                      -1.2140e-03, -1.0265e-02,  8.3623e-03,  3.2056e-05,  8.4896e-03,\n",
       "                       7.9599e-03,  8.4942e-03,  3.5085e-03, -8.8716e-03, -6.2623e-03,\n",
       "                      -9.5398e-03, -8.1803e-03,  9.9996e-03, -8.1094e-03,  9.0902e-03,\n",
       "                      -9.2228e-03,  9.3743e-03, -1.0892e-02, -2.0780e-03,  8.4122e-03,\n",
       "                      -8.7915e-03, -7.8287e-03, -9.2581e-03, -8.6728e-03, -8.3318e-03,\n",
       "                       7.7877e-03, -8.7018e-03,  1.0009e-02, -6.7364e-03,  7.2543e-03,\n",
       "                      -1.0729e-02,  7.5097e-03, -5.0470e-03,  9.4576e-03,  7.2030e-03,\n",
       "                      -1.0277e-02, -6.5454e-03, -9.1950e-04, -9.1756e-03,  8.2182e-03,\n",
       "                      -1.0453e-02, -7.0516e-03,  1.2100e-02])),\n",
       "             ('blocks.1.update.uattn.lq.weight',\n",
       "              tensor([-0.4282,  2.8135, -0.4624,  ...,  0.3177,  0.2244,  0.3860])),\n",
       "             ('blocks.1.update.uattn.lq.bias', tensor([])),\n",
       "             ('blocks.1.update.uattn.lq.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.uattn.lk.weight',\n",
       "              tensor([-0.7201,  1.1828, -1.2021,  ...,  0.7650,  0.4509, -0.4563])),\n",
       "             ('blocks.1.update.uattn.lk.bias', tensor([])),\n",
       "             ('blocks.1.update.uattn.lk.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.uattn.lv.weight',\n",
       "              tensor([-0.7871,  0.9637, -2.1058,  ...,  0.4358, -0.1928,  1.1919])),\n",
       "             ('blocks.1.update.uattn.lv.bias', tensor([])),\n",
       "             ('blocks.1.update.uattn.lv.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.uattn.ls.weight',\n",
       "              tensor([[-0.0504, -0.1362, -0.0686,  ...,  0.1055, -0.0269, -0.0202],\n",
       "                      [ 0.0635,  0.0620,  0.0077,  ...,  0.0242, -0.1135,  0.1026],\n",
       "                      [ 0.0895, -0.0697,  0.0656,  ..., -0.0011,  0.0951,  0.0846],\n",
       "                      ...,\n",
       "                      [ 0.1176,  0.0491,  0.0418,  ..., -0.0173,  0.0956,  0.1077],\n",
       "                      [-0.0462,  0.1469, -0.0751,  ..., -0.1423, -0.0561, -0.0533],\n",
       "                      [-0.0813, -0.0207,  0.0573,  ...,  0.1299, -0.0136, -0.0625]])),\n",
       "             ('blocks.1.update.uattn.ls.bias',\n",
       "              tensor([ 1.5841e-04, -8.9822e-03,  1.3244e-02,  2.8116e-03,  6.4791e-03,\n",
       "                       7.7843e-03, -8.7004e-03,  3.1292e-03,  4.9765e-03, -1.1665e-02,\n",
       "                       1.3799e-02,  6.6106e-03, -8.2775e-03, -8.0661e-03,  5.3562e-03,\n",
       "                      -9.0827e-03,  6.8088e-03,  1.8183e-02,  9.7793e-03, -1.3859e-02,\n",
       "                       1.5888e-02,  5.6839e-03,  9.7685e-04,  7.2085e-03,  9.9009e-03,\n",
       "                       7.4282e-03,  5.9723e-03, -7.8681e-03,  4.9553e-03,  1.0119e-02,\n",
       "                       1.0929e-02,  9.1311e-03, -7.3480e-04,  8.1150e-03,  7.6278e-03,\n",
       "                       7.7336e-03,  7.8914e-03,  4.5213e-03,  8.2768e-03,  7.9855e-03,\n",
       "                      -2.0981e-02, -1.2661e-02, -6.5056e-03,  8.6235e-03, -8.8547e-03,\n",
       "                      -1.4531e-02,  2.7048e-03, -8.5284e-03,  6.2622e-05, -5.8272e-03,\n",
       "                       8.2143e-03, -5.0018e-03, -9.3902e-03, -6.4372e-03, -6.9054e-03,\n",
       "                      -8.2680e-03,  4.8002e-02, -4.8762e-03, -1.8567e-02,  9.2017e-03,\n",
       "                      -5.7481e-03, -3.5918e-03, -1.5926e-03, -8.9692e-03,  3.5964e-03,\n",
       "                      -8.6390e-03,  4.2132e-03, -8.0064e-03,  1.8200e-02,  6.0044e-03,\n",
       "                       2.6250e-03,  5.0803e-03,  1.4668e-03,  1.4400e-02,  6.2835e-03,\n",
       "                       8.0861e-03, -8.2403e-03,  9.2020e-03,  6.5971e-03, -1.6754e-04,\n",
       "                      -3.7425e-03,  6.6034e-03, -6.7163e-03, -1.1087e-02,  4.4685e-03,\n",
       "                      -5.5025e-03, -8.6134e-03, -8.5259e-04,  8.6165e-05,  5.1916e-03,\n",
       "                      -3.0365e-03,  6.9418e-03, -2.0418e-03, -2.1311e-03, -1.5191e-02,\n",
       "                      -7.3435e-03,  8.3400e-03,  1.0931e-02,  1.4776e-03,  1.6018e-02,\n",
       "                       7.4986e-03, -9.1185e-03,  1.2093e-02, -1.2322e-02,  7.3619e-03,\n",
       "                      -8.1976e-03,  7.2853e-03,  4.6482e-03,  8.5912e-03,  4.0040e-03,\n",
       "                      -3.2053e-03, -7.7872e-03,  1.6101e-02,  2.4093e-02,  6.6038e-03,\n",
       "                      -7.1156e-03,  1.1708e-02, -6.0438e-03, -4.6303e-03, -1.0695e-02,\n",
       "                      -9.0214e-03,  6.3938e-03,  3.4305e-03, -7.1040e-03, -5.5755e-03,\n",
       "                       1.0617e-02, -8.9306e-03,  1.0624e-02, -5.6710e-03, -4.6557e-03,\n",
       "                      -2.7369e-03, -3.3462e-03, -2.7834e-03, -3.4660e-03, -3.6263e-03,\n",
       "                      -4.2620e-03, -3.4239e-03, -4.1845e-03, -5.4522e-03, -4.1165e-03,\n",
       "                      -3.3278e-03, -4.3720e-03, -3.9188e-03, -3.5405e-03, -3.0026e-03,\n",
       "                      -1.3086e-04, -4.3845e-03, -3.9281e-03, -2.1450e-03, -2.3976e-03,\n",
       "                      -2.4960e-03, -5.0772e-03, -3.7402e-03, -3.2480e-03, -2.7994e-03,\n",
       "                      -3.4394e-03, -4.2898e-03, -3.6465e-03, -1.0878e-03, -1.1935e-03,\n",
       "                      -2.6712e-03, -2.4241e-03, -7.4194e-03, -7.4230e-03, -4.5242e-03,\n",
       "                      -2.8419e-03, -3.3035e-03, -5.6566e-03, -1.4895e-03, -7.0880e-03,\n",
       "                      -3.3204e-03, -4.7036e-03, -2.8641e-03, -3.0612e-03, -2.7905e-03,\n",
       "                      -2.2042e-03, -5.5726e-03, -2.6813e-03, -2.9279e-03, -1.4172e-03,\n",
       "                      -3.4342e-03, -4.3228e-03, -5.9242e-03, -2.4679e-03, -6.2703e-03,\n",
       "                      -5.3365e-03, -3.8933e-03, -7.7324e-04, -2.1337e-03, -7.4706e-03,\n",
       "                       2.0531e-03, -3.1489e-03, -4.7314e-03, -1.3584e-03, -3.7259e-03,\n",
       "                      -4.2333e-03, -5.2680e-03, -4.5684e-03, -3.8053e-03, -4.0248e-03,\n",
       "                      -7.5127e-04, -1.4202e-03, -2.0560e-03, -4.5471e-03, -4.9561e-03,\n",
       "                      -5.3874e-03, -3.6620e-03, -5.5087e-03, -8.1799e-03, -5.8742e-03,\n",
       "                      -1.9980e-03, -3.8402e-03, -5.0157e-03, -9.3209e-05, -2.2427e-03,\n",
       "                      -5.7976e-03, -2.2840e-03, -5.3958e-03, -2.6628e-03, -5.7526e-03,\n",
       "                      -4.2310e-03, -4.2279e-03, -3.3449e-03, -9.9770e-04, -6.0199e-04,\n",
       "                      -4.2464e-03, -3.3872e-03, -5.9821e-03, -3.2575e-03, -3.2690e-03,\n",
       "                      -4.4301e-03, -4.5933e-03, -4.5655e-03, -4.5586e-03, -9.7444e-04,\n",
       "                      -2.7445e-03, -2.1881e-03, -2.6854e-03, -6.0861e-03, -3.2988e-03,\n",
       "                      -5.3790e-03, -3.4775e-03, -4.0864e-03, -3.9921e-03, -2.1368e-04,\n",
       "                      -3.6573e-03, -3.3199e-03, -2.4579e-03, -1.4065e-03, -1.6762e-03,\n",
       "                      -3.2150e-03, -3.6533e-03, -4.1297e-03, -4.3944e-03, -5.6634e-03,\n",
       "                      -4.3747e-03])),\n",
       "             ('blocks.1.update.uattn.lvs.weight',\n",
       "              tensor([[-0.1245,  0.0539,  0.1589,  ...,  0.0393,  0.0645, -0.0361],\n",
       "                      [-0.1438, -0.0928, -0.0801,  ...,  0.1415, -0.0358, -0.1232],\n",
       "                      [-0.1051,  0.1424,  0.1670,  ..., -0.1599,  0.0103, -0.0853],\n",
       "                      ...,\n",
       "                      [ 0.0783, -0.1291,  0.0882,  ..., -0.0989,  0.0787,  0.0520],\n",
       "                      [ 0.1344,  0.0308,  0.0853,  ..., -0.1109, -0.0517, -0.0063],\n",
       "                      [-0.0761, -0.1097, -0.1178,  ..., -0.1100,  0.0378,  0.1646]])),\n",
       "             ('blocks.1.update.uattn.lvs.bias',\n",
       "              tensor([ 0.0082,  0.0071,  0.0105,  0.0061,  0.0086, -0.0025,  0.0069, -0.0095,\n",
       "                      -0.0012, -0.0168,  0.0293, -0.0273, -0.0094, -0.0088,  0.0064, -0.0080,\n",
       "                       0.0073,  0.0128,  0.0102, -0.0071,  0.0091,  0.0079, -0.0074, -0.0108,\n",
       "                       0.0091, -0.0146,  0.0082, -0.0083,  0.0079,  0.0098, -0.0097, -0.0211,\n",
       "                      -0.0008, -0.0051,  0.0086,  0.0079, -0.0086,  0.0108,  0.0112, -0.0117,\n",
       "                      -0.0103, -0.0010, -0.0097,  0.0060, -0.0090, -0.0089,  0.0096, -0.0085,\n",
       "                      -0.0039, -0.0098, -0.0105,  0.0135, -0.0154,  0.0087, -0.0052, -0.0091,\n",
       "                       0.0451, -0.0080, -0.0070, -0.0101,  0.0020, -0.0071, -0.0073,  0.0121,\n",
       "                       0.0059, -0.0061,  0.0064, -0.0086,  0.0177,  0.0087,  0.0092,  0.0086,\n",
       "                       0.0169,  0.0297,  0.0090, -0.0095, -0.0091, -0.0043,  0.0079,  0.0056,\n",
       "                      -0.0045, -0.0087, -0.0056,  0.0040,  0.0075, -0.0104, -0.0086,  0.0133,\n",
       "                       0.0015,  0.0078,  0.0056,  0.0087,  0.0068, -0.0054, -0.0079, -0.0089,\n",
       "                      -0.0116,  0.0428, -0.0102,  0.0103, -0.0093,  0.0110, -0.0097, -0.0033,\n",
       "                       0.0091, -0.0085, -0.0080, -0.0049, -0.0123, -0.0081,  0.0016, -0.0077,\n",
       "                       0.0155,  0.0351,  0.0088, -0.0100,  0.0109, -0.0061,  0.0002,  0.0069,\n",
       "                      -0.0091, -0.0052,  0.0018, -0.0076,  0.0079,  0.0036, -0.0075,  0.0191])),\n",
       "             ('blocks.1.update.uattn.tp1.weight', tensor([])),\n",
       "             ('blocks.1.update.uattn.tp1.output_mask',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('blocks.1.update.uattn.tp2.weight',\n",
       "              tensor([-1.5015,  0.4006,  0.7847, -1.4452, -0.9960, -2.4392, -0.0847,  2.4055,\n",
       "                       0.4854,  0.3219,  0.9691, -1.0016,  0.1441, -0.6218, -0.1710, -1.1777,\n",
       "                      -0.0975, -0.9790,  1.1423,  1.5441, -0.4552,  0.3124, -0.8413,  1.5523,\n",
       "                       2.7591, -0.1813,  0.0078,  2.1537, -0.6281,  2.6915, -0.6833,  1.1168,\n",
       "                       0.9579, -1.8080, -0.2506,  0.4419,  0.1125, -1.4242,  0.8319, -0.3147,\n",
       "                       0.0361, -0.5028, -0.1890, -0.2480,  0.8611, -1.2723, -0.3129, -1.2744,\n",
       "                      -0.0168, -0.7004, -0.3390,  1.3789,  0.4246,  1.0120,  0.2798, -0.4726,\n",
       "                       0.0610,  0.8613, -0.0260,  0.6115, -0.3502,  0.6870, -0.5259, -0.2855,\n",
       "                       1.6703,  0.5601,  0.5582,  0.9927, -0.3601,  0.1907,  1.5312, -1.3213,\n",
       "                      -0.1831,  0.2293, -0.3421,  0.8352, -0.2161, -0.4904,  1.8543, -0.8800,\n",
       "                      -0.7310, -0.4192, -0.0507,  0.0391,  0.6442,  1.5963,  0.4772,  1.4617,\n",
       "                      -0.9807, -1.6996, -0.4034, -0.2368, -1.6103,  2.1720, -0.1245,  1.0332,\n",
       "                      -0.9388, -0.6647, -1.0024, -0.9131,  0.7125,  0.9541,  0.1169, -1.0094,\n",
       "                      -0.2324,  0.7750,  0.4200, -0.2803, -0.1291, -0.0140,  0.5881, -0.6460,\n",
       "                      -1.4072,  0.1149, -0.6661, -0.2013,  0.3538,  1.0731,  1.7656,  1.2272,\n",
       "                      -1.4622, -1.5442,  0.2256,  1.0166,  1.3385,  0.3915, -0.5793,  1.3057,\n",
       "                       0.9648, -0.3581, -0.9334, -0.1539,  0.5416,  0.0486,  1.1442,  1.3331,\n",
       "                       0.4110,  0.3083, -0.7091,  0.3635,  0.7376,  0.5743, -0.1359, -0.9454,\n",
       "                       0.5604,  0.9023,  0.6209,  0.5260,  1.7635, -1.2291, -1.5666, -1.2724,\n",
       "                      -0.3408,  0.0326,  1.1323, -1.2237,  0.5094,  0.1011,  1.9799, -0.5924,\n",
       "                      -0.0539,  1.4543, -0.3410, -0.9074, -1.7278, -0.3717, -0.8049, -2.4600,\n",
       "                      -1.4429,  0.7544,  0.2646, -0.0808, -0.2728,  0.2447, -0.0712,  1.4160,\n",
       "                      -0.5745,  0.2585,  0.0361,  0.3852,  0.2360,  0.4651,  0.0961, -0.4947,\n",
       "                       0.5250,  0.6542,  1.6826,  0.7441,  0.2754, -0.8814,  2.1216, -0.8175,\n",
       "                       0.4226,  0.5850,  0.9460, -0.8881, -0.5621,  0.4209,  0.4040,  1.7003,\n",
       "                       1.9396, -0.3181,  0.0721,  0.8105,  0.1697, -0.8554,  1.0798, -0.2750,\n",
       "                       0.5068, -0.8185,  0.9012,  0.4598, -0.2286,  1.9990, -0.0677, -0.0356,\n",
       "                       1.4847, -0.2756, -0.6374, -0.3312, -1.8661,  0.0835, -0.0729, -0.5084,\n",
       "                       2.3291, -0.3377, -0.5257,  0.8501, -1.2156,  2.2010, -0.7416, -1.5668,\n",
       "                       0.9787,  0.1365,  0.2137, -0.6037, -0.3209,  0.4851, -0.1489,  1.0105,\n",
       "                       1.0213,  0.1819, -0.9831,  0.4630,  0.5836,  1.4111, -0.1968,  1.5198,\n",
       "                      -0.3739, -1.7620, -0.7611,  2.0763, -0.6810,  1.3557,  0.2445,  0.9234,\n",
       "                      -1.4176,  0.9284,  0.7229, -1.6483, -1.8922,  1.2711,  0.4998, -0.9930,\n",
       "                       1.1438, -0.1341, -1.4249,  0.4501, -0.7350, -0.5307, -1.1319,  1.6413,\n",
       "                      -0.3576, -1.1322, -0.6746, -0.6359, -0.5590,  1.7712, -1.6188, -0.0381,\n",
       "                      -0.2976,  0.5249,  0.3065,  0.8990, -0.6649,  1.6767, -1.2105,  0.8637,\n",
       "                      -1.6869,  0.2475, -0.9009, -1.7427, -0.2949,  0.1194, -0.4762, -0.9492,\n",
       "                      -1.0280, -2.7817,  0.8490,  0.2082, -0.3938, -0.0883,  0.3995, -1.6908,\n",
       "                      -0.4838,  0.7618,  0.8019,  0.3991,  0.2957,  1.6133, -1.2184,  1.9349,\n",
       "                       0.2202, -0.6783,  1.1827,  0.5090, -0.1009, -0.3231, -0.6796,  1.2198,\n",
       "                       0.2851,  0.0473,  1.0048, -1.4268,  1.0645,  1.9296,  0.5825,  0.3286,\n",
       "                      -1.1650,  0.5281, -0.4108, -1.1897,  0.7277, -1.0748,  0.4256,  2.8740,\n",
       "                      -0.2782,  0.1108,  1.2848,  1.2454,  0.6709, -0.2469,  0.2209, -0.5867,\n",
       "                       1.2539, -0.7981, -1.0398,  0.3505,  0.6843, -0.4431,  0.1137, -0.8117,\n",
       "                       0.6687,  0.7818, -0.2083, -2.2543, -0.3914,  0.4362, -0.6801, -0.5368,\n",
       "                      -0.3120,  0.3350, -0.6964,  0.8815, -1.4470, -0.9775,  1.4078, -0.2069,\n",
       "                      -1.6379, -1.6729, -0.9831,  0.7793, -0.4018,  1.2577, -0.7681,  0.4842,\n",
       "                       1.2375, -0.4967, -0.7914, -0.2552,  1.8495, -0.6073, -1.2098, -0.6630])),\n",
       "             ('blocks.1.update.uattn.tp2.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.1.update.uattn.actlvs.alpha',\n",
       "              tensor([1.0001, 0.9906, 1.0115, 1.0034, 1.0078, 1.0368, 0.9948, 1.0118, 1.0035,\n",
       "                      0.9878, 1.0310, 1.0073, 0.9917, 0.9918, 1.0087, 0.9910, 1.0058, 1.0210,\n",
       "                      1.0095, 0.9825, 1.0086, 1.0063, 1.0030, 1.0074, 1.0115, 1.0073, 1.0056,\n",
       "                      0.9924, 1.0049, 1.0183, 1.0134, 1.0092, 1.0049, 1.0103, 1.0078, 1.0080,\n",
       "                      1.0080, 1.0068, 1.0158, 1.0083, 0.9963, 0.9886, 0.9972, 1.0421, 0.9910,\n",
       "                      0.9829, 1.0031, 0.9914, 1.0088, 0.9917, 1.0085, 0.9968, 0.9890, 0.9932,\n",
       "                      0.9944, 0.9917, 1.0847, 0.9958, 0.9889, 1.0086, 1.0077, 1.0007, 1.0010,\n",
       "                      0.9916, 1.0041, 0.9933, 1.0042, 0.9915, 1.0299, 1.0069, 1.0080, 1.0065,\n",
       "                      1.0363, 1.0192, 1.0063, 1.0065, 0.9919, 1.0089, 1.0067, 1.0021, 0.9984,\n",
       "                      1.0058, 0.9972, 0.9815, 1.0045, 0.9976, 0.9912, 1.0041, 1.0094, 1.0053,\n",
       "                      0.9996, 1.0070, 1.0034, 1.0077, 0.9885, 0.9935, 1.0076, 1.0378, 1.0038,\n",
       "                      1.0151, 1.0078, 0.9904, 1.0095, 0.9854, 1.0071, 0.9919, 1.0093, 1.0081,\n",
       "                      0.9847, 1.0076, 0.9986, 0.9941, 1.0139, 1.0650, 1.0084, 0.9978, 1.0115,\n",
       "                      0.9924, 0.9961, 0.9913, 0.9910, 1.0066, 1.0141, 0.9931, 1.0017, 1.0124,\n",
       "                      0.9898, 1.0454])),\n",
       "             ('blocks.1.update.uattn.actlvs.beta',\n",
       "              tensor([1.7085, 1.7128, 1.7138, 1.7081, 1.7092, 1.7130, 1.7079, 1.7145, 1.7065,\n",
       "                      1.6902, 1.7103, 1.6946, 1.6917, 1.6923, 1.7091, 1.6924, 1.7088, 1.7180,\n",
       "                      1.7126, 1.7111, 1.7264, 1.7097, 1.7047, 1.6962, 1.7117, 1.6934, 1.6828,\n",
       "                      1.6937, 1.7091, 1.7178, 1.6987, 1.6936, 1.6916, 1.6903, 1.7102, 1.7099,\n",
       "                      1.6941, 1.7121, 1.7132, 1.6926, 1.6993, 1.6900, 1.6952, 1.7035, 1.6930,\n",
       "                      1.6827, 1.7071, 1.6949, 1.7009, 1.6919, 1.6939, 1.7115, 1.6944, 1.7117,\n",
       "                      1.6966, 1.6932, 1.7440, 1.6973, 1.6791, 1.6943, 1.7133, 1.6882, 1.6868,\n",
       "                      1.7105, 1.7070, 1.6943, 1.7094, 1.6950, 1.7161, 1.7118, 1.7145, 1.7105,\n",
       "                      1.7055, 1.7093, 1.7123, 1.6946, 1.6939, 1.6914, 1.7101, 1.7054, 1.6998,\n",
       "                      1.6969, 1.6973, 1.7161, 1.7074, 1.7045, 1.6932, 1.7114, 1.7023, 1.7094,\n",
       "                      1.7085, 1.7108, 1.7024, 1.6908, 1.6914, 1.6937, 1.6961, 1.7652, 1.6949,\n",
       "                      1.7140, 1.6946, 1.7099, 1.6944, 1.6893, 1.7116, 1.6938, 1.6917, 1.7151,\n",
       "                      1.7010, 1.6924, 1.7103, 1.6907, 1.7116, 1.7036, 1.7123, 1.6943, 1.7143,\n",
       "                      1.6943, 1.7110, 1.7100, 1.6931, 1.6884, 1.7184, 1.6918, 1.6998, 1.6927,\n",
       "                      1.6951, 1.7014])),\n",
       "             ('blocks.2.message.Attention.actq.alpha',\n",
       "              tensor([0.9983, 1.0837, 1.0075, 0.9734, 1.0724, 0.9701, 1.0044, 1.1177, 1.0332,\n",
       "                      1.0373, 0.9992, 1.1193, 1.0557, 0.9782, 1.0177, 0.9742, 1.0275, 0.9762,\n",
       "                      1.0362, 1.0636, 0.9970, 1.0045, 1.0943, 1.1023, 1.0838, 1.0362, 1.0616,\n",
       "                      1.0629, 1.0932, 1.0618, 1.0372, 1.0447, 1.0197, 1.0931, 0.9999, 1.1397,\n",
       "                      1.0712, 1.0738, 1.0704, 1.1242, 1.1399, 0.9643, 1.0184, 1.0541, 1.0641,\n",
       "                      1.1171, 1.0329, 1.0712, 1.0767, 0.9841, 1.0226, 1.0655, 1.0322, 1.0826,\n",
       "                      1.1201, 1.0918, 1.0761, 1.0554, 1.0445, 1.0410, 1.1499, 1.0588, 1.0364,\n",
       "                      1.0685, 1.0540, 1.0929, 0.9840, 1.0448, 1.0971, 1.1044, 1.0717, 1.1264,\n",
       "                      1.1434, 1.0217, 0.9951, 1.0239, 1.0400, 1.0225, 0.9936, 0.9800, 1.0188,\n",
       "                      1.0794, 1.0158, 1.1353, 1.0154, 1.0716, 1.0732, 1.0458, 1.1815, 0.9945,\n",
       "                      1.0454, 1.0522, 1.0979, 1.0748, 1.0220, 1.0779, 1.0435, 1.0610, 0.9988,\n",
       "                      1.1617, 0.9988, 1.1157, 1.0312, 1.1078, 1.1089, 1.0701, 0.9938, 1.0180,\n",
       "                      1.0664, 1.1196, 1.0593, 1.0166, 1.0039, 1.0302, 1.0291, 1.0351, 1.0736,\n",
       "                      1.0630, 1.0810, 1.0597, 1.1089, 1.1646, 0.9968, 0.9947, 0.9992, 1.0205,\n",
       "                      1.0382, 1.0365])),\n",
       "             ('blocks.2.message.Attention.actq.beta',\n",
       "              tensor([1.7044, 1.7637, 1.6909, 1.7096, 1.7138, 1.7041, 1.7563, 1.6735, 1.7433,\n",
       "                      1.7384, 1.7004, 1.6417, 1.6343, 1.6903, 1.7343, 1.6829, 1.6982, 1.7413,\n",
       "                      1.6976, 1.6532, 1.7166, 1.7365, 1.7412, 1.6963, 1.6417, 1.7495, 1.7422,\n",
       "                      1.6648, 1.7064, 1.6773, 1.7373, 1.6989, 1.6774, 1.7407, 1.7331, 1.7526,\n",
       "                      1.7342, 1.6844, 1.6635, 1.7123, 1.7175, 1.7638, 1.7265, 1.6988, 1.7372,\n",
       "                      1.6692, 1.6998, 1.7639, 1.7008, 1.7278, 1.7029, 1.6963, 1.7442, 1.7088,\n",
       "                      1.7368, 1.6867, 1.7359, 1.7173, 1.6721, 1.6973, 1.6672, 1.6753, 1.7053,\n",
       "                      1.7402, 1.7155, 1.7483, 1.6692, 1.7075, 1.6847, 1.7004, 1.7135, 1.6924,\n",
       "                      1.7598, 1.7161, 1.6896, 1.6798, 1.7353, 1.7023, 1.6929, 1.6834, 1.6855,\n",
       "                      1.7481, 1.7103, 1.7174, 1.7234, 1.7188, 1.7662, 1.6883, 1.6951, 1.7138,\n",
       "                      1.6623, 1.6829, 1.7699, 1.7295, 1.7138, 1.7188, 1.7066, 1.7537, 1.7099,\n",
       "                      1.7131, 1.6913, 1.7205, 1.7240, 1.6472, 1.7351, 1.7568, 1.6745, 1.6966,\n",
       "                      1.7436, 1.7071, 1.7318, 1.6983, 1.6895, 1.7286, 1.7178, 1.6852, 1.6976,\n",
       "                      1.6901, 1.7445, 1.7049, 1.7497, 1.7635, 1.6791, 1.6953, 1.6911, 1.6725,\n",
       "                      1.6822, 1.7040])),\n",
       "             ('blocks.2.message.Attention.actk.alpha',\n",
       "              tensor([1.0234, 1.0735, 1.0435, 1.0676, 0.9925, 1.1267, 1.0765, 1.0237, 1.0948,\n",
       "                      0.9998, 1.0119, 1.0572, 1.0848, 1.0736, 1.0460, 1.0632, 1.1240, 1.0033,\n",
       "                      1.0381, 0.9939, 1.0383, 1.0560, 1.1048, 1.0384, 1.0063, 1.0324, 0.9870,\n",
       "                      1.0016, 1.0039, 1.0944, 1.0215, 1.1496, 0.9811, 1.0377, 0.9745, 1.0035,\n",
       "                      1.1089, 1.0405, 1.0114, 0.9968, 1.0378, 0.9989, 1.0212, 1.0233, 0.9676,\n",
       "                      1.0422, 1.0816, 0.9949, 1.0615, 1.0385, 0.9930, 1.1007, 1.0708, 0.9934,\n",
       "                      1.0761, 1.0562, 1.0500, 1.0412, 1.0179, 1.1241, 1.0316, 1.0612, 1.0563,\n",
       "                      1.0133, 1.0172, 1.0885, 0.9928, 0.9726, 1.0132, 0.9955, 0.9949, 1.1198,\n",
       "                      1.0841, 1.1966, 0.9964, 1.0193, 1.0787, 1.0156, 1.0278, 1.0356, 1.0381,\n",
       "                      1.1016, 1.0131, 1.0699, 1.0276, 1.0405, 1.0438, 1.1484, 1.0934, 1.0680,\n",
       "                      1.0481, 1.0667, 0.9663, 1.1108, 1.0541, 1.0028, 0.9931, 1.0428, 1.0161,\n",
       "                      1.0382, 1.0267, 1.1065, 0.9684, 1.0448, 1.0981, 1.0758, 1.0095, 1.1093,\n",
       "                      1.1057, 1.0406, 1.0138, 0.9900, 1.0370, 1.0622, 0.9825, 1.0560, 1.0355,\n",
       "                      1.0941, 0.9598, 1.0898, 1.1754, 1.0104, 0.9995, 1.0334, 1.1238, 1.1169,\n",
       "                      0.9982, 1.0341])),\n",
       "             ('blocks.2.message.Attention.actk.beta',\n",
       "              tensor([1.7066, 1.6930, 1.7314, 1.7317, 1.6877, 1.7196, 1.7142, 1.7055, 1.6513,\n",
       "                      1.6589, 1.7103, 1.7171, 1.7057, 1.6721, 1.7002, 1.7348, 1.7164, 1.7104,\n",
       "                      1.6713, 1.6866, 1.7544, 1.7101, 1.7115, 1.6954, 1.6858, 1.7680, 1.6780,\n",
       "                      1.7122, 1.6840, 1.7493, 1.7376, 1.7179, 1.7266, 1.6764, 1.6978, 1.7035,\n",
       "                      1.7486, 1.6645, 1.6338, 1.6722, 1.7435, 1.6847, 1.7351, 1.7010, 1.7130,\n",
       "                      1.7796, 1.7391, 1.6955, 1.7112, 1.7482, 1.7159, 1.7567, 1.7097, 1.7033,\n",
       "                      1.7332, 1.6800, 1.7335, 1.7267, 1.7007, 1.7065, 1.6990, 1.7055, 1.6753,\n",
       "                      1.6963, 1.7078, 1.7386, 1.6918, 1.7085, 1.6873, 1.7086, 1.7060, 1.6847,\n",
       "                      1.8350, 1.7513, 1.7134, 1.7156, 1.7936, 1.6870, 1.7209, 1.7336, 1.7229,\n",
       "                      1.6776, 1.7122, 1.6013, 1.6881, 1.7331, 1.7398, 1.7131, 1.7295, 1.6742,\n",
       "                      1.6649, 1.7138, 1.7017, 1.6939, 1.7219, 1.6875, 1.7052, 1.6718, 1.6860,\n",
       "                      1.6966, 1.7167, 1.6625, 1.7327, 1.7081, 1.7063, 1.7270, 1.7078, 1.7261,\n",
       "                      1.7376, 1.7002, 1.6908, 1.6930, 1.6989, 1.6785, 1.7274, 1.7400, 1.7329,\n",
       "                      1.7042, 1.7447, 1.6794, 1.7011, 1.7285, 1.7210, 1.6672, 1.6978, 1.7744,\n",
       "                      1.7064, 1.7101])),\n",
       "             ('blocks.2.message.Attention.actv.alpha',\n",
       "              tensor([1.0588, 1.0015, 0.9997, 1.0230, 0.9920, 0.9997, 0.9955, 0.9945, 1.0032,\n",
       "                      1.0020, 1.0090, 1.0050, 1.0064, 1.0010, 1.0281, 0.9870, 0.9900, 0.9831,\n",
       "                      1.0012, 1.0037, 1.0080, 1.0038, 1.0000, 1.0072, 0.9904, 0.9827, 1.0015,\n",
       "                      0.9950, 1.0561, 1.0552, 1.0015, 1.0022, 1.0164, 1.0029, 0.9909, 1.1000,\n",
       "                      1.0004, 0.9985, 0.9995, 1.0058, 0.9992, 1.0009, 1.0475, 0.9885, 1.0006,\n",
       "                      0.9800, 1.0038, 1.0140, 0.9996, 0.9950, 1.0028, 0.9842, 0.9916, 0.9894,\n",
       "                      1.0163, 1.0038, 0.9848, 0.9956, 1.0013, 1.0030, 1.0559, 1.0083, 0.9981,\n",
       "                      0.9783, 0.9918, 1.0036, 1.0026, 1.0261, 0.9829, 0.9986, 0.9913, 1.0266,\n",
       "                      1.0077, 0.9986, 1.0160, 1.0106, 1.0187, 1.0425, 1.0008, 1.0040, 1.0047,\n",
       "                      1.0137, 1.0013, 1.0155, 0.9944, 0.9925, 1.0030, 1.0036, 1.0020, 1.0413,\n",
       "                      0.9985, 0.9927, 1.0131, 0.9923, 1.0027, 0.9850, 1.0360, 1.0036, 1.0030,\n",
       "                      1.0431, 1.0139, 0.9837, 1.0076, 1.0109, 1.0005, 1.0001, 1.0633, 1.0112,\n",
       "                      1.0046, 1.0161, 1.0030, 1.0517, 1.0022, 0.9964, 1.0052, 1.0080, 1.0019,\n",
       "                      1.0069, 0.9994, 1.0195, 1.0008, 1.0006, 1.0031, 1.0028, 1.0064, 0.9958,\n",
       "                      1.0134, 0.9944, 1.0356, 1.0276, 1.0004, 1.0630, 1.0012, 1.0250, 1.0020,\n",
       "                      1.0033, 0.9970, 0.9887, 1.0150, 1.0126, 1.0004, 1.0717, 1.0019, 0.9968,\n",
       "                      0.9988, 1.0801, 1.0037, 1.0255, 1.0028, 1.0003, 1.0021, 0.9840, 0.9974,\n",
       "                      1.0025, 1.0029, 1.0062, 1.0251, 1.0017, 1.0239, 1.0121, 1.0466, 1.0030,\n",
       "                      0.9886, 1.0011, 1.0033, 1.0147, 1.0343, 1.0355, 1.0254, 1.0004, 1.0697,\n",
       "                      0.9900, 1.0004, 1.0042, 1.0036, 0.9921, 1.0044, 1.0065, 1.0174, 0.9993,\n",
       "                      0.9938, 0.9883, 0.9847, 1.0023, 0.9998, 1.0136, 0.9873, 0.9956, 1.0492,\n",
       "                      1.0003, 1.0070, 0.9985, 1.0542, 1.0015, 1.0021, 1.0217, 1.0051, 1.0564,\n",
       "                      1.0018, 1.0022, 1.0063, 1.0052, 1.0734, 1.0032, 0.9996, 1.0021, 1.0000,\n",
       "                      0.9949, 0.9880, 0.9957, 0.9781, 1.0023, 1.0440, 0.9992, 1.0032, 1.0049,\n",
       "                      1.0043, 1.0048, 1.0005, 1.0032, 1.0227, 1.0174, 1.0025, 0.9975, 1.0018,\n",
       "                      0.9887, 1.0008, 0.9835, 1.0328, 0.9905, 1.0071, 0.9993, 0.9865, 1.0040,\n",
       "                      1.0531, 1.0032, 1.0035, 0.9988, 0.9999, 1.0020, 1.0090, 1.0095, 0.9993,\n",
       "                      1.0257, 1.0050, 1.0058, 1.0015, 0.9988, 0.9960, 0.9971, 1.0013, 1.0047,\n",
       "                      1.0454, 1.0076, 0.9993, 0.9899])),\n",
       "             ('blocks.2.message.Attention.actv.beta',\n",
       "              tensor([1.7186, 1.7011, 1.7021, 1.7074, 1.6831, 1.7043, 1.6999, 1.7146, 1.6990,\n",
       "                      1.7054, 1.7167, 1.7067, 1.7067, 1.7019, 1.7020, 1.6947, 1.7140, 1.7088,\n",
       "                      1.7062, 1.6995, 1.6941, 1.7012, 1.6778, 1.6956, 1.6885, 1.6860, 1.7130,\n",
       "                      1.7101, 1.7250, 1.7190, 1.7071, 1.7058, 1.7465, 1.6990, 1.7107, 1.7050,\n",
       "                      1.7021, 1.6996, 1.7165, 1.6973, 1.7064, 1.7033, 1.7152, 1.6959, 1.7036,\n",
       "                      1.7154, 1.7143, 1.7092, 1.6985, 1.7016, 1.7044, 1.6955, 1.6966, 1.7002,\n",
       "                      1.7027, 1.7058, 1.6927, 1.7048, 1.7091, 1.7026, 1.7321, 1.6976, 1.6927,\n",
       "                      1.7306, 1.6905, 1.6926, 1.7071, 1.6974, 1.7211, 1.6999, 1.7011, 1.7527,\n",
       "                      1.6994, 1.7033, 1.6823, 1.6784, 1.7221, 1.7271, 1.7032, 1.7042, 1.6984,\n",
       "                      1.7019, 1.7046, 1.7189, 1.7010, 1.7047, 1.7007, 1.7056, 1.7053, 1.7271,\n",
       "                      1.7014, 1.7032, 1.7237, 1.6945, 1.7012, 1.6978, 1.7147, 1.6994, 1.7053,\n",
       "                      1.7296, 1.6967, 1.7173, 1.6995, 1.7106, 1.7014, 1.6993, 1.7258, 1.7126,\n",
       "                      1.7060, 1.7081, 1.7048, 1.7128, 1.6949, 1.6966, 1.7154, 1.7062, 1.7070,\n",
       "                      1.6973, 1.7023, 1.7069, 1.7044, 1.7033, 1.6986, 1.6979, 1.7421, 1.6962,\n",
       "                      1.7155, 1.6836, 1.7245, 1.7121, 1.7139, 1.6820, 1.6987, 1.7307, 1.7049,\n",
       "                      1.6968, 1.7074, 1.7061, 1.7225, 1.6937, 1.7035, 1.7006, 1.7038, 1.7110,\n",
       "                      1.7022, 1.7181, 1.7104, 1.7082, 1.7053, 1.7021, 1.7045, 1.7014, 1.7010,\n",
       "                      1.6989, 1.7051, 1.7151, 1.7222, 1.7037, 1.7010, 1.7023, 1.7402, 1.7052,\n",
       "                      1.7020, 1.7100, 1.7034, 1.7076, 1.7186, 1.7016, 1.7299, 1.7064, 1.7600,\n",
       "                      1.7122, 1.7044, 1.7064, 1.7296, 1.6958, 1.6980, 1.7191, 1.7084, 1.6711,\n",
       "                      1.6460, 1.7081, 1.6952, 1.7051, 1.7008, 1.6793, 1.7010, 1.7053, 1.7446,\n",
       "                      1.6999, 1.7071, 1.7024, 1.7536, 1.7041, 1.7051, 1.6984, 1.7019, 1.7056,\n",
       "                      1.7045, 1.7066, 1.6979, 1.6990, 1.7223, 1.7009, 1.7039, 1.7009, 1.6998,\n",
       "                      1.7076, 1.7039, 1.6928, 1.7025, 1.7028, 1.7070, 1.7037, 1.7051, 1.6985,\n",
       "                      1.7055, 1.6813, 1.7049, 1.7060, 1.7217, 1.6998, 1.7035, 1.7101, 1.6856,\n",
       "                      1.6923, 1.7046, 1.7205, 1.7274, 1.7091, 1.6978, 1.7053, 1.7080, 1.6998,\n",
       "                      1.7194, 1.7048, 1.7033, 1.6988, 1.6970, 1.7053, 1.7026, 1.7234, 1.6981,\n",
       "                      1.7074, 1.6980, 1.7081, 1.7019, 1.6956, 1.6991, 1.7021, 1.6986, 1.6990,\n",
       "                      1.7431, 1.6991, 1.6964, 1.7015])),\n",
       "             ('blocks.2.message.Attention.acta.alpha',\n",
       "              tensor([0.9938, 0.9833, 1.0701, 0.9718, 0.9850, 1.0147, 1.0092, 0.9876, 0.9504,\n",
       "                      0.9750, 0.9514, 1.0134, 1.0492, 1.0129, 0.9854, 0.9785, 1.0305, 0.9879,\n",
       "                      0.9914, 0.9788, 0.9409, 0.9177, 1.0273, 1.0031, 1.0240, 0.9830, 1.0262,\n",
       "                      0.9742, 0.9467, 0.9636, 0.9945, 1.0129, 0.9317, 0.9601, 0.9532, 1.0314,\n",
       "                      0.9662, 1.0061, 0.9665, 1.0129, 0.9776, 0.9874, 1.0120, 1.0692, 0.9513,\n",
       "                      0.9806, 1.0015, 0.9784, 1.0188, 1.0046, 1.0156, 1.0040, 0.9773, 0.9658,\n",
       "                      0.9663, 1.0009, 1.0125, 0.9795, 1.0192, 1.0263, 0.9963, 0.9898, 1.0032,\n",
       "                      0.9943, 1.0437, 0.9890, 0.9876, 1.0408, 1.0116, 0.9865, 0.9801, 1.0073,\n",
       "                      1.0678, 1.0101, 1.0241, 1.0253, 1.0183, 0.9467, 0.9462, 0.9982, 0.9684,\n",
       "                      0.9701, 0.9998, 0.9738, 0.9865, 1.0165, 0.9888, 0.9896, 0.9996, 0.9547,\n",
       "                      0.9530, 1.0428, 1.0062, 0.9776, 0.9889, 0.9496, 1.0088, 0.9295, 0.9962,\n",
       "                      1.0072, 1.0236, 0.9376, 1.0236, 0.9725, 0.9674, 0.9922, 0.9987, 0.9906,\n",
       "                      1.0113, 0.9694, 0.9885, 1.0466, 0.9814, 0.9852, 0.9701, 1.0200, 0.9917,\n",
       "                      1.0200, 1.0195, 0.9997, 1.0299, 0.9642, 1.0187, 0.9465, 0.9381, 0.9790,\n",
       "                      1.0686, 0.9465, 1.0060, 1.0012, 0.9963, 1.0053, 1.0013, 1.0024, 1.0057,\n",
       "                      0.9974, 1.0042, 1.0233, 0.9949, 0.9989, 1.0006, 0.9938, 1.0023, 1.0382,\n",
       "                      1.0051, 1.0066, 0.9945, 1.0057, 0.9966, 0.9999, 1.0045, 0.9991, 1.0071,\n",
       "                      1.0038, 1.0060, 1.0034, 1.0119, 1.0048, 0.9981, 0.9985, 1.0002, 1.0093,\n",
       "                      1.0018, 1.0011, 1.0032, 0.9979, 1.0017, 1.0003, 1.0037, 0.9985, 1.0037,\n",
       "                      1.0005, 1.0125, 1.0032, 1.0292, 1.0024, 1.0076, 1.0069, 1.0051, 1.0146,\n",
       "                      1.0260, 1.0283, 1.0013, 1.0017, 1.0084, 1.0028, 1.0023, 1.0050, 1.0122,\n",
       "                      0.9988, 1.0165, 1.0243, 1.0316, 1.0310, 0.9957, 1.0046, 0.9966, 1.0034,\n",
       "                      1.0043, 1.0016, 1.0021, 1.0033, 0.9998, 1.0614, 1.0006, 0.9961, 0.9955,\n",
       "                      1.0229, 1.0197, 1.0019, 0.9954, 1.0040, 1.0071, 1.0057, 1.0297, 1.0009,\n",
       "                      1.0282, 1.0015, 0.9980, 1.0068, 1.0039, 0.9991, 1.0022, 1.0030, 1.0112,\n",
       "                      0.9986, 1.0137, 0.9946, 1.0007, 1.0433, 1.0030, 1.0001, 0.9981, 1.0010,\n",
       "                      1.0324, 1.0039, 1.0042, 0.9869, 1.0184, 0.9860, 1.0025, 1.0007, 0.9992,\n",
       "                      1.0080, 0.9933, 1.0430, 0.9993, 1.0329, 1.0033, 1.0041, 1.0000, 0.9892,\n",
       "                      1.0065, 1.0035, 1.0148, 1.0047])),\n",
       "             ('blocks.2.message.Attention.acta.beta',\n",
       "              tensor([1.7020, 1.6903, 1.6764, 1.7140, 1.7122, 1.7045, 1.7125, 1.6558, 1.7191,\n",
       "                      1.7310, 1.6824, 1.6363, 1.6767, 1.6892, 1.7125, 1.7082, 1.6818, 1.7232,\n",
       "                      1.7200, 1.6796, 1.7281, 1.7325, 1.6722, 1.7611, 1.6210, 1.7292, 1.7103,\n",
       "                      1.7378, 1.7147, 1.7562, 1.6866, 1.7354, 1.6854, 1.6768, 1.6895, 1.6822,\n",
       "                      1.6928, 1.7187, 1.6798, 1.6908, 1.7041, 1.6223, 1.6785, 1.6485, 1.6941,\n",
       "                      1.7145, 1.7108, 1.6769, 1.7036, 1.6479, 1.6870, 1.6930, 1.7139, 1.7347,\n",
       "                      1.7082, 1.7144, 1.7073, 1.7569, 1.7565, 1.7402, 1.7152, 1.7726, 1.7301,\n",
       "                      1.7241, 1.6701, 1.7068, 1.6818, 1.7228, 1.6845, 1.6859, 1.6891, 1.7085,\n",
       "                      1.6603, 1.6396, 1.6844, 1.6614, 1.6949, 1.6352, 1.7247, 1.7320, 1.7072,\n",
       "                      1.6781, 1.7064, 1.7109, 1.7207, 1.7282, 1.6868, 1.6734, 1.7044, 1.7155,\n",
       "                      1.6353, 1.6778, 1.7334, 1.7419, 1.7140, 1.6528, 1.6598, 1.6900, 1.7175,\n",
       "                      1.7051, 1.6771, 1.7487, 1.7072, 1.6367, 1.6697, 1.6773, 1.7300, 1.7520,\n",
       "                      1.7113, 1.7157, 1.7025, 1.6463, 1.7410, 1.7185, 1.6823, 1.6689, 1.6945,\n",
       "                      1.7303, 1.6945, 1.7495, 1.7337, 1.7108, 1.6884, 1.6975, 1.6788, 1.7360,\n",
       "                      1.6522, 1.6813, 1.7208, 1.6991, 1.7085, 1.7434, 1.6905, 1.7210, 1.7047,\n",
       "                      1.6823, 1.7171, 1.6900, 1.6862, 1.7021, 1.6779, 1.7107, 1.6969, 1.6578,\n",
       "                      1.7121, 1.6862, 1.6918, 1.7017, 1.6899, 1.7002, 1.7002, 1.6913, 1.7001,\n",
       "                      1.6960, 1.7085, 1.6999, 1.7124, 1.7132, 1.7124, 1.6980, 1.6945, 1.7096,\n",
       "                      1.7045, 1.6903, 1.7080, 1.6982, 1.7129, 1.7013, 1.6981, 1.6993, 1.7156,\n",
       "                      1.6961, 1.7372, 1.6921, 1.6745, 1.7038, 1.7012, 1.7178, 1.7010, 1.6942,\n",
       "                      1.7340, 1.7101, 1.6871, 1.7085, 1.7107, 1.7200, 1.7121, 1.7071, 1.6646,\n",
       "                      1.6947, 1.7026, 1.6927, 1.7254, 1.6935, 1.7024, 1.6867, 1.7129, 1.6848,\n",
       "                      1.6987, 1.7009, 1.6991, 1.6813, 1.7011, 1.7206, 1.6891, 1.6968, 1.7062,\n",
       "                      1.7174, 1.6683, 1.6975, 1.6937, 1.6865, 1.6949, 1.6940, 1.7077, 1.6955,\n",
       "                      1.7394, 1.7110, 1.6948, 1.7060, 1.6936, 1.6980, 1.6975, 1.7020, 1.7169,\n",
       "                      1.6962, 1.6958, 1.7007, 1.6970, 1.7014, 1.6826, 1.7088, 1.7017, 1.6682,\n",
       "                      1.6724, 1.7021, 1.7073, 1.6719, 1.6932, 1.7035, 1.7007, 1.6945, 1.7105,\n",
       "                      1.7027, 1.6993, 1.7251, 1.7027, 1.7040, 1.6949, 1.6873, 1.6948, 1.6686,\n",
       "                      1.7079, 1.7063, 1.6888, 1.6896])),\n",
       "             ('blocks.2.message.Attention.lq.weight',\n",
       "              tensor([[-6.5274e-05,  9.6785e-02, -1.5364e-01,  ..., -4.6745e-02,\n",
       "                       -1.2700e-01, -1.0686e-01],\n",
       "                      [ 5.5362e-02,  1.7102e-02,  3.2651e-03,  ..., -1.8575e-01,\n",
       "                       -1.1458e-01, -1.7124e-02],\n",
       "                      [-3.2767e-03,  3.3815e-02,  5.0644e-02,  ...,  6.2877e-02,\n",
       "                        1.7397e-01, -1.3775e-01],\n",
       "                      ...,\n",
       "                      [ 6.7308e-02, -2.3915e-02,  1.7113e-01,  ...,  6.8509e-02,\n",
       "                       -6.2983e-02, -9.9990e-02],\n",
       "                      [ 1.4492e-01, -9.9613e-02,  5.4630e-02,  ..., -1.8309e-02,\n",
       "                        1.4696e-02,  1.0869e-02],\n",
       "                      [ 5.4154e-02, -7.4961e-02,  3.9839e-02,  ...,  5.1793e-02,\n",
       "                        1.3037e-01, -3.9264e-02]])),\n",
       "             ('blocks.2.message.Attention.lq.bias',\n",
       "              tensor([ 0.0034,  0.0456, -0.0145, -0.0286,  0.0386, -0.0162, -0.0064,  0.0556,\n",
       "                       0.0167,  0.0368,  0.0071, -0.0026,  0.0236, -0.0313,  0.0278, -0.0113,\n",
       "                       0.0170, -0.0510,  0.0099,  0.0256,  0.0100, -0.0077,  0.0641,  0.0232,\n",
       "                       0.0204,  0.0230,  0.0490,  0.0358,  0.0337,  0.0074,  0.0275,  0.0262,\n",
       "                       0.0156,  0.0531, -0.0088,  0.0439,  0.0435,  0.0650,  0.1055,  0.0061,\n",
       "                       0.0653, -0.0398, -0.0214,  0.0103,  0.0295,  0.0631,  0.0042, -0.0261,\n",
       "                       0.0053, -0.0526,  0.0105,  0.0195,  0.0138,  0.0594,  0.0785,  0.0718,\n",
       "                       0.0249,  0.0200,  0.0034,  0.0008,  0.0924,  0.0059,  0.0057,  0.0005,\n",
       "                       0.0389,  0.0758, -0.0146,  0.0155,  0.0031,  0.0844,  0.0662,  0.0200,\n",
       "                       0.0875,  0.0084, -0.0117, -0.0038, -0.0004,  0.0357,  0.0037,  0.0112,\n",
       "                       0.0188,  0.0678, -0.0012,  0.0075,  0.0150,  0.0478,  0.0309,  0.0129,\n",
       "                       0.0733,  0.0047, -0.0120,  0.0085,  0.0317, -0.0117,  0.0119,  0.0349,\n",
       "                       0.0232,  0.0504, -0.0083,  0.0853, -0.0272,  0.1003,  0.0481,  0.0279,\n",
       "                       0.0557,  0.0585, -0.0062, -0.0020,  0.0276,  0.0420,  0.0049, -0.0162,\n",
       "                      -0.0037,  0.0301,  0.0056,  0.0118,  0.0354,  0.0269,  0.0547,  0.0039,\n",
       "                       0.0505,  0.0839, -0.0202, -0.0069, -0.0041,  0.0151,  0.0017, -0.0198])),\n",
       "             ('blocks.2.message.Attention.lk.weight',\n",
       "              tensor([[ 0.0233,  0.1492,  0.0652,  ...,  0.1078,  0.1116,  0.0997],\n",
       "                      [-0.0599, -0.1003, -0.0886,  ...,  0.0181, -0.0595, -0.0271],\n",
       "                      [-0.0388, -0.1036,  0.1163,  ..., -0.0432,  0.1379,  0.1589],\n",
       "                      ...,\n",
       "                      [ 0.0658, -0.0805, -0.1035,  ..., -0.0594, -0.0012, -0.0061],\n",
       "                      [ 0.0329, -0.0133, -0.0947,  ...,  0.1044, -0.1233, -0.0153],\n",
       "                      [-0.0585, -0.1573,  0.1338,  ..., -0.0974,  0.0034,  0.0016]])),\n",
       "             ('blocks.2.message.Attention.lk.bias',\n",
       "              tensor([ 0.0094,  0.0368,  0.0346,  0.0332, -0.0262,  0.0909,  0.0461,  0.0079,\n",
       "                      -0.0006, -0.0140, -0.0022,  0.0351,  0.0750,  0.0280,  0.0184,  0.0363,\n",
       "                       0.0463, -0.0176, -0.0058, -0.0196,  0.0160,  0.0024,  0.1077,  0.0051,\n",
       "                       0.0247, -0.0013, -0.0161, -0.0213, -0.0060,  0.0396, -0.0088,  0.1041,\n",
       "                      -0.0043, -0.0195, -0.0029,  0.0024,  0.0776, -0.0049, -0.0317, -0.0175,\n",
       "                       0.0228,  0.0020,  0.0121,  0.0284, -0.0121,  0.0521,  0.0640, -0.0049,\n",
       "                       0.0803,  0.0191,  0.0156,  0.0714,  0.0393,  0.0009,  0.0553,  0.0237,\n",
       "                       0.0296,  0.0167,  0.0249,  0.0336,  0.0059,  0.0276,  0.0077, -0.0091,\n",
       "                       0.0029,  0.0658, -0.0092, -0.0018, -0.0265, -0.0326,  0.0043,  0.0643,\n",
       "                       0.0553,  0.1074, -0.0038,  0.0193,  0.0355,  0.0100,  0.0159,  0.0318,\n",
       "                       0.0177,  0.0674,  0.0259, -0.0129,  0.0027,  0.0127,  0.0489,  0.1526,\n",
       "                       0.0416,  0.0126,  0.0773,  0.0240,  0.0102,  0.0581,  0.0382, -0.0061,\n",
       "                       0.0072,  0.0382, -0.0043,  0.0225,  0.0124,  0.0531, -0.0205, -0.0002,\n",
       "                       0.0430,  0.0344,  0.0102,  0.0377,  0.1075,  0.0237, -0.0070, -0.0133,\n",
       "                       0.0025,  0.0483, -0.0393,  0.0391,  0.0104,  0.0897, -0.0217,  0.0313,\n",
       "                       0.0776,  0.0047, -0.0322,  0.0067,  0.0847,  0.0823, -0.0186,  0.0038])),\n",
       "             ('blocks.2.message.Attention.lv.weight',\n",
       "              tensor([[ 0.1647,  0.0062,  0.1486,  ..., -0.0189,  0.0846,  0.0006],\n",
       "                      [ 0.0540, -0.0233, -0.1336,  ...,  0.0381,  0.0474, -0.0740],\n",
       "                      [-0.0667, -0.0439,  0.0174,  ..., -0.0641, -0.0113,  0.0911],\n",
       "                      ...,\n",
       "                      [ 0.1036,  0.0932,  0.1158,  ...,  0.0544, -0.0343, -0.0164],\n",
       "                      [ 0.1072,  0.0158,  0.1176,  ...,  0.1317,  0.0792, -0.1188],\n",
       "                      [ 0.0732, -0.1254,  0.0827,  ..., -0.0752,  0.0496,  0.1047]])),\n",
       "             ('blocks.2.message.Attention.lv.bias',\n",
       "              tensor([ 3.2702e-02, -1.5024e-02,  8.2140e-04, -1.1826e-02, -1.2905e-02,\n",
       "                       2.7338e-03, -4.1889e-03,  4.7684e-04, -1.6810e-02,  4.0658e-03,\n",
       "                       4.2035e-03,  4.4828e-03,  7.7062e-03,  3.4882e-03,  3.4715e-03,\n",
       "                      -1.3005e-02,  8.1566e-03,  5.5388e-04, -1.9727e-03, -1.4890e-03,\n",
       "                       7.7145e-04, -1.4808e-02,  6.4540e-03,  5.5280e-03, -7.8647e-03,\n",
       "                      -1.9149e-02,  5.4127e-03,  3.6551e-03,  5.2495e-03,  2.6679e-02,\n",
       "                       4.8991e-03, -2.4086e-02,  1.3857e-02, -4.6798e-03, -5.9301e-03,\n",
       "                      -6.0744e-03,  3.2955e-03, -4.2810e-03,  4.1629e-03, -1.3468e-02,\n",
       "                       5.3487e-03,  2.4307e-03,  5.4798e-03, -1.1287e-02,  2.9843e-03,\n",
       "                      -6.5545e-03, -3.8666e-03,  9.4583e-03, -3.6759e-03, -7.9648e-05,\n",
       "                       2.1537e-03,  1.4478e-03, -1.1757e-02, -1.3606e-03,  3.0334e-03,\n",
       "                       4.6570e-03, -1.1560e-02,  2.5349e-03, -3.1079e-03, -2.9728e-03,\n",
       "                       3.3394e-02,  5.3474e-03, -1.7590e-04, -9.4246e-03, -2.4962e-03,\n",
       "                       1.0056e-02,  4.7497e-03, -1.4039e-03, -1.3988e-02, -4.4127e-03,\n",
       "                       2.4280e-03,  1.3907e-02, -7.6014e-03,  1.3449e-02, -1.2864e-02,\n",
       "                      -6.7965e-03,  6.4449e-03, -6.7478e-04,  2.6103e-03,  4.6487e-03,\n",
       "                      -1.4138e-02,  5.0445e-03, -1.0401e-03,  1.6407e-02, -6.2429e-03,\n",
       "                      -1.2099e-02,  3.8897e-04,  5.7091e-03,  4.0450e-03,  2.6658e-02,\n",
       "                       3.3420e-03,  5.1785e-03,  9.0919e-03, -7.6886e-03, -8.7972e-03,\n",
       "                       1.5024e-03,  3.1570e-02, -5.7371e-03,  4.0212e-03, -9.4567e-05,\n",
       "                      -8.3186e-03, -3.6583e-03, -4.0941e-03,  1.1166e-02, -7.5680e-04,\n",
       "                       1.4433e-03,  4.8265e-02,  9.5991e-03,  4.0871e-03,  6.4111e-03,\n",
       "                       3.3845e-03,  3.4949e-02, -2.2636e-02, -2.0770e-04,  1.3540e-03,\n",
       "                       6.6079e-03,  4.8367e-03, -2.6684e-03,  1.7694e-03, -2.6494e-04,\n",
       "                       2.4574e-03,  2.0163e-03, -1.1633e-02,  3.1936e-03, -8.1454e-03,\n",
       "                      -1.3030e-02,  2.0393e-03, -9.6075e-03,  2.4919e-03,  6.3832e-02,\n",
       "                       4.4419e-03,  8.4142e-03, -1.4957e-02,  4.0063e-02,  3.7824e-03,\n",
       "                      -1.3561e-02,  2.1457e-03,  2.7148e-03,  1.1330e-02,  6.2014e-03,\n",
       "                       1.9491e-03,  4.2882e-02,  3.0184e-03,  4.8898e-03,  1.5946e-03,\n",
       "                       6.2394e-02, -5.7919e-03,  7.9828e-03,  6.0639e-03,  1.3999e-03,\n",
       "                       2.6101e-03, -1.0025e-02, -1.3376e-02,  3.9090e-03,  3.3867e-03,\n",
       "                       6.5572e-03,  1.8046e-02,  2.7064e-03, -1.3311e-03,  8.4596e-03,\n",
       "                       2.5220e-02,  3.5759e-03, -1.0281e-02, -1.5193e-03,  6.4304e-03,\n",
       "                       3.1394e-04,  1.9139e-02,  2.3726e-02,  4.2227e-02,  3.4624e-03,\n",
       "                       7.4799e-03, -7.7302e-03,  3.0796e-03,  5.7939e-03, -6.8485e-05,\n",
       "                      -6.0135e-03, -1.1363e-02,  5.6568e-03,  5.5753e-03, -5.6525e-03,\n",
       "                      -7.1341e-03, -4.5849e-03, -1.2185e-02,  3.2344e-03,  1.3287e-03,\n",
       "                      -5.2233e-03,  3.8326e-03, -3.7590e-03,  5.0635e-02,  2.8784e-03,\n",
       "                       5.7721e-03,  2.3628e-03,  4.6949e-02,  3.4543e-03,  3.9158e-03,\n",
       "                       3.1313e-03, -6.7610e-05,  4.4785e-02,  3.7871e-03,  5.8377e-03,\n",
       "                      -6.5818e-03, -1.0358e-02,  4.7864e-02,  3.3768e-03, -1.2066e-02,\n",
       "                       1.9775e-03, -6.6459e-03, -4.8494e-03,  2.9710e-03, -4.8421e-03,\n",
       "                       6.2530e-03,  3.2403e-03,  4.6028e-02,  2.9510e-03,  3.7223e-03,\n",
       "                      -1.5298e-02,  4.9783e-03, -2.6362e-03,  1.3066e-03,  3.5467e-03,\n",
       "                       1.5790e-02, -9.6414e-03,  7.8326e-04, -1.0661e-02, -1.6221e-02,\n",
       "                      -7.9639e-03,  3.3546e-03, -9.4180e-03, -4.4125e-03, -2.8108e-03,\n",
       "                       4.0706e-04, -1.0329e-03,  9.1244e-03, -1.2567e-02,  6.2560e-04,\n",
       "                       5.2160e-03, -3.4890e-03, -9.2201e-05, -1.1126e-02,  3.5776e-03,\n",
       "                       1.7258e-02,  7.9070e-03, -8.5459e-04,  1.3940e-03, -4.9073e-03,\n",
       "                       4.9314e-03, -7.3877e-03, -4.7879e-03,  1.1626e-03,  1.6066e-03,\n",
       "                      -1.1844e-02,  2.9415e-06,  9.1087e-03, -7.9853e-04, -3.8553e-03,\n",
       "                      -5.5245e-03])),\n",
       "             ('blocks.2.message.Attention.la.weight',\n",
       "              tensor([[-0.1418, -0.0320,  0.0703,  ...,  0.0741, -0.0250, -0.0135],\n",
       "                      [ 0.0486,  0.0124, -0.0902,  ...,  0.1044, -0.0211, -0.0025],\n",
       "                      [ 0.1255, -0.0733, -0.0735,  ...,  0.0856, -0.0460,  0.0153],\n",
       "                      ...,\n",
       "                      [ 0.1149, -0.0191,  0.0401,  ...,  0.0042,  0.0654,  0.0149],\n",
       "                      [-0.0722,  0.0317, -0.0684,  ..., -0.0866, -0.0128, -0.0065],\n",
       "                      [ 0.0029, -0.0355,  0.0778,  ...,  0.0308,  0.0475,  0.0333]])),\n",
       "             ('blocks.2.message.Attention.la.bias',\n",
       "              tensor([ 0.0170, -0.0016, -0.0835,  0.0366, -0.0183,  0.0005, -0.0038, -0.0100,\n",
       "                       0.0065, -0.0250,  0.0049,  0.0460, -0.0374, -0.0108,  0.0318, -0.0037,\n",
       "                      -0.0251, -0.0036, -0.0261, -0.0170, -0.0146, -0.0248, -0.0042, -0.0020,\n",
       "                      -0.0462,  0.0160,  0.0149, -0.0149, -0.0159, -0.0152, -0.0058, -0.0172,\n",
       "                      -0.0266,  0.0185, -0.0409,  0.0218,  0.0063, -0.0013, -0.0404,  0.0322,\n",
       "                      -0.0064, -0.0304, -0.0313,  0.0179, -0.0023, -0.0106, -0.0092, -0.0101,\n",
       "                      -0.0108, -0.0233,  0.0031,  0.0157, -0.0181,  0.0272, -0.0151, -0.0181,\n",
       "                       0.0202, -0.0169,  0.0262,  0.0036,  0.0052, -0.0170, -0.0277,  0.0343,\n",
       "                      -0.0113, -0.0111,  0.0033, -0.0081, -0.0098, -0.0156, -0.0289, -0.0316,\n",
       "                       0.0168, -0.0462, -0.0285, -0.0249,  0.0197, -0.0455,  0.0391, -0.0023,\n",
       "                      -0.0118, -0.0281, -0.0203,  0.0256,  0.0093,  0.0099,  0.0222, -0.0231,\n",
       "                      -0.0671, -0.0537, -0.0446, -0.0071,  0.0187, -0.0297, -0.0050,  0.0222,\n",
       "                      -0.0376,  0.0152, -0.0239,  0.0291, -0.0068, -0.0212,  0.0235, -0.0138,\n",
       "                      -0.0041,  0.0045, -0.0678, -0.0052,  0.0395, -0.0333,  0.0188, -0.0229,\n",
       "                      -0.0108,  0.0045,  0.0115, -0.0566,  0.0534, -0.0023, -0.0371, -0.0066,\n",
       "                       0.0736, -0.0170,  0.0435, -0.0235, -0.0115,  0.0398,  0.0007, -0.0046,\n",
       "                       0.0030, -0.0046,  0.0050,  0.0037, -0.0049,  0.0067,  0.0062,  0.0050,\n",
       "                      -0.0064, -0.0070,  0.0032, -0.0042, -0.0058, -0.0074,  0.0062, -0.0090,\n",
       "                       0.0048,  0.0049,  0.0050, -0.0067,  0.0059,  0.0078, -0.0048, -0.0065,\n",
       "                      -0.0058, -0.0064, -0.0038,  0.0069, -0.0074, -0.0070,  0.0070,  0.0059,\n",
       "                      -0.0052, -0.0059, -0.0013, -0.0005, -0.0084, -0.0089, -0.0068,  0.0076,\n",
       "                       0.0010, -0.0058, -0.0050,  0.0057, -0.0058, -0.0068, -0.0114,  0.0027,\n",
       "                       0.0059, -0.0074,  0.0073,  0.0058, -0.0085, -0.0210,  0.0056, -0.0043,\n",
       "                       0.0017,  0.0098, -0.0046,  0.0010, -0.0079,  0.0066, -0.0001, -0.0117,\n",
       "                      -0.0014,  0.0044, -0.0009,  0.0076,  0.0018, -0.0058,  0.0019, -0.0053,\n",
       "                       0.0068,  0.0065,  0.0053,  0.0018, -0.0081,  0.0060, -0.0020,  0.0112,\n",
       "                       0.0024,  0.0052, -0.0088,  0.0031, -0.0078,  0.0064, -0.0139,  0.0075,\n",
       "                       0.0015, -0.0066, -0.0052, -0.0048,  0.0071, -0.0047, -0.0073,  0.0073,\n",
       "                       0.0072,  0.0044, -0.0143,  0.0033,  0.0052, -0.0061,  0.0055,  0.0061,\n",
       "                       0.0065, -0.0034, -0.0020, -0.0064, -0.0066, -0.0018, -0.0078,  0.0053,\n",
       "                       0.0058,  0.0062,  0.0072, -0.0100, -0.0036,  0.0018, -0.0056, -0.0050,\n",
       "                      -0.0057, -0.0062, -0.0067, -0.0045, -0.0057, -0.0062, -0.0083,  0.0056])),\n",
       "             ('blocks.2.message.Attention.lrbf.weight',\n",
       "              tensor([[ 0.1567, -0.0548,  0.0109,  ...,  0.1303, -0.0162,  0.1539],\n",
       "                      [-0.0657,  0.1256,  0.0116,  ..., -0.0017, -0.0301,  0.0699],\n",
       "                      [-0.0287,  0.0889,  0.0414,  ..., -0.0060, -0.1409, -0.0804],\n",
       "                      ...,\n",
       "                      [ 0.0730, -0.0202,  0.1412,  ...,  0.0236, -0.1994, -0.1108],\n",
       "                      [-0.1776,  0.1215,  0.1335,  ..., -0.0460, -0.1219,  0.1649],\n",
       "                      [ 0.0256, -0.0787,  0.1122,  ..., -0.0253, -0.0439,  0.1303]])),\n",
       "             ('blocks.2.message.Attention.lkrbf.weight',\n",
       "              tensor([[-0.0845,  0.0167,  0.1459,  ..., -0.1387,  0.1274,  0.1430],\n",
       "                      [-0.1846, -0.1131,  0.1002,  ...,  0.0072, -0.1923, -0.0707],\n",
       "                      [-0.1042, -0.1530,  0.1052,  ...,  0.0851,  0.1109,  0.0137],\n",
       "                      ...,\n",
       "                      [-0.0138,  0.0511, -0.1575,  ...,  0.0885, -0.0450,  0.0740],\n",
       "                      [-0.0554,  0.0052, -0.0308,  ...,  0.1587,  0.0715, -0.1521],\n",
       "                      [ 0.1490, -0.1124, -0.0648,  ...,  0.1017, -0.0399,  0.1142]])),\n",
       "             ('blocks.2.message.Attention.lvrbf.weight',\n",
       "              tensor([[ 0.0200, -0.1545,  0.0755,  ..., -0.0265, -0.0263,  0.0013],\n",
       "                      [ 0.0207, -0.0464,  0.1146,  ..., -0.0427, -0.0561,  0.1085],\n",
       "                      [-0.0988,  0.0170,  0.0872,  ..., -0.0701, -0.0110, -0.0237],\n",
       "                      ...,\n",
       "                      [ 0.0773, -0.0631,  0.0662,  ...,  0.1177,  0.0330,  0.0685],\n",
       "                      [-0.1083, -0.0016,  0.0494,  ...,  0.0086,  0.1219,  0.0846],\n",
       "                      [ 0.0131, -0.0291, -0.0066,  ...,  0.0415,  0.0246, -0.0408]])),\n",
       "             ('blocks.2.message.tp.weight',\n",
       "              tensor([-0.2385,  0.0066,  0.9461,  0.3474,  0.5085, -0.2504,  2.0331,  1.2352,\n",
       "                       1.1022, -0.6642,  0.4340,  1.3958, -0.9267, -0.1594, -1.3259,  0.6822,\n",
       "                       0.2438,  0.1147,  0.4582, -0.1637,  1.1819,  0.2840,  0.9897, -1.4204,\n",
       "                      -0.8278, -0.7332,  1.0106, -0.7202, -0.3728,  0.7961, -0.1758, -2.2010,\n",
       "                      -1.1299, -0.6030,  2.0499, -1.0044, -0.2308,  0.5767, -1.0474,  1.5041,\n",
       "                      -1.3427, -0.4012,  0.6286,  1.8001, -1.9955,  1.3838,  0.1509, -0.5881,\n",
       "                       0.5391,  0.0197,  0.5908, -0.7599,  1.1941, -0.2149, -0.6214, -0.1696,\n",
       "                      -1.5836,  1.3628,  1.6664,  1.7206,  0.2296, -1.0827,  1.9210,  0.6413,\n",
       "                       0.7553, -0.3744, -0.1701, -2.1871,  0.9272, -0.6321, -0.0120,  0.0567,\n",
       "                      -0.7575, -0.7696, -0.8934,  0.5098,  1.0881, -0.4006,  0.7657,  0.6544,\n",
       "                       0.6211, -0.3735,  0.3767, -1.5082,  0.8477,  0.5392, -0.9826,  0.5206,\n",
       "                       0.7489,  0.7616,  0.9023, -0.5369, -0.1791,  0.6017,  0.4178,  0.1268,\n",
       "                       1.6273,  1.0718,  0.5847, -1.2694,  1.9799,  1.1541, -0.2908, -1.1740,\n",
       "                       1.1354, -0.0070, -0.0388, -1.6781,  0.0489,  0.2305,  1.3460, -0.9673,\n",
       "                       0.1255, -0.8207, -0.1526,  1.5702,  0.9132, -1.1686, -2.3729, -2.2054,\n",
       "                      -1.2617, -0.9959,  0.8965, -0.3463,  0.9349,  0.0350, -0.2945, -1.1442,\n",
       "                      -1.3915,  1.4296, -0.9694,  0.4876, -0.4464,  0.0964, -0.3171,  0.3794,\n",
       "                       1.1763, -1.0836,  1.4139, -0.0287, -0.5090, -1.1080,  2.0633, -0.0837,\n",
       "                      -1.4080,  0.0193, -1.2012, -0.0075,  0.8698, -0.4431, -0.7390,  1.1014,\n",
       "                       0.5968,  0.0116,  1.3607,  0.2475,  0.2296, -1.1404, -1.3514, -1.3713,\n",
       "                       0.7516,  1.6206, -0.8589,  2.0267, -0.8503,  1.8529,  0.2794, -0.7735,\n",
       "                       0.6663,  2.2796, -0.1707, -0.4812, -0.4714, -0.1222,  0.0899,  0.4201,\n",
       "                       0.7946,  0.6922, -0.6942, -0.6138, -0.2431,  0.2225,  0.1502, -0.7560,\n",
       "                       0.6924,  0.7713,  0.5059, -0.0653,  1.1364, -0.5571, -0.5160,  0.2649,\n",
       "                      -0.0973, -0.6226,  1.1500,  0.4145, -0.8393,  0.9702,  0.0343,  0.7194,\n",
       "                       0.5857, -0.9970,  2.7926,  1.3272,  0.2876, -1.1865, -0.8269,  0.4665,\n",
       "                      -0.4110,  0.5287,  0.6788, -0.4427,  2.1313,  1.5753, -0.0978,  0.6674,\n",
       "                       1.0666, -1.3939,  1.2552, -0.7331, -1.4244, -0.5654,  0.7422, -0.9708,\n",
       "                       0.2654,  0.0242,  0.8117, -0.3320, -0.0893, -0.4073, -1.6205, -0.6378,\n",
       "                       0.3836, -1.2615,  0.3414, -0.9498, -0.4848,  0.3368, -0.7375, -0.1957,\n",
       "                      -0.2747, -0.3759,  0.0271,  0.6419,  0.4142, -0.0415, -0.2722,  0.3595,\n",
       "                      -1.6316,  1.5715,  0.7686,  2.1405,  0.6100, -0.6576,  1.7840, -0.1416,\n",
       "                       0.2229,  2.1670, -1.0086,  0.9964, -0.1044, -0.7248, -0.8166, -1.5147,\n",
       "                      -0.6109, -1.1255,  0.0328, -0.3825, -1.1533,  2.6145,  0.1343,  0.1688,\n",
       "                       0.7128, -0.2934,  0.0727,  1.4151, -2.0583, -0.2942, -1.7051,  0.6307,\n",
       "                       0.5865, -1.3211, -0.3844, -0.0127,  0.1380, -0.4335, -1.1275,  0.8914,\n",
       "                       0.5401, -1.4838,  0.1107,  1.3498, -1.1958,  0.8789, -0.7576, -1.3347,\n",
       "                       0.7753,  0.2716, -2.0099, -1.8175,  2.0755,  1.1341, -0.3370,  0.7821,\n",
       "                      -1.4301, -0.4108,  0.1348,  0.8234, -0.0677, -0.0504, -0.9166, -1.4019,\n",
       "                       0.1282, -0.7129, -0.1551,  0.7852,  0.7602, -0.1964,  0.3456,  1.2565,\n",
       "                      -0.0148, -0.4694,  0.3281, -0.1406,  1.1266, -1.7780, -0.1642, -0.6236,\n",
       "                      -0.8285,  0.1699, -0.1246,  0.1774, -0.6907,  0.8226, -0.5928,  1.2126,\n",
       "                      -0.1286,  0.3691, -1.8221,  0.5132,  0.6978, -1.1004,  0.7638, -1.1091,\n",
       "                       0.0717, -0.7089, -0.8083, -0.4096,  1.8111, -1.8872,  1.3167, -0.6693,\n",
       "                      -0.6209,  1.0057, -0.8436,  0.2805,  0.6938, -0.6261,  0.3245, -0.2090,\n",
       "                      -0.7289, -1.1654,  0.4165,  0.6965, -1.0231, -0.0157,  0.9013, -0.0886,\n",
       "                      -0.1432, -0.1347,  0.9583,  0.3145, -0.3919, -0.5750, -0.9457, -1.6492,\n",
       "                      -0.4056, -1.2976,  0.1172,  1.3296, -0.6842,  2.0937, -1.4781,  0.4143])),\n",
       "             ('blocks.2.message.tp.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.actu.alpha',\n",
       "              tensor([1.0155, 0.9811, 1.0009, 1.0036, 1.0042, 0.9798, 0.9989, 1.0295, 1.0066,\n",
       "                      1.0080, 1.0005, 1.0018, 1.0120, 1.0001, 1.0332, 1.0355, 1.0013, 1.0172,\n",
       "                      1.0019, 1.0062, 1.0013, 1.0040, 1.0075, 1.0045, 1.0047, 1.0083, 1.0047,\n",
       "                      1.0031, 1.0037, 1.0028, 0.9919, 1.0091, 0.9957, 0.9940, 1.0004, 1.0033,\n",
       "                      1.0036, 0.9719, 1.0004, 1.0108, 0.9987, 0.9901, 1.0283, 1.0017, 0.9956,\n",
       "                      0.9965, 0.9997, 1.0071, 1.0092, 0.9964, 1.0051, 1.0038, 1.0001, 1.0007,\n",
       "                      0.9848, 1.0113, 0.9809, 1.0073, 1.0064, 1.0066, 0.9998, 1.0109, 1.0136,\n",
       "                      0.9989, 0.9968, 1.0090, 1.0105, 1.0021, 1.0070, 0.9936, 0.9968, 0.9901,\n",
       "                      1.0331, 1.0287, 1.0305, 0.9971, 1.0055, 1.0071, 1.0014, 1.0037, 0.9869,\n",
       "                      1.0042, 1.0180, 0.9966, 1.0039, 0.9960, 1.0026, 0.9937, 1.0050, 1.0014,\n",
       "                      1.0078, 1.0027, 1.0709, 0.9958, 1.0090, 1.0054, 0.9961, 0.9969, 0.9924,\n",
       "                      1.0060, 1.0037, 0.9953, 0.9979, 0.9822, 0.9962, 1.0098, 1.0033, 1.0013,\n",
       "                      0.9869, 0.9889, 1.0027, 1.0309, 0.9780, 1.0148, 0.9999, 1.0018, 1.0000,\n",
       "                      1.0036, 1.0021, 0.9994, 1.0013, 1.0251, 0.9894, 1.0087, 0.9981, 1.0281,\n",
       "                      0.9979, 1.0100])),\n",
       "             ('blocks.2.update.actu.beta',\n",
       "              tensor([1.7493, 1.7123, 1.7031, 1.7050, 1.7052, 1.6672, 1.6999, 1.6939, 1.7078,\n",
       "                      1.7171, 1.7004, 1.7032, 1.7010, 1.7004, 1.7489, 1.7110, 1.7020, 1.7060,\n",
       "                      1.7027, 1.6872, 1.7011, 1.7105, 1.7196, 1.6981, 1.7023, 1.7044, 1.7065,\n",
       "                      1.7079, 1.7046, 1.7107, 1.6833, 1.7032, 1.7215, 1.6958, 1.7015, 1.7045,\n",
       "                      1.7037, 1.6807, 1.7018, 1.7175, 1.7129, 1.6892, 1.7970, 1.6976, 1.7103,\n",
       "                      1.6955, 1.7031, 1.7126, 1.7316, 1.7020, 1.7010, 1.7049, 1.7065, 1.7013,\n",
       "                      1.6785, 1.7036, 1.6882, 1.7021, 1.6978, 1.7006, 1.6945, 1.6728, 1.7046,\n",
       "                      1.7025, 1.7018, 1.7128, 1.7355, 1.7054, 1.6989, 1.6975, 1.7011, 1.7012,\n",
       "                      1.7781, 1.7079, 1.7502, 1.7044, 1.7020, 1.7014, 1.7015, 1.6899, 1.6987,\n",
       "                      1.7009, 1.7084, 1.7108, 1.7078, 1.7507, 1.7048, 1.7075, 1.6953, 1.7020,\n",
       "                      1.7291, 1.7016, 1.7669, 1.6970, 1.6922, 1.7034, 1.6909, 1.7195, 1.6972,\n",
       "                      1.7066, 1.7019, 1.7077, 1.6963, 1.7264, 1.7038, 1.6956, 1.7166, 1.7036,\n",
       "                      1.6693, 1.6931, 1.6852, 1.7134, 1.6752, 1.6802, 1.7015, 1.7024, 1.7189,\n",
       "                      1.6978, 1.7022, 1.7017, 1.6996, 1.6958, 1.6762, 1.7044, 1.7106, 1.6889,\n",
       "                      1.7036, 1.7167])),\n",
       "             ('blocks.2.update.outt.weight',\n",
       "              tensor([-0.1383, -0.5822, -0.9873,  ...,  0.7781, -0.6095, -0.3080])),\n",
       "             ('blocks.2.update.outt.bias', tensor([])),\n",
       "             ('blocks.2.update.outt.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.outs.weight',\n",
       "              tensor([[ 0.0788,  0.0812,  0.0464,  ...,  0.0398, -0.0370,  0.1248],\n",
       "                      [-0.0680, -0.0852,  0.0430,  ...,  0.0847,  0.1016, -0.0517],\n",
       "                      [-0.1624,  0.0134,  0.0746,  ...,  0.1091, -0.0848, -0.0622],\n",
       "                      ...,\n",
       "                      [-0.1103,  0.1197,  0.0236,  ...,  0.0331, -0.0407,  0.0663],\n",
       "                      [ 0.0752, -0.1103,  0.0314,  ..., -0.0554, -0.1417, -0.1000],\n",
       "                      [ 0.1481, -0.0941, -0.1223,  ...,  0.0941, -0.0027, -0.0546]])),\n",
       "             ('blocks.2.update.outs.bias',\n",
       "              tensor([ 1.0061e-02,  7.9850e-03,  1.1059e-02,  7.7864e-03,  8.0961e-03,\n",
       "                      -6.6842e-03,  8.0334e-03, -7.9470e-03, -1.9426e-03, -1.2988e-02,\n",
       "                       6.9111e-03, -1.0485e-02, -1.0898e-02, -1.1231e-02,  4.7255e-03,\n",
       "                      -8.1639e-03,  5.9337e-03,  1.0199e-02,  9.8011e-03, -1.6383e-03,\n",
       "                       8.9304e-03,  8.7489e-03, -1.0760e-02, -1.0076e-02,  8.1693e-03,\n",
       "                      -9.7309e-03,  1.2304e-02, -8.8985e-03,  8.2374e-03,  8.1980e-03,\n",
       "                      -8.8395e-03, -9.4935e-03, -6.2250e-03, -1.1560e-02,  7.6472e-03,\n",
       "                       8.3937e-03, -9.7970e-03,  9.0206e-03,  6.1816e-03, -9.0300e-03,\n",
       "                      -8.4580e-03,  2.2409e-03, -1.0444e-02,  6.5432e-05, -1.0114e-02,\n",
       "                      -5.3501e-03,  9.1191e-03, -7.8813e-03, -1.0714e-02, -1.0024e-02,\n",
       "                      -8.4320e-03,  9.1822e-03, -8.2989e-03,  7.4833e-03, -5.8361e-03,\n",
       "                      -1.0994e-02,  8.7071e-03, -8.7233e-03, -7.5855e-03, -1.0464e-02,\n",
       "                      -1.8423e-03, -8.3772e-03, -9.0436e-03,  7.7695e-03,  8.8384e-03,\n",
       "                      -7.3521e-03,  8.1325e-03, -1.0136e-02,  1.0406e-02,  7.7639e-03,\n",
       "                       7.1453e-03,  1.0948e-02, -7.3432e-03,  6.4198e-03,  1.0010e-02,\n",
       "                      -1.0817e-02, -9.9525e-03, -8.1425e-03,  7.4543e-03,  8.3572e-03,\n",
       "                      -4.8176e-03, -9.6802e-03, -4.1397e-03,  8.8671e-03,  8.4513e-03,\n",
       "                      -2.6807e-03, -1.0960e-02,  7.8885e-03,  3.3928e-03,  8.3828e-03,\n",
       "                       8.7679e-03,  8.3057e-03,  9.2339e-03, -7.9014e-03, -5.4301e-03,\n",
       "                      -1.0057e-02, -7.3656e-03,  1.0572e-02, -9.1558e-03,  9.7296e-03,\n",
       "                      -9.1537e-03,  1.0131e-02, -1.0527e-02, -6.1005e-04,  8.1580e-03,\n",
       "                      -8.6646e-03, -8.8418e-03, -9.7477e-03, -9.1925e-03, -8.2323e-03,\n",
       "                       7.0080e-03, -9.2271e-03,  9.6821e-03, -1.7803e-03,  7.3909e-03,\n",
       "                      -1.1283e-02,  8.1716e-03, -7.4324e-03,  9.0665e-03,  7.6450e-03,\n",
       "                      -9.8120e-03, -5.4291e-03, -3.4002e-03, -8.9795e-03,  9.1509e-03,\n",
       "                      -1.0676e-02, -8.4899e-03,  1.6361e-02])),\n",
       "             ('blocks.2.update.uattn.lq.weight',\n",
       "              tensor([ 0.5011, -0.7197,  0.0858,  ...,  0.4255,  0.9786, -0.4423])),\n",
       "             ('blocks.2.update.uattn.lq.bias', tensor([])),\n",
       "             ('blocks.2.update.uattn.lq.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.uattn.lk.weight',\n",
       "              tensor([ 1.5820, -1.5092, -0.1678,  ...,  0.8452,  0.1398, -1.0091])),\n",
       "             ('blocks.2.update.uattn.lk.bias', tensor([])),\n",
       "             ('blocks.2.update.uattn.lk.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.uattn.lv.weight',\n",
       "              tensor([ 0.2974,  0.2875, -0.5312,  ...,  0.5640, -0.8566, -0.2966])),\n",
       "             ('blocks.2.update.uattn.lv.bias', tensor([])),\n",
       "             ('blocks.2.update.uattn.lv.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.uattn.ls.weight',\n",
       "              tensor([[ 1.6302e-01, -4.8573e-02,  5.2519e-03,  ..., -1.6299e-04,\n",
       "                       -9.5521e-02, -1.5027e-01],\n",
       "                      [-4.2075e-02, -7.2521e-02,  2.9026e-02,  ...,  3.1234e-02,\n",
       "                        8.6451e-03,  1.2122e-01],\n",
       "                      [-7.0458e-02,  1.3621e-01,  1.2402e-02,  ...,  1.5024e-01,\n",
       "                        1.1478e-01, -4.6782e-02],\n",
       "                      ...,\n",
       "                      [ 1.5164e-01,  1.1332e-01,  1.2755e-02,  ...,  3.8387e-02,\n",
       "                        1.2399e-02, -3.8639e-02],\n",
       "                      [ 1.0347e-02, -2.7543e-02,  7.3569e-02,  ...,  7.3278e-03,\n",
       "                       -1.0370e-01,  1.0119e-01],\n",
       "                      [ 1.2475e-01,  1.6715e-02,  6.5263e-02,  ...,  4.7533e-02,\n",
       "                        8.4678e-02,  3.0013e-02]])),\n",
       "             ('blocks.2.update.uattn.ls.bias',\n",
       "              tensor([-4.3777e-03, -8.0895e-03, -3.1794e-03,  3.7588e-03, -4.6086e-04,\n",
       "                       9.9628e-03,  2.0588e-03,  5.0917e-03,  3.5940e-03,  2.4649e-02,\n",
       "                       3.5995e-02,  4.5508e-03, -1.4268e-02,  1.1282e-02,  2.6485e-02,\n",
       "                       1.5981e-02,  3.9484e-04,  9.1270e-03,  9.1726e-03,  1.5012e-02,\n",
       "                       3.9613e-03,  7.9906e-03, -1.4033e-02,  9.7257e-03,  7.4103e-03,\n",
       "                       9.7077e-03, -1.5338e-02,  4.5218e-03, -5.6172e-03,  3.4691e-03,\n",
       "                      -8.2606e-03, -1.1331e-02, -8.4046e-03, -1.0609e-02, -1.1668e-02,\n",
       "                       4.6699e-03, -1.0567e-02, -1.4366e-03, -9.6061e-03, -8.9287e-03,\n",
       "                      -9.6560e-03, -1.3178e-02, -9.0694e-03,  3.1794e-03, -1.0669e-02,\n",
       "                      -7.3616e-03,  2.3269e-02, -1.7138e-02, -5.0251e-03, -5.5660e-03,\n",
       "                      -6.8127e-03,  6.6352e-03, -1.1665e-02, -9.9541e-03, -1.1773e-02,\n",
       "                      -7.6376e-03,  2.8698e-02, -9.8741e-03, -1.6068e-02,  1.1011e-02,\n",
       "                      -2.5915e-03, -1.6218e-02, -1.2482e-02, -1.3181e-03, -1.1346e-03,\n",
       "                      -1.4964e-03,  1.8069e-03,  9.6606e-03, -1.3825e-02, -7.2661e-03,\n",
       "                      -1.9278e-02,  1.1758e-02, -1.0073e-02, -6.0138e-03, -1.0865e-02,\n",
       "                       1.4850e-02,  6.9575e-03,  1.9668e-02, -9.3918e-03, -1.3456e-02,\n",
       "                      -1.7726e-02,  7.1936e-03, -2.3740e-03, -9.3546e-03, -1.4133e-02,\n",
       "                      -3.7668e-02,  1.1722e-02,  1.2488e-02, -4.8205e-03, -7.0807e-03,\n",
       "                      -9.8933e-03,  6.1274e-03, -7.4672e-03,  8.5403e-03, -9.3934e-03,\n",
       "                      -8.7842e-03, -1.2596e-02, -1.5582e-02,  5.4581e-03,  8.8657e-03,\n",
       "                       5.8216e-05,  6.6829e-03, -1.0259e-02, -2.4607e-03, -1.0714e-02,\n",
       "                      -1.7731e-02, -1.0387e-02, -3.0704e-03, -2.2521e-02,  1.4980e-03,\n",
       "                       2.5106e-02,  1.7048e-02,  1.9510e-03, -1.0910e-02,  5.4372e-03,\n",
       "                       3.1994e-03,  3.1516e-03, -1.2180e-02,  1.8129e-02,  4.0188e-03,\n",
       "                      -4.3291e-04, -9.2840e-03,  1.9574e-02, -1.4184e-02,  2.1629e-02,\n",
       "                       8.2765e-03, -9.8083e-03,  2.5349e-02, -5.5476e-03, -1.6547e-02,\n",
       "                      -1.9004e-02, -7.5552e-03, -1.8344e-02, -8.0387e-03, -7.1030e-03,\n",
       "                       6.0310e-04, -7.9786e-03, -1.0615e-02, -6.3473e-03, -1.3053e-02,\n",
       "                      -7.8336e-03,  2.9588e-03, -1.4430e-02, -1.4298e-02, -7.2499e-03,\n",
       "                      -4.7025e-03, -1.8071e-02, -7.3995e-03, -6.5130e-03, -9.0575e-03,\n",
       "                      -1.1005e-02, -1.4981e-02, -1.5011e-02, -2.1139e-02, -7.7923e-03,\n",
       "                      -2.1669e-02, -1.1529e-02, -9.9272e-03, -8.5742e-03, -1.5131e-02,\n",
       "                      -5.4211e-03, -3.8899e-03, -7.6358e-03, -9.1397e-03, -3.5809e-03,\n",
       "                      -1.1059e-02, -8.2965e-03, -6.6904e-03, -7.3276e-03, -6.7807e-03,\n",
       "                      -1.6065e-02, -1.1879e-02, -7.2858e-03, -8.3382e-03, -1.6856e-02,\n",
       "                      -1.2708e-02, -1.0615e-02, -1.1621e-02, -1.2026e-02, -1.2918e-02,\n",
       "                      -1.6074e-02, -1.7439e-02, -1.6105e-02, -1.4312e-02, -9.6872e-03,\n",
       "                      -1.7220e-02, -1.6495e-02, -1.0913e-02, -1.0844e-02, -1.2303e-02,\n",
       "                      -8.2018e-03, -7.7996e-03, -1.6528e-02, -7.1467e-03, -1.4789e-02,\n",
       "                      -1.5623e-02, -1.6500e-02, -1.1875e-02, -9.4352e-03, -6.8936e-03,\n",
       "                      -7.9904e-03, -1.3406e-02, -8.6749e-03, -1.2007e-02, -1.1754e-02,\n",
       "                      -6.3221e-03, -1.2645e-02, -1.5027e-02, -1.6334e-02, -1.9413e-02,\n",
       "                      -5.7618e-03, -1.3667e-02, -1.3773e-02, -1.3610e-02, -1.9192e-02,\n",
       "                      -1.7201e-02, -1.1692e-02, -1.0291e-02, -2.1180e-02, -1.6569e-02,\n",
       "                      -1.0889e-02, -1.1216e-02, -3.1901e-02, -1.1275e-02, -6.1800e-03,\n",
       "                      -1.4882e-02, -4.3173e-03, -1.1908e-02, -8.1083e-03, -9.8638e-03,\n",
       "                      -1.6364e-02, -2.1352e-02, -2.7058e-02, -1.0902e-02, -9.2801e-03,\n",
       "                      -2.1682e-02, -1.2098e-02, -6.7505e-03, -9.0495e-03, -9.8174e-03,\n",
       "                       5.7620e-03, -2.0495e-02, -3.6395e-04, -8.7247e-03,  2.9634e-03,\n",
       "                      -6.5962e-03, -6.8578e-03, -1.5834e-02, -1.6950e-02, -8.5138e-03,\n",
       "                      -6.9163e-03, -8.5400e-03, -1.3476e-02, -4.7680e-03, -1.6358e-02,\n",
       "                      -1.9504e-02])),\n",
       "             ('blocks.2.update.uattn.lvs.weight',\n",
       "              tensor([[-0.1100, -0.0145,  0.0547,  ...,  0.0917,  0.1509,  0.1548],\n",
       "                      [-0.0946, -0.1301,  0.0022,  ...,  0.1308,  0.1018,  0.1321],\n",
       "                      [-0.1046,  0.0317, -0.1436,  ..., -0.0321, -0.0351,  0.1683],\n",
       "                      ...,\n",
       "                      [ 0.1611, -0.1592, -0.0871,  ..., -0.0227,  0.0363,  0.0683],\n",
       "                      [-0.1600, -0.0808,  0.0949,  ...,  0.0284, -0.0807,  0.0003],\n",
       "                      [-0.1016,  0.0458,  0.0558,  ..., -0.1630,  0.1149, -0.0673]])),\n",
       "             ('blocks.2.update.uattn.lvs.bias',\n",
       "              tensor([-1.1852e-03,  4.2311e-03,  4.3824e-02,  7.7997e-03,  8.1173e-03,\n",
       "                      -2.9627e-03,  8.4847e-03, -1.8292e-02,  3.9232e-03,  1.4734e-02,\n",
       "                       1.6961e-02, -1.7545e-02, -9.0710e-03, -1.4135e-02,  6.8573e-03,\n",
       "                       1.6873e-02,  6.1356e-03,  1.0633e-02,  1.0330e-02,  1.5397e-02,\n",
       "                       1.2211e-02,  9.0453e-03, -1.0069e-02, -5.7730e-03,  7.9897e-03,\n",
       "                      -9.4408e-03,  1.2000e-02, -9.5898e-03,  9.0827e-03,  8.5372e-03,\n",
       "                      -7.7098e-03, -9.7562e-03, -5.3460e-03, -9.6316e-03,  6.6942e-03,\n",
       "                       7.9636e-03, -9.3011e-03,  4.2924e-03,  7.9533e-03, -8.3894e-03,\n",
       "                      -7.8322e-03,  8.3737e-03, -9.5340e-03, -4.7203e-05, -1.0237e-02,\n",
       "                      -9.9718e-04,  1.4647e-02, -5.2865e-03, -9.2515e-03, -7.1832e-03,\n",
       "                      -7.9596e-03,  8.2942e-03, -4.5014e-03,  1.1654e-02, -6.6593e-03,\n",
       "                      -8.5272e-03,  1.2864e-02, -8.4490e-03, -5.3360e-03, -9.2024e-03,\n",
       "                       9.6767e-03, -8.8153e-03, -9.3015e-03,  7.6995e-03,  7.6296e-03,\n",
       "                      -6.0431e-03,  7.2349e-03, -8.9663e-03, -7.8304e-04,  2.3719e-02,\n",
       "                       1.4758e-02,  1.1038e-02, -1.0235e-02,  8.8674e-03,  1.0079e-02,\n",
       "                       2.7950e-02, -9.0963e-03, -6.2454e-03, -9.4948e-03, -6.1600e-03,\n",
       "                      -5.3339e-03,  5.9876e-03, -5.4978e-03,  4.4506e-03,  4.3789e-03,\n",
       "                      -6.2749e-03, -9.6702e-03,  4.4583e-02,  2.4123e-03,  1.1858e-02,\n",
       "                       2.0541e-02,  7.8153e-03,  1.4016e-02, -8.3766e-03, -6.9532e-03,\n",
       "                      -9.2645e-03, -4.0096e-03,  8.2195e-03, -9.6162e-03,  9.1760e-03,\n",
       "                      -8.1630e-03,  8.7759e-03, -9.1415e-03, -4.0566e-03,  1.1348e-02,\n",
       "                      -8.5378e-03, -7.4387e-03, -8.8251e-03, -8.9264e-03, -8.5338e-03,\n",
       "                       7.8158e-03,  3.9750e-02,  8.9414e-03, -8.9985e-03,  5.4870e-03,\n",
       "                      -8.4507e-03,  7.0486e-03, -7.8930e-03,  1.1186e-02,  8.1310e-03,\n",
       "                      -9.1702e-03, -7.7890e-03,  4.2676e-03, -8.2686e-03,  4.6586e-02,\n",
       "                       1.0149e-02, -7.7043e-03,  2.0730e-02])),\n",
       "             ('blocks.2.update.uattn.tp1.weight', tensor([])),\n",
       "             ('blocks.2.update.uattn.tp1.output_mask',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('blocks.2.update.uattn.tp2.weight',\n",
       "              tensor([ 1.3199,  0.2538,  0.5129,  0.5079, -0.2320, -0.3205, -0.4965, -0.2666,\n",
       "                      -0.4385, -0.3008,  0.1237, -2.1876,  2.1171, -0.5454, -0.2815, -0.0221,\n",
       "                      -1.5291,  0.1159, -0.6503, -0.1074, -0.7860,  0.1148, -0.3727, -1.9089,\n",
       "                       0.8881,  1.0865,  0.2524,  0.7652,  0.0060,  0.7545,  0.2912,  0.9813,\n",
       "                      -1.9557, -0.0087,  0.7706,  0.3531,  0.2541, -0.4493,  1.2704, -1.8156,\n",
       "                      -1.6906, -0.9980, -0.3818, -1.5544,  2.0180,  0.7499, -0.3345,  0.0783,\n",
       "                      -0.7152, -1.1565, -0.7998, -1.4712, -0.4058, -0.0414, -0.0451, -0.0500,\n",
       "                       0.3861, -0.3138, -1.0708,  0.1884,  0.2165,  0.5124,  0.1208,  0.7418,\n",
       "                       1.0150, -0.7946, -0.4255,  0.5482, -0.9690, -0.1532, -0.1252, -0.4475,\n",
       "                       0.6811,  1.9878, -1.3851,  0.2752,  0.4003, -1.4933,  0.8987,  0.7245,\n",
       "                      -1.3351, -1.4252,  0.0402,  0.5640,  0.2851,  1.2195,  0.1456,  1.2486,\n",
       "                      -0.2583, -0.9659,  0.0895,  0.2795, -0.1342, -0.7152,  0.9290,  1.4749,\n",
       "                       0.6718,  1.4013, -0.2188,  0.1500,  0.2032, -0.1918,  0.2030, -1.1960,\n",
       "                       0.9828, -1.8346, -0.4663,  0.4844, -0.2510,  0.7700,  0.8446, -1.0648,\n",
       "                       0.1716,  0.6511,  0.3326, -0.4188,  0.9838, -1.4087,  0.5368,  1.0668,\n",
       "                       0.2132, -0.1868,  0.9488, -1.3628, -0.6068,  1.1179,  0.9417,  0.3958,\n",
       "                       0.8299, -0.7546, -0.2289,  0.0582,  0.0408, -1.0573, -0.0322,  0.2720,\n",
       "                      -0.2481, -0.6436,  0.0799,  0.2204,  0.7198,  1.0847, -0.2215, -1.1355,\n",
       "                       0.0306,  0.8375,  0.9104, -0.8083, -1.7297, -0.7968,  0.2138,  0.8595,\n",
       "                      -1.3859, -1.5101,  2.2441,  2.1056,  0.1420, -0.1230,  0.6237,  0.9801,\n",
       "                       0.6105, -1.1863, -1.6291,  0.4162, -0.5621,  0.3384, -1.2463, -0.1134,\n",
       "                       0.3749,  1.4960,  0.3225, -0.8574, -1.4282, -0.0554,  0.2766, -0.5008,\n",
       "                       0.1812,  0.5706,  0.1916, -0.8674, -0.2618, -0.0593, -0.0968,  1.0929,\n",
       "                       1.1971, -0.7145,  0.4645, -0.9260,  0.6881, -0.0838,  1.0791,  0.3190,\n",
       "                      -0.6178, -1.4290, -0.7639,  0.0992,  0.6107, -0.2516,  0.6774, -2.2624,\n",
       "                       0.3831,  0.1226,  1.4578,  0.7001,  1.6108, -0.1078,  0.3293,  1.2914,\n",
       "                       0.0425, -0.6586,  1.3752,  0.8207,  1.4342,  0.8093, -0.6698,  0.7824,\n",
       "                      -0.1419, -2.4353, -0.9826, -0.3318,  1.0390,  0.1715,  0.4874, -0.6096,\n",
       "                      -0.3157,  0.5873,  0.1157, -0.8040,  0.6184, -0.2847,  1.0489, -1.2448,\n",
       "                      -1.4064, -0.0536,  0.5980,  0.9351,  0.5597, -1.5338, -0.6497,  0.6053,\n",
       "                       1.7870,  1.3953, -0.5135,  0.2852, -1.2458,  0.3590,  1.3432, -1.0256,\n",
       "                       1.0977, -1.7861,  1.3622,  1.3257, -0.8421, -1.7753,  0.2984,  0.1638,\n",
       "                       0.9159,  1.8623, -0.7999,  0.2278,  0.9475,  0.8029, -0.5356,  1.6454,\n",
       "                       0.5172, -0.5569, -0.6363,  1.0324,  0.5770,  1.4671,  0.2431,  0.6759,\n",
       "                       1.3685, -1.2744, -0.4892,  1.2079, -0.8143, -0.7153,  0.7457,  0.0333,\n",
       "                       0.2005,  1.0961, -1.1326,  0.4355, -1.5926, -0.6177,  0.3326, -1.5668,\n",
       "                       0.1582,  0.6053,  0.6589,  0.7849,  1.0540,  0.1423,  1.1418,  0.2446,\n",
       "                      -0.3646,  1.9498, -1.1567,  0.5046,  0.9505,  0.2253, -0.3634,  2.1380,\n",
       "                      -0.9249, -0.1691,  0.9193, -0.9603,  1.4643, -0.5881, -0.8925,  0.5895,\n",
       "                       0.4417,  0.8524,  0.0719, -1.5141, -1.1465, -0.6454,  0.8416,  0.1980,\n",
       "                       1.2411,  0.3219, -1.1790,  0.6658,  0.7534,  0.5023, -0.7475,  0.0686,\n",
       "                      -0.7129,  0.4651,  0.2475,  0.6901, -1.0204, -1.6940,  0.0708, -0.0093,\n",
       "                       0.7509, -0.9704,  0.4326,  0.2480,  2.3131,  0.4544,  0.0937,  1.5098,\n",
       "                      -1.0390,  1.3322, -1.0812,  0.6353, -0.2754, -0.7535,  0.7616, -0.1741,\n",
       "                      -0.1543,  0.8045,  1.1985,  0.7122,  0.0602, -0.9517,  0.1783,  2.1158,\n",
       "                      -0.6773, -0.0629,  1.5685, -0.4149,  0.1430,  0.4885, -0.1670, -0.4011,\n",
       "                      -1.2465, -2.0498, -0.0576,  0.6036, -0.8348,  0.1141,  0.4270,  0.6669,\n",
       "                      -0.6653,  0.2111,  0.0468,  0.3144,  0.8570, -0.6498, -1.2862,  1.0931])),\n",
       "             ('blocks.2.update.uattn.tp2.output_mask',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('blocks.2.update.uattn.actlvs.alpha',\n",
       "              tensor([0.9942, 0.9707, 1.0395, 1.0041, 1.0085, 1.0094, 1.0027, 1.0186, 1.0034,\n",
       "                      1.0367, 1.0283, 1.0079, 0.9936, 1.0141, 1.0189, 1.0280, 1.0025, 1.0109,\n",
       "                      1.0094, 1.0327, 1.0173, 1.0092, 0.9878, 1.0096, 1.0082, 1.0107, 0.9989,\n",
       "                      1.0061, 1.0010, 1.0112, 0.9924, 0.9893, 0.9975, 0.9910, 0.9921, 1.0053,\n",
       "                      0.9965, 1.0073, 0.9933, 0.9925, 0.9984, 0.9963, 0.9910, 1.0042, 0.9919,\n",
       "                      1.0083, 1.0122, 1.0112, 1.0001, 0.9973, 1.0069, 1.0078, 0.9962, 0.9952,\n",
       "                      0.9909, 0.9945, 1.0349, 0.9908, 0.9962, 1.0086, 1.0049, 0.9903, 1.0118,\n",
       "                      1.0063, 1.0038, 1.0092, 1.0064, 1.0093, 0.9925, 0.9946, 0.9919, 1.0121,\n",
       "                      0.9903, 1.0183, 1.0087, 1.1198, 1.0083, 1.0350, 0.9924, 0.9873, 0.9921,\n",
       "                      1.0086, 1.0021, 0.9890, 0.9921, 0.9908, 1.0093, 1.0421, 1.0011, 0.9927,\n",
       "                      0.9918, 1.0065, 0.9974, 1.0083, 0.9922, 0.9914, 0.9981, 0.9890, 1.0077,\n",
       "                      1.0089, 1.0023, 1.0063, 0.9910, 0.9901, 0.9884, 0.9914, 0.9929, 1.0074,\n",
       "                      0.9958, 1.0084, 1.0362, 1.0601, 1.0099, 0.9897, 1.0018, 1.0042, 1.0035,\n",
       "                      0.9855, 1.0256, 1.0047, 1.0091, 0.9927, 1.0163, 0.9924, 1.0491, 1.0149,\n",
       "                      0.9915, 1.0548])),\n",
       "             ('blocks.2.update.uattn.actlvs.beta',\n",
       "              tensor([1.7120, 1.6986, 1.7129, 1.7104, 1.7110, 1.6955, 1.7124, 1.6895, 1.7087,\n",
       "                      1.7225, 1.7202, 1.6956, 1.6912, 1.6929, 1.7150, 1.7015, 1.7050, 1.7111,\n",
       "                      1.7112, 1.6971, 1.7215, 1.7113, 1.6930, 1.6940, 1.7106, 1.6928, 1.7227,\n",
       "                      1.6934, 1.7108, 1.7176, 1.6937, 1.6918, 1.6974, 1.6924, 1.7004, 1.7096,\n",
       "                      1.6968, 1.6735, 1.7085, 1.6935, 1.6917, 1.6972, 1.6915, 1.7075, 1.6987,\n",
       "                      1.6963, 1.7120, 1.6907, 1.6993, 1.6952, 1.6936, 1.7118, 1.6898, 1.7121,\n",
       "                      1.6945, 1.6929, 1.7101, 1.6926, 1.6684, 1.6974, 1.6982, 1.6938, 1.6985,\n",
       "                      1.7110, 1.7012, 1.6972, 1.7019, 1.6952, 1.7095, 1.7113, 1.7067, 1.7122,\n",
       "                      1.6952, 1.7074, 1.7107, 1.6967, 1.6973, 1.7044, 1.7111, 1.7104, 1.6922,\n",
       "                      1.6940, 1.6853, 1.7107, 1.7103, 1.7067, 1.6950, 1.7177, 1.7004, 1.7109,\n",
       "                      1.7112, 1.7109, 1.7039, 1.6943, 1.6977, 1.6932, 1.6866, 1.7121, 1.6942,\n",
       "                      1.7128, 1.6985, 1.7084, 1.6930, 1.6931, 1.7025, 1.6919, 1.6910, 1.6972,\n",
       "                      1.6934, 1.6955, 1.7266, 1.7038, 1.7142, 1.6933, 1.7059, 1.6939, 1.7071,\n",
       "                      1.7011, 1.7144, 1.7100, 1.6915, 1.6989, 1.7089, 1.6905, 1.7316, 1.6915,\n",
       "                      1.6925, 1.7225])),\n",
       "             ('sout.mlp.0.weight',\n",
       "              tensor([[ 0.1294,  0.0762, -0.0739,  ...,  0.1372, -0.0728, -0.0135],\n",
       "                      [-0.0329, -0.0086, -0.0768,  ...,  0.0320,  0.0622,  0.0832],\n",
       "                      [ 0.1614, -0.0432, -0.0082,  ...,  0.0820, -0.0776,  0.0616],\n",
       "                      ...,\n",
       "                      [-0.1272, -0.0474,  0.0006,  ...,  0.1061,  0.0987,  0.0540],\n",
       "                      [-0.0242, -0.0226,  0.0090,  ...,  0.0864, -0.1410,  0.0619],\n",
       "                      [ 0.0805,  0.0440,  0.0488,  ..., -0.0548, -0.0813, -0.1382]])),\n",
       "             ('sout.mlp.0.bias',\n",
       "              tensor([ 0.0092, -0.0093,  0.0105, -0.0136, -0.0112, -0.0132,  0.0102,  0.0095,\n",
       "                      -0.0099, -0.0097,  0.0084, -0.0098,  0.0089, -0.0093,  0.0100,  0.0089,\n",
       "                      -0.0096,  0.0262,  0.0092, -0.0108,  0.0011, -0.0091,  0.0263, -0.0117,\n",
       "                      -0.0088, -0.0093,  0.0087,  0.0040,  0.0006,  0.0092,  0.0092,  0.0088,\n",
       "                      -0.0094, -0.0093, -0.0089,  0.0087,  0.0091,  0.0099,  0.0089,  0.0122,\n",
       "                      -0.0097, -0.0090, -0.0094,  0.0094, -0.0094,  0.0092,  0.0096,  0.0087,\n",
       "                      -0.0086,  0.0095,  0.0093, -0.0103,  0.0083, -0.0098, -0.0089,  0.0098,\n",
       "                       0.0095,  0.0093,  0.0098,  0.0116,  0.0089,  0.0092, -0.0093, -0.0093,\n",
       "                      -0.0125, -0.0110, -0.0080,  0.0085, -0.0095, -0.0102,  0.0104,  0.0116,\n",
       "                      -0.0096, -0.0034, -0.0098,  0.0097, -0.0096, -0.0104, -0.0094,  0.0088,\n",
       "                      -0.0094,  0.0095,  0.0096,  0.0093, -0.0084,  0.0100, -0.0094, -0.0098,\n",
       "                      -0.0115, -0.0128, -0.0121,  0.0102,  0.0093,  0.0304, -0.0093,  0.0100,\n",
       "                      -0.0110,  0.0087, -0.0149, -0.0093,  0.0130,  0.0099,  0.0083,  0.0091,\n",
       "                      -0.0092,  0.0091,  0.0088,  0.0098,  0.0090, -0.0097,  0.0105, -0.0097,\n",
       "                       0.0092,  0.0121, -0.0092, -0.0100,  0.0092,  0.0082,  0.0096, -0.0096,\n",
       "                       0.0117,  0.0090,  0.0084,  0.0098,  0.0137, -0.0093, -0.0093, -0.0098])),\n",
       "             ('sout.mlp.1.alpha',\n",
       "              tensor([1.0062, 0.9911, 1.0009, 1.0098, 1.0067, 1.0079, 1.0091, 1.0068, 0.9904,\n",
       "                      1.0015, 1.0013, 1.0118, 1.0051, 0.9918, 1.0061, 1.0069, 0.9906, 1.0029,\n",
       "                      0.9961, 1.0043, 0.9901, 0.9922, 1.0008, 1.0114, 1.0088, 0.9904, 0.9997,\n",
       "                      1.0102, 0.9963, 1.0068, 1.0067, 0.9894, 1.0043, 0.9958, 1.0081, 1.0060,\n",
       "                      1.0065, 1.0103, 1.0042, 1.0573, 0.9909, 0.9918, 0.9941, 1.0100, 0.9908,\n",
       "                      1.0043, 1.0068, 1.0032, 0.9964, 0.9954, 0.9921, 1.0084, 1.0005, 0.9909,\n",
       "                      1.0101, 1.0110, 1.0087, 1.0006, 1.0059, 1.0030, 1.0098, 1.0054, 0.9921,\n",
       "                      1.0046, 1.0032, 1.0085, 1.0066, 1.0062, 0.9912, 1.0133, 0.9927, 0.9912,\n",
       "                      0.9925, 1.0084, 0.9976, 1.0067, 0.9924, 1.0001, 0.9927, 1.0043, 0.9926,\n",
       "                      1.0038, 1.0087, 1.0069, 1.0021, 1.0063, 0.9911, 0.9893, 1.0083, 1.0096,\n",
       "                      1.0075, 1.0200, 1.0063, 0.9969, 0.9904, 0.9892, 0.9808, 1.0064, 1.0101,\n",
       "                      0.9934, 1.0370, 1.0065, 0.9900, 1.0039, 1.0020, 1.0067, 1.0073, 1.0063,\n",
       "                      1.0061, 0.9900, 0.9894, 0.9894, 1.0040, 1.0095, 0.9912, 0.9928, 1.0068,\n",
       "                      0.9993, 1.0062, 0.9897, 1.0106, 1.0053, 1.0039, 1.0067, 1.0120, 0.9978,\n",
       "                      0.9972, 0.9902])),\n",
       "             ('sout.mlp.1.beta',\n",
       "              tensor([1.7108, 1.6931, 1.7124, 1.6925, 1.6933, 1.6924, 1.7138, 1.7091, 1.6948,\n",
       "                      1.6926, 1.7110, 1.6936, 1.7111, 1.6935, 1.7141, 1.7113, 1.6932, 1.7123,\n",
       "                      1.7122, 1.6945, 1.7121, 1.6936, 1.7124, 1.6926, 1.6929, 1.6914, 1.7103,\n",
       "                      1.6928, 1.7027, 1.7104, 1.7135, 1.7121, 1.6941, 1.6971, 1.6958, 1.7106,\n",
       "                      1.7118, 1.7143, 1.7096, 1.7235, 1.6921, 1.6930, 1.6934, 1.7111, 1.6931,\n",
       "                      1.7072, 1.7119, 1.7101, 1.6951, 1.7115, 1.7150, 1.6932, 1.7090, 1.6925,\n",
       "                      1.6927, 1.7178, 1.7128, 1.7144, 1.7134, 1.7132, 1.7130, 1.7086, 1.6924,\n",
       "                      1.6936, 1.6979, 1.6926, 1.6929, 1.7121, 1.6931, 1.6933, 1.7107, 1.7115,\n",
       "                      1.6986, 1.6925, 1.6987, 1.7112, 1.6979, 1.6934, 1.6940, 1.7102, 1.6948,\n",
       "                      1.7069, 1.6921, 1.7133, 1.6934, 1.7131, 1.6944, 1.6920, 1.6944, 1.6928,\n",
       "                      1.6941, 1.7132, 1.7134, 1.7117, 1.6900, 1.7109, 1.6931, 1.7111, 1.6934,\n",
       "                      1.6959, 1.7124, 1.7137, 1.7108, 1.7084, 1.6961, 1.7088, 1.7107, 1.7126,\n",
       "                      1.7129, 1.6929, 1.7104, 1.6956, 1.7079, 1.7118, 1.6920, 1.6994, 1.7094,\n",
       "                      1.7093, 1.7132, 1.6910, 1.7116, 1.7113, 1.7107, 1.7128, 1.7122, 1.6916,\n",
       "                      1.6921, 1.6922])),\n",
       "             ('sout.mlp.3.weight',\n",
       "              tensor([[-0.1955,  0.0245, -0.1139,  0.0251,  0.0253,  0.0755, -0.1724, -0.1631,\n",
       "                        0.1318,  0.1309, -0.0920,  0.2266, -0.0649,  0.0136, -0.1500, -0.1181,\n",
       "                        0.0684, -0.1536, -0.1636,  0.0999, -0.1629,  0.1815, -0.1540,  0.1199,\n",
       "                        0.1725,  0.0211, -0.0415,  0.2182, -0.0059, -0.0395, -0.1111, -0.0559,\n",
       "                        0.1965,  0.1953,  0.1644, -0.0221, -0.2191, -0.1496, -0.1843, -0.0568,\n",
       "                        0.1532,  0.0208,  0.1091, -0.0407,  0.0532, -0.1618, -0.1341, -0.1410,\n",
       "                        0.1177, -0.1536, -0.0848,  0.0442, -0.1284,  0.0182,  0.1907, -0.1869,\n",
       "                       -0.2065, -0.1591, -0.1313, -0.1421, -0.1556, -0.0660,  0.1318,  0.1540,\n",
       "                        0.0142,  0.0991,  0.1150, -0.1359,  0.1518,  0.0720, -0.1969, -0.0679,\n",
       "                        0.1194,  0.0499,  0.1086, -0.0772,  0.1710,  0.1908,  0.1688, -0.1528,\n",
       "                        0.1819, -0.1473,  0.0942, -0.1227,  0.1857, -0.0872,  0.1660,  0.0595,\n",
       "                        0.1786,  0.2052,  0.1089, -0.0922, -0.1892, -0.2043,  0.0217, -0.0839,\n",
       "                        0.0546, -0.0410,  0.0688,  0.1600, -0.1328, -0.2147, -0.1291, -0.1169,\n",
       "                        0.1184, -0.0361, -0.0824, -0.1011, -0.1318,  0.0384, -0.0900,  0.0380,\n",
       "                       -0.1655, -0.0281,  0.0200,  0.1290, -0.0146, -0.0353, -0.2079,  0.0557,\n",
       "                       -0.1833, -0.1883, -0.1706, -0.0907, -0.0327,  0.1918,  0.1848,  0.0761]])),\n",
       "             ('sout.mlp.3.bias', tensor([-0.0097]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''After the training is completed, take out the learnable parameters and save them as a. pth file.'''\n",
    "state_dict=model.state_dict\n",
    "\n",
    "state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'trained_param/homo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_429375/3306458689.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/media/maria/work_space/capsule-3259363/code/trained_param/homo.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "device=torch.device('cpu')\n",
    "\n",
    "trained_model=DetaNet(num_features=128,\n",
    "                 act='swish',\n",
    "                 maxl=3,\n",
    "                 num_block=3,\n",
    "                 radial_type='trainable_bessel',\n",
    "                 num_radial=32,\n",
    "                 attention_head=8,\n",
    "                 rc=5.0,\n",
    "                 dropout=0.0,\n",
    "                 use_cutoff=False,\n",
    "                 max_atomic_number=9,\n",
    "                 atom_ref=None,\n",
    "                 scale=None,\n",
    "                 scalar_outsize=1,\n",
    "                 irreps_out=None,\n",
    "                 summation=False,\n",
    "                 norm=False,\n",
    "                 out_type='scalar',\n",
    "                 grad_type=None ,\n",
    "                 device=torch.device(device))\n",
    "state_dict = torch.load(\"/media/maria/work_space/capsule-3259363/code/trained_param/homo.pth\")\n",
    "trained_model.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation setting: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Summation setting:\", trained_model.summation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetaNet(\n",
       "  (Embedding): Embedding(\n",
       "    (act): Swish()\n",
       "    (elec_emb): Linear(in_features=16, out_features=128, bias=False)\n",
       "    (nuclare_emb): Embedding(10, 128)\n",
       "    (ls): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (Radial): Radial_Basis(\n",
       "    (radial): Bessel_Function()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Interaction_Block(\n",
       "      (message): Message(\n",
       "        (Attention): Edge_Attention(\n",
       "          (actq): Swish()\n",
       "          (actk): Swish()\n",
       "          (actv): Swish()\n",
       "          (acta): Swish()\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (lq): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lk): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (lv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (la): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (lrbf): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (lkrbf): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (lvrbf): Linear(in_features=128, out_features=256, bias=False)\n",
       "        )\n",
       "        (tp): TensorProduct(128x0e x 1x1o+1x2e+1x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "      )\n",
       "      (update): Update(\n",
       "        (actu): Swish()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (outt): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "        (outs): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (uattn): Tensorproduct_Attention(\n",
       "          (lq): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lk): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (lv): Linear(128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 49152 weights)\n",
       "          (ls): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (lvs): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (tp1): TensorProduct(128x1o+128x2e+128x3o x 128x1o+128x2e+128x3o -> 128x0e | 384 paths | 0 weights)\n",
       "          (tp2): TensorProduct(128x0e x 128x1o+128x2e+128x3o -> 128x1o+128x2e+128x3o | 384 paths | 384 weights)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (actlvs): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sout): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "trained_model.eval()\n",
    "device=torch.device('cpu')\n",
    "\n",
    "eval_data = val_datasets[:300]\n",
    "eval_loader = DataLoader(eval_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "for batch in eval_loader:\n",
    "    true_values.append(batch.y)\n",
    "    with torch.no_grad():\n",
    "            val_out = trained_model(pos=batch.pos.to(device), z=batch.z.to(device),\n",
    "                    batch=batch.batch.to(device))\n",
    "            val_graph_out = global_mean_pool(val_out, batch.batch)  # Shape: [val_batch_size, d]\n",
    "            predictions.append(val_graph_out.item())\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(true_values, predictions, c='blue', alpha=0.7, label='Predictions')\n",
    "plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], color='red', linestyle='--', label='y=x')\n",
    "plt.xlabel('True HOMO Values (eV)')\n",
    "plt.ylabel('Predicted HOMO Values (eV)')\n",
    "plt.title('HOMO Predictions vs True Values')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected R2: 0.8733291868423835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(true_values, predictions)\n",
    "print(\"Corrected R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAOVNJREFUeJzt3XtYVVX+x/HPAeTgDdBQLkqiaIhZqDg62EUtDNNMm6nMSpG8VV7DLjqVqE1ZpuZMkWam9jhdbBqzfmkaokxZqIlSaV7ylqaBmhcUFRTW748ezngEFBA4sH2/nmc/01l7rb2/++zO8Gnvtc+xGWOMAAAALMLN1QUAAACUJ8INAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINUEWEhIRo4MCBri7D8l599VU1a9ZM7u7uatOmjavLKaRLly7q0qXLZfulpKTIZrMpJSWlQuux2WyaOHFihe4DKG+EG6ACLFiwQDabTRs2bChyfZcuXdS6desr3s+yZcv4w1MKX375pZ5++mnddNNNmj9/vl566aVi+w4cOFA2m82x2O12XXfddZowYYLOnj1biVUDKC0PVxcA4A/bt2+Xm1vp/ntj2bJlSkxMJOCU0KpVq+Tm5qZ33nlHnp6el+1vt9s1d+5cSdKJEyf06aef6oUXXtCuXbv03nvvVUiNX375ZYVsF7iaEG6AKsJut7u6hFLLzs5W7dq1XV1GiR06dEg1a9YsUbCRJA8PDz388MOO148//rg6deqkDz74QDNmzJC/v3+511jS2gAUj9tSQBVx8Zybc+fOadKkSWrRooW8vLx0zTXX6Oabb1ZSUpKkP26bJCYmSpLT7ZMC2dnZGjt2rIKDg2W32xUWFqZp06bJGOO03zNnzmjUqFHy8/NT3bp1dffdd+vAgQOF5lpMnDhRNptNP/30kx588EHVq1dPN998syTphx9+0MCBA9WsWTN5eXkpICBAjzzyiH7//XenfRVsY8eOHXr44Yfl4+OjBg0a6Pnnn5cxRvv371fv3r3l7e2tgIAATZ8+vUTv3fnz5/XCCy8oNDRUdrtdISEh+tvf/qacnBxHH5vNpvnz5ys7O9vxXi1YsKBE279wGzfffLOMMdq9e7fTui+++EK33HKLateurbp166pnz57asmWLU5+MjAzFxcWpcePGstvtCgwMVO/evbV3715Hn6Lm3Pz666/q06ePateurYYNG+qJJ55wOrYCxc3bunibubm5mjBhgiIjI+Xj46PatWvrlltu0erVqy/7Hpw8eVJjxoxRSEiI7Ha7GjZsqG7dumnjxo2XHQtUFq7cABXoxIkTOnLkSKH2c+fOXXbsxIkTNWXKFA0ePFgdOnRQVlaWNmzYoI0bN6pbt24aNmyYDh48qKSkJC1cuNBprDFGd999t1avXq1BgwapTZs2WrFihZ566ikdOHBAr732mqPvwIED9dFHH6l///7685//rP/+97/q2bNnsXXdd999atGihV566SVHUEpKStLu3bsVFxengIAAbdmyRXPmzNGWLVu0du1ap9AlSX379lV4eLhefvllLV26VH//+99Vv359vfXWW7rtttv0yiuv6L333tOTTz6pP/3pT7r11lsv+V4NHjxY7777ru69916NHTtW69at05QpU7R161Z98sknkqSFCxdqzpw5Wr9+veNWU6dOnS57Hi5WEETq1avnaFu4cKFiY2MVExOjV155RadPn9asWbN08803a9OmTQoJCZEk/fWvf9WWLVs0cuRIhYSE6NChQ0pKStK+ffscfS525swZ3X777dq3b59GjRqloKAgLVy4UKtWrSp17QWysrI0d+5c9evXT0OGDNHJkyf1zjvvKCYmRuvXr7/kROtHH31UH3/8sUaMGKFWrVrp999/15o1a7R161a1a9euzDUB5coAKHfz5883ki65XH/99U5jmjRpYmJjYx2vIyIiTM+ePS+5n+HDh5uiPsZLliwxkszf//53p/Z7773X2Gw2s3PnTmOMMWlpaUaSGTNmjFO/gQMHGkkmISHB0ZaQkGAkmX79+hXa3+nTpwu1ffDBB0aS+eqrrwptY+jQoY628+fPm8aNGxubzWZefvllR/uxY8dMzZo1nd6ToqSnpxtJZvDgwU7tTz75pJFkVq1a5WiLjY01tWvXvuT2Lu57+PBhc/jwYbNz504zbdo0Y7PZTOvWrU1+fr4xxpiTJ08aX19fM2TIEKfxGRkZxsfHx9F+7NgxI8m8+uqrl9xv586dTefOnR2vZ86caSSZjz76yNGWnZ1tmjdvbiSZ1atXO9ov/neouG2eP3/e5OTkOPU5duyY8ff3N4888ohT+8X/Hvj4+Jjhw4df8hgAV+O2FFCBEhMTlZSUVGi58cYbLzvW19dXW7Zs0c8//1zq/S5btkzu7u4aNWqUU/vYsWNljNEXX3whSVq+fLmkP+aSXGjkyJHFbvvRRx8t1FazZk3HP589e1ZHjhzRn//8Z0kq8nbF4MGDHf/s7u6u9u3byxijQYMGOdp9fX0VFhZW6PbPxZYtWyZJio+Pd2ofO3asJGnp0qWXHH8p2dnZatCggRo0aKDmzZvrySef1E033aRPP/3UcTUqKSlJx48fV79+/XTkyBHH4u7uro4dOzpu9RTM9UlJSdGxY8dKXMOyZcsUGBioe++919FWq1YtDR06tMzH5e7u7pjbk5+fr6NHj+r8+fNq3779ZW8v+fr6at26dTp48GCZ9w9UNG5LARWoQ4cOat++faH2evXqFXm76kKTJ09W7969dd1116l169bq3r27+vfvX6Jg9MsvvygoKEh169Z1ag8PD3esL/hfNzc3NW3a1Klf8+bNi932xX0l6ejRo5o0aZI+/PBDHTp0yGndiRMnCvW/9tprnV77+PjIy8tLfn5+hdovnrdzsYJjuLjmgIAA+fr6Oo61LLy8vPR///d/kv6Y9zJ16lTHpOQCBeHztttuK3Ib3t7ekv6YMP7KK69o7Nix8vf315///GfdddddGjBggAICAi55fM2bNy90ay8sLKzMxyVJ7777rqZPn65t27Y53SYt6vxeaOrUqYqNjVVwcLAiIyPVo0cPDRgwQM2aNbuieoDyxJUboIq69dZbtWvXLs2bN0+tW7fW3Llz1a5dO8d8EVe58A97gfvvv19vv/22Hn30US1evFhffvml46pQfn5+of7u7u4lapNUaAJ0cS7+418e3N3dFR0drejoaA0cOFDJycnKyMjQsGHDHH0Kjm/hwoVFXqX79NNPHX3HjBmjHTt2aMqUKfLy8tLzzz+v8PBwbdq0qVzqLe49yMvLc3r9r3/9SwMHDlRoaKjeeecdLV++XElJSbrtttuKPF8Xuv/++7V79269/vrrCgoK0quvvqrrr7/ecTUQqAoIN0AVVr9+fcXFxemDDz7Q/v37deONNzo9wVTcH7MmTZro4MGDOnnypFP7tm3bHOsL/jc/P1979uxx6rdz584S13js2DElJydr3LhxmjRpku655x5169at0v5LvuAYLr59l5mZqePHjzuOtTwEBgbqiSee0P/93/9p7dq1kqTQ0FBJUsOGDR1B6MLl4iefQkNDNXbsWH355ZfavHmzcnNzL/lUWJMmTbRr165CIW/79u2F+tarV0/Hjx8v1H7x1auPP/5YzZo10+LFi9W/f3/FxMQoOjq6xF9OGBgYqMcff1xLlizRnj17dM011+jFF18s0VigMhBugCrq4tsxderUUfPmzZ0eAS74jpmL/6D16NFDeXl5euONN5zaX3vtNdlsNt15552SpJiYGEnSm2++6dTv9ddfL3GdBVdcLv7jO3PmzBJv40r06NGjyP3NmDFDki755FdZjBw5UrVq1dLLL78s6Y/30NvbWy+99FKRT8EdPnxYknT69OlC4SE0NFR169Yt8rHuAj169NDBgwf18ccfO9pOnz6tOXPmFOobGhqqtWvXKjc319H2+eefa//+/U79ijpn69atU2pqarF1SH9cAbr4NmPDhg0VFBR0yWMAKhtzboAqqlWrVurSpYsiIyNVv359bdiwwfEIboHIyEhJ0qhRoxQTEyN3d3c98MAD6tWrl7p27apnn31We/fuVUREhL788kt9+umnGjNmjONqQ2RkpP76179q5syZ+v333x2Pgu/YsUNSyW71eHt769Zbb9XUqVN17tw5NWrUSF9++WWhq0EVJSIiQrGxsZozZ46OHz+uzp07a/369Xr33XfVp08fde3atVz3d8011yguLk5vvvmmtm7dqvDwcM2aNUv9+/dXu3bt9MADD6hBgwbat2+fli5dqptuuklvvPGGduzYodtvv13333+/WrVqJQ8PD33yySfKzMzUAw88UOz+hgwZojfeeEMDBgxQWlqaAgMDtXDhQtWqVatQ38GDB+vjjz9W9+7ddf/992vXrl3617/+5TjfBe666y4tXrxY99xzj3r27Kk9e/Zo9uzZatWqlU6dOlVsLSdPnlTjxo117733KiIiQnXq1NHKlSv13Xfflfg7iYBK4cpHtQCrKngU/LvvvityfefOnS/7KPjf//5306FDB+Pr62tq1qxpWrZsaV588UWTm5vr6HP+/HkzcuRI06BBA2Oz2ZweCz958qR54oknTFBQkKlRo4Zp0aKFefXVVx2PMBfIzs42w4cPN/Xr1zd16tQxffr0Mdu3bzeSnB7NLniM+/Dhw4WO59dffzX33HOP8fX1NT4+Pua+++4zBw8eLPZx8ou3Udwj2kW9T0U5d+6cmTRpkmnatKmpUaOGCQ4ONuPHjzdnz54t0X6Kcqm+u3btMu7u7k7na/Xq1SYmJsb4+PgYLy8vExoaagYOHGg2bNhgjDHmyJEjZvjw4aZly5amdu3axsfHx3Ts2NHpEe+CY77wsW1jjPnll1/M3XffbWrVqmX8/PzM6NGjzfLlyws9Cm6MMdOnTzeNGjUydrvd3HTTTWbDhg2Ftpmfn29eeukl06RJE2O3203btm3N559/bmJjY02TJk2ctnfhOczJyTFPPfWUiYiIMHXr1jW1a9c2ERER5s033yzRewpUFpsxJZytB+CqkZ6errZt2+pf//qXHnroIVeXAwClwpwb4Cp35syZQm0zZ86Um5vbZb8ZGACqIubcAFe5qVOnKi0tTV27dpWHh4e++OILffHFFxo6dKiCg4NdXR4AlBq3pYCrXFJSkiZNmqSffvpJp06d0rXXXqv+/fvr2WeflYcH//0DoPoh3AAAAEthzg0AALAUwg0AALCUq+6Gen5+vg4ePKi6detWyG/RAACA8meM0cmTJxUUFCQ3t0tfm7nqws3Bgwd5AgQAgGpq//79aty48SX7XHXhpm7dupL+eHO8vb1dXA0AACiJrKwsBQcHO/6OX8pVF24KbkV5e3sTbgAAqGZKMqWECcUAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSXBpuvvrqK/Xq1UtBQUGy2WxasmTJZcekpKSoXbt2stvtat68uRYsWFDhdQIAgOrDpeEmOztbERERSkxMLFH/PXv2qGfPnuratavS09M1ZswYDR48WCtWrKjgSgEAQHXh0h/OvPPOO3XnnXeWuP/s2bPVtGlTTZ8+XZIUHh6uNWvW6LXXXlNMTExFlQkAAKqRajXnJjU1VdHR0U5tMTExSk1NdVFFAACgqnHplZvSysjIkL+/v1Obv7+/srKydObMGdWsWbPQmJycHOXk5DheZ2VlVXidAADAdarVlZuymDJlinx8fBxLcHCwq0uyvIkTJ5a47/S+d5XDDn2ufBtXKGB1etkGXlR7yLilJX5PkleFOm+qFO/7hcpce3mrAucRgDVUq3ATEBCgzMxMp7bMzEx5e3sXedVGksaPH68TJ044lv3791dGqQAAwEWq1W2pqKgoLVu2zKktKSlJUVFRxY6x2+2y2+0VXRoAAKgiXHrl5tSpU0pPT1d6erqkPx71Tk9P1759+yT9cdVlwIABjv6PPvqodu/eraefflrbtm3Tm2++qY8++khPPPGEK8oHAABVkEvDzYYNG9S2bVu1bdtWkhQfH6+2bdtqwoQJkqTffvvNEXQkqWnTplq6dKmSkpIUERGh6dOna+7cuTwGDgAAHFx6W6pLly4yxhS7vqhvH+7SpYs2bdpUgVUBAIDqrFpNKAYAALgcwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUl4ebxMREhYSEyMvLSx07dtT69esv2X/mzJkKCwtTzZo1FRwcrCeeeEJnz56tpGoBAEBV59Jws2jRIsXHxyshIUEbN25URESEYmJidOjQoSL7v//++xo3bpwSEhK0detWvfPOO1q0aJH+9re/VXLlAACgqnJpuJkxY4aGDBmiuLg4tWrVSrNnz1atWrU0b968Ivt/++23uummm/Tggw8qJCREd9xxh/r163fZqz0AAODq4bJwk5ubq7S0NEVHR/+vGDc3RUdHKzU1tcgxnTp1UlpamiPM7N69W8uWLVOPHj2K3U9OTo6ysrKcFgAAYF0ertrxkSNHlJeXJ39/f6d2f39/bdu2rcgxDz74oI4cOaKbb75ZxhidP39ejz766CVvS02ZMkWTJk0q19oBAEDV5fIJxaWRkpKil156SW+++aY2btyoxYsXa+nSpXrhhReKHTN+/HidOHHCsezfv78SKwYAAJXNZVdu/Pz85O7urszMTKf2zMxMBQQEFDnm+eefV//+/TV48GBJ0g033KDs7GwNHTpUzz77rNzcCmc1u90uu91e/gcAAACqJJddufH09FRkZKSSk5Mdbfn5+UpOTlZUVFSRY06fPl0owLi7u0uSjDEVVywAAKg2XHblRpLi4+MVGxur9u3bq0OHDpo5c6ays7MVFxcnSRowYIAaNWqkKVOmSJJ69eqlGTNmqG3bturYsaN27typ559/Xr169XKEHAAAcHVzabjp27evDh8+rAkTJigjI0Nt2rTR8uXLHZOM9+3b53Sl5rnnnpPNZtNzzz2nAwcOqEGDBurVq5defPFFVx0CAACoYlwabiRpxIgRGjFiRJHrUlJSnF57eHgoISFBCQkJlVAZAACojqrV01IAAACXQ7gBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACW4vJwk5iYqJCQEHl5ealjx45av379JfsfP35cw4cPV2BgoOx2u6677jotW7askqoFAABVnYcrd75o0SLFx8dr9uzZ6tixo2bOnKmYmBht375dDRs2LNQ/NzdX3bp1U8OGDfXxxx+rUaNG+uWXX+Tr61v5xQMAgCrJpeFmxowZGjJkiOLi4iRJs2fP1tKlSzVv3jyNGzeuUP958+bp6NGj+vbbb1WjRg1JUkhISGWWDAAAqjiX3ZbKzc1VWlqaoqOj/1eMm5uio6OVmppa5JjPPvtMUVFRGj58uPz9/dW6dWu99NJLysvLq6yyAQBAFeeyKzdHjhxRXl6e/P39ndr9/f21bdu2Isfs3r1bq1at0kMPPaRly5Zp586devzxx3Xu3DklJCQUOSYnJ0c5OTmO11lZWeV3EAAAoMpx+YTi0sjPz1fDhg01Z84cRUZGqm/fvnr22Wc1e/bsYsdMmTJFPj4+jiU4OLgSKwYAAJXNZeHGz89P7u7uyszMdGrPzMxUQEBAkWMCAwN13XXXyd3d3dEWHh6ujIwM5ebmFjlm/PjxOnHihGPZv39/+R0EAACoclwWbjw9PRUZGank5GRHW35+vpKTkxUVFVXkmJtuukk7d+5Ufn6+o23Hjh0KDAyUp6dnkWPsdru8vb2dFgAAYF0uvS0VHx+vt99+W++++662bt2qxx57TNnZ2Y6npwYMGKDx48c7+j/22GM6evSoRo8erR07dmjp0qV66aWXNHz4cFcdAgAAqGJc+ih43759dfjwYU2YMEEZGRlq06aNli9f7phkvG/fPrm5/S9/BQcHa8WKFXriiSd04403qlGjRho9erSeeeYZVx0CAACoYlwabiRpxIgRGjFiRJHrUlJSCrVFRUVp7dq1FVwVAACorqrV01IAAACXQ7gBAACWUqZw06xZM/3++++F2o8fP65mzZpdcVEAAABlVaZws3fv3iJ/8iAnJ0cHDhy44qIAAADKqlQTij/77DPHP69YsUI+Pj6O13l5eUpOTuaHLAEAgEuVKtz06dNHkmSz2RQbG+u0rkaNGgoJCdH06dPLrTgAAIDSKlW4Kfhm4KZNm+q7776Tn59fhRQFAABQVmX6nps9e/aUdx0AAADlosxf4pecnKzk5GQdOnTI6beeJGnevHlXXBgAAEBZlCncTJo0SZMnT1b79u0VGBgom81W3nUBAACUSZnCzezZs7VgwQL179+/vOsBAAC4ImX6npvc3Fx16tSpvGsBAAC4YmUKN4MHD9b7779f3rUAAABcsTLdljp79qzmzJmjlStX6sYbb1SNGjWc1s+YMaNcigMAACitMoWbH374QW3atJEkbd682Wkdk4sBAIArlSncrF69urzrAAAAKBdlmnMDAABQVZXpyk3Xrl0veftp1apVZS4IAADgSpQp3BTMtylw7tw5paena/PmzYV+UBMAAKAylSncvPbaa0W2T5w4UadOnbqiggAAAK5Euc65efjhh/ldKQAA4FLlGm5SU1Pl5eVVnpsEAAAolTLdlvrLX/7i9NoYo99++00bNmzQ888/Xy6FAQAAlEWZwo2Pj4/Tazc3N4WFhWny5Mm64447yqUwAACAsihTuJk/f3551wEAAFAuyhRuCqSlpWnr1q2SpOuvv15t27Ytl6IAAADKqkzh5tChQ3rggQeUkpIiX19fSdLx48fVtWtXffjhh2rQoEF51ggAAFBiZXpaauTIkTp58qS2bNmio0eP6ujRo9q8ebOysrI0atSo8q4RAACgxMp05Wb58uVauXKlwsPDHW2tWrVSYmIiE4oBAIBLlenKTX5+vmrUqFGovUaNGsrPz7/iogAAAMqqTOHmtttu0+jRo3Xw4EFH24EDB/TEE0/o9ttvL7fiAAAASqtM4eaNN95QVlaWQkJCFBoaqtDQUDVt2lRZWVl6/fXXy7tGAACAEivTnJvg4GBt3LhRK1eu1LZt2yRJ4eHhio6OLtfiAAAASqtUV25WrVqlVq1aKSsrSzabTd26ddPIkSM1cuRI/elPf9L111+vr7/+uqJqBQAAuKxShZuZM2dqyJAh8vb2LrTOx8dHw4YN04wZM8qtOAAAgNIqVbj5/vvv1b1792LX33HHHUpLS7viogAAAMqqVOEmMzOzyEfAC3h4eOjw4cNXXBQAAEBZlSrcNGrUSJs3by52/Q8//KDAwMArLgoAAKCsShVuevTooeeff15nz54ttO7MmTNKSEjQXXfdVW7FAQAAlFapHgV/7rnntHjxYl133XUaMWKEwsLCJEnbtm1TYmKi8vLy9Oyzz1ZIoQAAACVRqnDj7++vb7/9Vo899pjGjx8vY4wkyWazKSYmRomJifL396+QQgEAAEqi1F/i16RJEy1btkzHjh3Tzp07ZYxRixYtVK9evYqoDwAAoFTK9A3FklSvXj396U9/Ks9aAAAArliZflsKAACgqiLcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAAS6kS4SYxMVEhISHy8vJSx44dtX79+hKN+/DDD2Wz2dSnT5+KLRAAAFQbLg83ixYtUnx8vBISErRx40ZFREQoJiZGhw4duuS4vXv36sknn9Qtt9xSSZUCAIDqwOXhZsaMGRoyZIji4uLUqlUrzZ49W7Vq1dK8efOKHZOXl6eHHnpIkyZNUrNmzSqxWgAAUNW5NNzk5uYqLS1N0dHRjjY3NzdFR0crNTW12HGTJ09Ww4YNNWjQoMvuIycnR1lZWU4LAACwLpeGmyNHjigvL0/+/v5O7f7+/srIyChyzJo1a/TOO+/o7bffLtE+pkyZIh8fH8cSHBx8xXUDAICqy+W3pUrj5MmT6t+/v95++235+fmVaMz48eN14sQJx7J///4KrhIAALiShyt37ufnJ3d3d2VmZjq1Z2ZmKiAgoFD/Xbt2ae/everVq5ejLT8/X5Lk4eGh7du3KzQ01GmM3W6X3W6vgOoBAEBV5NIrN56enoqMjFRycrKjLT8/X8nJyYqKiirUv2XLlvrxxx+Vnp7uWO6++2517dpV6enp3HICAACuvXIjSfHx8YqNjVX79u3VoUMHzZw5U9nZ2YqLi5MkDRgwQI0aNdKUKVPk5eWl1q1bO4339fWVpELtAADg6uTycNO3b18dPnxYEyZMUEZGhtq0aaPly5c7Jhnv27dPbm7VamoQAABwIZeHG0kaMWKERowYUeS6lJSUS45dsGBB+RcEAACqLS6JAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAAS6kS4SYxMVEhISHy8vJSx44dtX79+mL7vv3227rllltUr1491atXT9HR0ZfsDwAAri4uDzeLFi1SfHy8EhIStHHjRkVERCgmJkaHDh0qsn9KSor69eun1atXKzU1VcHBwbrjjjt04MCBSq4cAABURS4PNzNmzNCQIUMUFxenVq1aafbs2apVq5bmzZtXZP/33ntPjz/+uNq0aaOWLVtq7ty5ys/PV3JyciVXDgAAqiKXhpvc3FylpaUpOjra0ebm5qbo6GilpqaWaBunT5/WuXPnVL9+/SLX5+TkKCsry2kBAADW5dJwc+TIEeXl5cnf39+p3d/fXxkZGSXaxjPPPKOgoCCngHShKVOmyMfHx7EEBwdfcd0AAKDqcvltqSvx8ssv68MPP9Qnn3wiLy+vIvuMHz9eJ06ccCz79++v5CoBAEBl8nDlzv38/OTu7q7MzEyn9szMTAUEBFxy7LRp0/Tyyy9r5cqVuvHGG4vtZ7fbZbfby6VeAABQ9bn0yo2np6ciIyOdJgMXTA6OiooqdtzUqVP1wgsvaPny5Wrfvn1llAoAAKoJl165kaT4+HjFxsaqffv26tChg2bOnKns7GzFxcVJkgYMGKBGjRppypQpkqRXXnlFEyZM0Pvvv6+QkBDH3Jw6deqoTp06LjsOAABQNbg83PTt21eHDx/WhAkTlJGRoTZt2mj58uWOScb79u2Tm9v/LjDNmjVLubm5uvfee522k5CQoIkTJ1Zm6QAAoApyebiRpBEjRmjEiBFFrktJSXF6vXfv3oovCAAAVFvV+mkpAACAixFuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApVSJcJOYmKiQkBB5eXmpY8eOWr9+/SX7//vf/1bLli3l5eWlG264QcuWLaukSgEAQFXn8nCzaNEixcfHKyEhQRs3blRERIRiYmJ06NChIvt/++236tevnwYNGqRNmzapT58+6tOnjzZv3lzJlQMAgKrI5eFmxowZGjJkiOLi4tSqVSvNnj1btWrV0rx584rs/49//EPdu3fXU089pfDwcL3wwgtq166d3njjjUquHAAAVEUuDTe5ublKS0tTdHS0o83NzU3R0dFKTU0tckxqaqpTf0mKiYkptj8AALi6eLhy50eOHFFeXp78/f2d2v39/bVt27Yix2RkZBTZPyMjo8j+OTk5ysnJcbw+ceKEJCkrK+tKSscl5OTklPj9PXvu3JWfixwjufh85mefKttxXFR7fs7pEr8n2dn5Tv1K875fqMy1l7cqcB4BVF0F/z9ljLl8Z+NCBw4cMJLMt99+69T+1FNPmQ4dOhQ5pkaNGub99993aktMTDQNGzYssn9CQoKRxMLCwsLCwmKBZf/+/ZfNFy69cuPn5yd3d3dlZmY6tWdmZiogIKDIMQEBAaXqP378eMXHxzte5+fn6+jRo7rmmmtks9mu8AiqhqysLAUHB2v//v3y9vZ2dTkoAueoeuA8VX2co6qvos6RMUYnT55UUFDQZfu6NNx4enoqMjJSycnJ6tOnj6Q/wkdycrJGjBhR5JioqCglJydrzJgxjrakpCRFRUUV2d9ut8tutzu1+fr6lkf5VY63tzcf9iqOc1Q9cJ6qPs5R1VcR58jHx6dE/VwabiQpPj5esbGxat++vTp06KCZM2cqOztbcXFxkqQBAwaoUaNGmjJliiRp9OjR6ty5s6ZPn66ePXvqww8/1IYNGzRnzhxXHgYAAKgiXB5u+vbtq8OHD2vChAnKyMhQmzZttHz5csek4X379snN7X8PdXXq1Envv/++nnvuOf3tb39TixYttGTJErVu3dpVhwAAAKoQl4cbSRoxYkSxt6FSUlIKtd1333267777Kriq6sNutyshIaHQ7TdUHZyj6oHzVPVxjqq+qnCObMaU5JkqAACA6sHl31AMAABQngg3AADAUgg3AADAUgg3AADAUgg31dTRo0f10EMPydvbW76+vho0aJBOnTp1yTFdunSRzWZzWh599NFKqtj6EhMTFRISIi8vL3Xs2FHr16+/ZP9///vfatmypby8vHTDDTdo2bJllVTp1a0052nBggWFPjNeXl6VWO3V5auvvlKvXr0UFBQkm82mJUuWXHZMSkqK2rVrJ7vdrubNm2vBggUVXufVrrTnKSUlpdDnyGazFfubkOWBcFNNPfTQQ9qyZYuSkpL0+eef66uvvtLQoUMvO27IkCH67bffHMvUqVMroVrrW7RokeLj45WQkKCNGzcqIiJCMTExOnToUJH9v/32W/Xr10+DBg3Spk2b1KdPH/Xp00ebN2+u5MqvLqU9T9If37J64Wfml19+qcSKry7Z2dmKiIhQYmJiifrv2bNHPXv2VNeuXZWenq4xY8Zo8ODBWrFiRQVXenUr7XkqsH37dqfPUsOGDSuoQsmlP5yJsvnpp5+MJPPdd9852r744gtjs9nMgQMHih3XuXNnM3r06Eqo8OrToUMHM3z4cMfrvLw8ExQUZKZMmVJk//vvv9/07NnTqa1jx45m2LBhFVrn1a6052n+/PnGx8enkqrDhSSZTz755JJ9nn76aXP99dc7tfXt29fExMRUYGW4UEnO0+rVq40kc+zYsUqpyRhjuHJTDaWmpsrX11ft27d3tEVHR8vNzU3r1q275Nj33ntPfn5+at26tcaPH6/Tp09XdLmWl5ubq7S0NEVHRzva3NzcFB0drdTU1CLHpKamOvWXpJiYmGL748qV5TxJ0qlTp9SkSRMFBwerd+/e2rJlS2WUixLgc1S9tGnTRoGBgerWrZu++eabCt1XlfiGYpRORkZGoct5Hh4eql+//iXvYT744INq0qSJgoKC9MMPP+iZZ57R9u3btXjx4oou2dKOHDmivLw8x0+GFPD399e2bduKHJORkVFk/4q8B321K8t5CgsL07x583TjjTfqxIkTmjZtmjp16qQtW7aocePGlVE2LqG4z1FWVpbOnDmjmjVruqgyXCgwMFCzZ89W+/btlZOTo7lz56pLly5at26d2rVrVyH7JNxUIePGjdMrr7xyyT5bt24t8/YvnJNzww03KDAwULfffrt27dql0NDQMm8XsKqoqChFRUU5Xnfq1Enh4eF666239MILL7iwMqD6CAsLU1hYmON1p06dtGvXLr322mtauHBhheyTcFOFjB07VgMHDrxkn2bNmikgIKDQBMjz58/r6NGjCggIKPH+OnbsKEnauXMn4eYK+Pn5yd3dXZmZmU7tmZmZxZ6PgICAUvXHlSvLebpYjRo11LZtW+3cubMiSkQpFfc58vb25qpNFdehQwetWbOmwrbPnJsqpEGDBmrZsuUlF09PT0VFRen48eNKS0tzjF21apXy8/MdgaUk0tPTJf1xyRBl5+npqcjISCUnJzva8vPzlZyc7PRf/ReKiopy6i9JSUlJxfbHlSvLebpYXl6efvzxRz4zVQSfo+orPT29Yj9HlTZ1GeWqe/fupm3btmbdunVmzZo1pkWLFqZfv36O9b/++qsJCwsz69atM8YYs3PnTjN58mSzYcMGs2fPHvPpp5+aZs2amVtvvdVVh2ApH374obHb7WbBggXmp59+MkOHDjW+vr4mIyPDGGNM//79zbhx4xz9v/nmG+Ph4WGmTZtmtm7dahISEkyNGjXMjz/+6KpDuCqU9jxNmjTJrFixwuzatcukpaWZBx54wHh5eZktW7a46hAs7eTJk2bTpk1m06ZNRpKZMWOG2bRpk/nll1+MMcaMGzfO9O/f39F/9+7dplatWuapp54yW7duNYmJicbd3d0sX77cVYdwVSjteXrttdfMkiVLzM8//2x+/PFHM3r0aOPm5mZWrlxZYTUSbqqp33//3fTr18/UqVPHeHt7m7i4OHPy5EnH+j179hhJZvXq1cYYY/bt22duvfVWU79+fWO3203z5s3NU089ZU6cOOGiI7Ce119/3Vx77bXG09PTdOjQwaxdu9axrnPnziY2Ntap/0cffWSuu+464+npaa6//nqzdOnSSq746lSa8zRmzBhHX39/f9OjRw+zceNGF1R9dSh4ZPjipeCcxMbGms6dOxca06ZNG+Pp6WmaNWtm5s+fX+l1X21Ke55eeeUVExoaary8vEz9+vVNly5dzKpVqyq0RpsxxlTcdSEAAIDKxZwbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbACUycOBA9enTx/G6S5cuGjNmzBVtszy2Ud1t375dAQEBOnnyZLlud/ny5WrTpo3y8/PLdbtAdUC4AaqxgQMHymazyWazydPTU82bN9fkyZN1/vz5Ct/34sWL9cILL5Sob0pKimw2m44fP17mbZTV3r17He/RxcvatWsrdN8lMX78eI0cOVJ169YtUf8bbrhBjz76aJHrFi5cKLvdriNHjqh79+6qUaOG3nvvvfIsF6gWCDdANde9e3f99ttv+vnnnzV27FhNnDhRr776apF9c3Nzy22/9evXL/Ef5IrcRkmtXLlSv/32m9MSGRlZZN/i3qdz586Vad/Fjdu3b58+//xzDRw4sMTbGjRokD788EOdOXOm0Lr58+fr7rvvlp+fn6Q/wu8///nPMtUMVGeEG6Cas9vtCggIUJMmTfTYY48pOjpan332maT/3Up68cUXFRQUpLCwMEnS/v37df/998vX11f169dX7969tXfvXsc28/LyFB8fL19fX11zzTV6+umndfHP0F18SyknJ0fPPPOMgoODZbfb1bx5c73zzjvau3evunbtKkmqV6+ebDab44/5xds4duyYBgwYoHr16qlWrVq688479fPPPzvWL1iwQL6+vlqxYoXCw8NVp04dR7i7nGuuuUYBAQFOS40aNSRJEydOVJs2bTR37lw1bdpUXl5ekiSbzaZZs2bp7rvvVu3atfXiiy9KkmbNmqXQ0FB5enoqLCxMCxcudNpXceMu9tFHHykiIkKNGjVyal+zZo1uueUW1axZU8HBwRo1apSys7MlSQ8//LDOnDmj//znP05j9uzZo5SUFA0aNMjR1qtXL23YsEG7du267PsDWAnhBrCYmjVrOl15SE5O1vbt25WUlKTPP/9c586dU0xMjOrWrauvv/5a33zzjSMkFIybPn26FixYoHnz5mnNmjU6evSoPvnkk0vud8CAAfrggw/0z3/+U1u3btVbb72lOnXqKDg42PGHePv27frtt9/0j3/8o8htDBw4UBs2bNBnn32m1NRUGWPUo0cPpysfp0+f1rRp07Rw4UJ99dVX2rdvn5588skrfdu0c+dO/ec//9HixYuVnp7uaJ84caLuuece/fjjj3rkkUf0ySefaPTo0Ro7dqw2b96sYcOGKS4uTqtXr3ba3sXjivL111+rffv2Tm27du1S9+7d9de//lU//PCDFi1apDVr1mjEiBGSJD8/P/Xu3Vvz5s1zGrdgwQI1btxYd9xxh6Pt2muvlb+/v77++usreWuA6qdCf3McQIWKjY01vXv3NsYYk5+fb5KSkozdbjdPPvmkY72/v7/JyclxjFm4cKEJCwsz+fn5jracnBxTs2ZNs2LFCmOMMYGBgWbq1KmO9efOnTONGzd27MsYYzp37mxGjx5tjDFm+/btRpJJSkoqss7Vq1cbSebYsWNO7RduY8eOHUaS+eabbxzrjxw5YmrWrGk++ugjY4wx8+fPN5LMzp07HX0SExONv79/se/Rnj17jCRTs2ZNU7t2baelQEJCgqlRo4Y5dOiQ01hJZsyYMU5tnTp1MkOGDHFqu++++0yPHj0uOa4oERERZvLkyU5tgwYNMkOHDnVq+/rrr42bm5s5c+aMMcaY5cuXG5vNZnbv3m2M+ePcN2nSxDz33HOF9tG2bVszceLEy9YCWImHC3MVgHLw+eefq06dOjp37pzy8/P14IMPauLEiY71N9xwgzw9PR2vv//+e+3cubPQXJezZ89q165dOnHihH777Td17NjRsc7Dw0Pt27cvdGuqQHp6utzd3dW5c+cyH8fWrVvl4eHhtN9rrrlGYWFh2rp1q6OtVq1aCg0NdbwODAzUoUOHLrv9RYsWKTw8vNj1TZo0UYMGDQq1X3xlZevWrRo6dKhT20033VToatTF44py5swZxy2wAt9//71++OEHp4nAxhjl5+drz549Cg8PV7du3dS4cWPNnz9fkydPVnJysvbt26e4uLhC+6hZs6ZOnz592VoAKyHcANVc165dNWvWLHl6eiooKEgeHs4f69q1azu9PnXqlCIjI4t8iqaoP+4lUbNmzTKNK4uCeTIFbDZbsaHrQsHBwWrevHmx6y9+ny7XfjklGefn56djx445tZ06dUrDhg3TqFGjCvW/9tprJUlubm4aOHCg3n33XU2cOFHz589X165d1axZs0Jjjh49WubzClRXzLkBqrnatWurefPmuvbaawsFm6K0a9dOP//8sxo2bKjmzZs7LT4+PvLx8VFgYKDWrVvnGHP+/HmlpaUVu80bbrhB+fn5+u9//1vk+oIrR3l5ecVuIzw8XOfPn3fa7++//67t27erVatWlz2uyhIeHq5vvvnGqe2bb74pU41t27bVTz/95NTWrl07/fTTT4XOTfPmzZ2uwMXFxWn//v1avHixPvnkE6eJxAUKrsa1bdu21LUB1RnhBrjKPPTQQ45JqV9//bXjKZtRo0bp119/lSSNHj1aL7/8spYsWaJt27bp8ccfL/QdNRcKCQlRbGysHnnkES1ZssSxzY8++kjSH7d8bDabPv/8cx0+fFinTp0qtI0WLVqod+/eGjJkiNasWaPvv/9eDz/8sBo1aqTevXtf8XH//vvvysjIcFrOnj1b6u089dRTWrBggWbNmqWff/5ZM2bM0OLFi8s0qTkmJkapqalOoe+ZZ57Rt99+qxEjRig9PV0///yzPv30U8eE4gJNmzbVbbfdpqFDh8put+svf/lLoe2vXbtWdrtdUVFRpa4NqM4IN8BVplatWvrqq6907bXX6i9/+YvCw8M1aNAgnT17Vt7e3pKksWPHqn///oqNjVVUVJTq1q2re+6555LbnTVrlu699149/vjjatmypYYMGeJ4fLlRo0aaNGmSxo0bJ39//0J/qAvMnz9fkZGRuuuuuxQVFSVjjJYtW1boVlRZREdHKzAw0GlZsmRJqbfTp08f/eMf/9C0adN0/fXX66233tL8+fPVpUuXUm/rzjvvlIeHh1auXOlou/HGG/Xf//5XO3bs0C233KK2bdtqwoQJCgoKKjR+0KBBOnbsmB588MFCc3ck6YMPPtBDDz2kWrVqlbo2oDqzmZLcrAYAVIjExER99tlnWrFiRblu98iRIwoLC9OGDRvUtGnTct02UNUxoRgAXGjYsGE6fvy4Tp48Wa7f1rx37169+eabBBtclbhyAwAALIU5NwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFL+HyPTXhct2A5zAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = [true - pred for true, pred in zip(true_values, predictions)]\n",
    "plt.hist(residuals, bins=30)\n",
    "plt.xlabel('Prediction Error (eV)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detanet_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
